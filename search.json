[{"title":"Vue3项目支持离线加载","path":"/2025/43489/","content":"PWA（Progressive Web App，渐进式网页应用） 是一种融合了 网页（Web） 与 原生应用（Native App） 优点的技术。简单来说，就是可以让你的网页从标签页面变成一个可以安装的应用，并且允许用户在没有网络连接的情况下使用。 技术简介 PWA（Progressive Web App，渐进式网页应用） 是一种融合了 网页（Web） 与 原生应用（Native App） 优点的技术。它让你用网页技术（HTML、CSS、JavaScript）构建出一个能像原生应用那样： 可以安装到桌面或手机； 能在离线或网络差的情况下运行； 能接收推送通知； 启动速度快，体验接近原生 App； 并且依然是通过浏览器访问，不需要上架应用商店。换句话说，PWA 网页 + 应用体验。 —— ChatGPT 简单来说，就是浏览器为你的网页提供了离线应用支持，它将你的网页所需资源缓存了下来，并拦截了页面请求，能匹配上这些资源的就直接存缓存中拿出来。 这里涉及到了两个重要的技术点： Service Worker：它是一个在浏览器后台运行的脚本，负责拦截网络请求、缓存资源、处理推送通知等。 Manifest 文件：它是一个 JSON 格式的文件，用于描述你的 PWA 应用的名称、图标、启动 URL 等信息。 Service Worker 帮你实现离线使用与同步更新，Manifest 则告诉浏览器你配置的应用信息。 就像这样： 右上角会出现一个安装应用的按钮，点击即可安装到桌面。 核心原理PWA的核心原理是利用Service Worker来拦截网络请求，匹配到缓存中的资源就直接返回，否则就去请求服务器。 而PWA检测更新的原理更加简单，当使用PWA时，会生成一个类似于检验码的js文件，文件地址是固定的。就像md5校验码一样，浏览器会定期请求这个文件，如果与本地的文件有差异则表示此网页有新的版本。 简单开始以下内容弄个基于Vue3 + Vite进行说明。 安装vite-plugin-pwavite-plugin-pwa是Vite提供的便利性插件，让你通过简单的配置即可完成离线应用。 npm install vite-plugin-pwa 配置PWA在vite.config.js中添加如下配置： import VitePWA from vite-plugin-pwaimport defineConfig from viteimport vue from @vitejs/plugin-vueexport default defineConfig( plugins: [ vue(), VitePWA( registerType: autoUpdate, // 自动更新 service worker includeAssets: [favicon.svg, favicon.ico, robots.txt, apple-touch-icon.png], manifest: name: My Vue App, short_name: VueApp, description: My awesome offline-ready Vue 3 app, theme_color: #42b883, icons: [ src: pwa-192x192.png, sizes: 192x192, type: image/png , src: pwa-512x512.png, sizes: 512x512, type: image/png ] , ) ]) 参数说明 registerType: 更新类型，包括两个值[prompt | autoUpdate]，prompt表示让用户进行弹窗确认来升级，autoUpdate表示自动进行的升级。 includeAssets: 你需要进行缓存的静态资源，比如favicon.ico、robots.txt等。 manifest: 应用清单，也就是应用信息。这里需要特别注意icons参数，它需要真实的图片地址与分辨率，如有错误则不会在浏览器上显示应用安装按钮。 进行升级配置在main.js中添加如下内容： import registerSW from virtual:pwa-registerconst updateSW = registerSW( onNeedRefresh() if (confirm(有新版本可用，是否立即更新？)) updateSW(true) , onOfflineReady() console.log(应用已准备好离线使用) ) 这里的updateSW定义是为了让开发者根据需要来进行手动升级，开发者可以通过调用updateSW(true)来手动进行页面更新。你也可以不进行定义，这样只能等待用户手动关闭页面并重新打开才能完成新页面的载入。 小提示 如果你是使用的类似于vite dev的开发模式，那么离线应用是不会生效的，你需要先对应用进行打包，然后再使用vite preview的方式进行预览。 你可以打开f12，随后刷新网页来看一下是否缓存成功了（出现ServiceWorker则表示成功了）：","tags":["Vue3","前端"],"categories":["技术"]},{"title":"【一页·ONE】正式开源","path":"/2025/3/","content":"可视化编辑的在线个人主页，何不来试试？ 一页 · ONE 是一个无服务器的纯前端主页，专为个性化和私有化设计。 不同于传统的个人主页，一页 · ONE 采用 组件 + 网格 的形式，用户可以自由组合、定制和分享自己的页面或是组件。 项目地址 仓库地址：Github 项目地址：OnePageWeb 在线地址以下为Page挂载地址，请根据网络环境选择访问： GithubPage Cloudflare Netlify Vercel 截图"},{"title":"Docker下安装ES","path":"/2025/23819/","content":"Docker下安装ElasticSearch还是很简单的，只需要两步，第一步创建卷，第二部创建并运行容器 创建数据卷创建数据的目的是为了避免权限问题 docker volume create es_datadocker volume create es_configdocker volume create es_plugins pulldocker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e discovery.type=single-node -e ES_JAVA_OPTS=-Xms1g -Xmx1g -e xpack.security.enabled=false -v es_data:/usr/share/elasticsearch/data -v es_config:/usr/share/elasticsearch/config -v es_plugins:/usr/share/elasticsearch/plugins swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/library/elasticsearch:7.17.3","tags":["教程"]},{"title":"Xxl-job安装简述","path":"/2025/10209/","content":"Xxl-job的安装应该是很简单，只是有些地方需要注意一下 pull镜像docker run -di -e PARAMS=--spring.datasource.url=jdbc:mysql://192.168.252.135:3306/xxl_job?allowPublicKeyRetrieval=trueUnicode=truecharacterEncoding=UTF-8autoReconnect=trueserverTimezone=Asia/Shanghai --spring.datasource.username=root --spring.datasource.password=mysql@14321 --xxl.job.accessToken=archives \\-p 8080:8080 \\-v /home/verlif/xxljob:/data/applogs \\--name xxl-job \\--privileged=true \\xuxueli/xxl-job-admin:2.4.0 参数说明 参数 值说明 spring.datasource.url 数据库jdbcurl地址 spring.datasource.username 数据库连接名称 spring.datasource.password 数据库连接密码 xxl.job.accessToken xxljob任务token web端访问web端访问地址为：http://ip:port/xxl-job-admin 默认用户名为admin 默认密码为123456 corn表达式错误换个浏览器即可解决。 java配置xxl: job: executor: address: # 本机地址，为空则自动获取 appname: archives # 在xxljob中注册的执行器名称 ip: # 本机IP logpath: /data/applogs/xxl-job/jobhandler # 日志目录 logretentiondays: -1 # 日志保留时间 port: 9999 # 本机执行端口 admin: addresses: http://192.168.252.135:8080/xxl-job-admin # xxljob web访问地址，通常是 http://ip:port/xxl-job-admin accessToken: archives # xxljob token，与上文中的xxl.job.accessToken对应","tags":["教程"]},{"title":"ForkJoinTask的简单使用","path":"/2024/13608/","content":"简述场景合并运算的场景，例如批量数据组合运算。 例如在合并两个来源的数据时，我们往往需要通过两个方法来分别获取数据。当数据量或等待时间过长时，我们可以将这两个方法并行执行来缩短执行时间。 通常使用 通过fork()方法将任务交给任务池异步执行 使用join()方法阻塞式获取执行结果 举例： public static void main(String[] args) throws Exception ForkJoinTaskInteger task = new RecursiveTaskInteger() @Override protected Integer compute() // TODO: 模拟计算耗时 try Thread.sleep(1200); catch (InterruptedException ignored) System.out.println(计算完成); return 123; ; task.fork(); // TODO: 模拟耗时任务 Thread.sleep(1000); System.out.println(主线程完成); // 处理结果 Integer result = task.join(); System.out.println(计算结果： + result); 方法简述fork将任务推送到任务队列中异步执行。 由于ForkJoinTask的状态机制，导致了在任务执行途中（isDone()方法返回false时）再次调用fork时，会再次推送任务并异步执行。 但由于fork的返回值是自身，因此在fork后的join中，只会返回最近的一次缓存结果。 join等待进行中的任务并返回运算结果。 当任务被多次执行时，则会等待最后一次任务的执行返回。 public static void main(String[] args) throws Exception ForkJoinTaskInteger task = new RecursiveTaskInteger() @Override protected Integer compute() // TODO: 模拟计算耗时 try Thread.sleep(1200); catch (InterruptedException ignored) int i = new Random().nextInt(100); System.out.println(计算完成 - + i); return i; ; task.fork(); Thread.sleep(200); task.fork(); Thread.sleep(200); task.fork(); Integer join = task.join(); System.out.println(join); 这里的返回结果就是最后一次fork中计算的结果。 invokeinvoke方法实质上是让任务尝试执行，阻塞并返回结果。 有一点值得注意，在调用invoke方法时，若任务处于运行中时，则任务会被再次执行。若任务已经结束，则直接返回结果。 示例如下： public static void main(String[] args) throws Exception ForkJoinTaskInteger task = new RecursiveTaskInteger() @Override protected Integer compute() // TODO: 模拟计算耗时 try Thread.sleep(1200); catch (InterruptedException ignored) int i = new Random().nextInt(100); System.out.println(计算完成 - + i); return i; ; task.fork(); // TODO: 模拟耗时任务 Thread.sleep(1000); System.out.println(主线程完成); // 此时任务还未完成，调用invoke方法时会导致任务再次执行 task.invoke(); System.out.println(计算结果： + task.getRawResult()); // 此时任务已经完成，调用invoke方法则会直接返回缓存结果 task.invoke(); System.out.println(计算结果： + task.getRawResult()); 此时的结果输出如下： 主线程完成计算完成 - 92计算完成 - 21计算结果：21计算结果：21 getRawResult直接返回任务的缓存结果。若无结果则返回null。","tags":["Java","多线程"]},{"title":"Callable与Future","path":"/2024/22258/","content":"Callable两年前我写过一篇Callable的简单应用，如今来看以前写的东西和没写没什么区别。 所以今天我详细记录一下我对于Callable的理解与使用。 设计逻辑Callable与Runnable相似，都是单方法的接口，但Callable存在一个返回值，由类上的泛型定义。 @FunctionalInterfacepublic interface CallableV /** * Computes a result, or throws an exception if unable to do so. * * @return computed result * @throws Exception if unable to compute a result */ V call() throws Exception; 这是因为Callable除了做运行逻辑的封装，还负责对外提供运行结果，结果类型就需要定义在类上。 举例，我现在在做饭，但正好快递到了需要我去取。此时我有以下的方式（包括但不限于）可以完成这两件事： 做完饭后再去取，我一个人就能完成（消耗资源少），但最终完成时间太久（耗时） 让别人（进程中的资源）帮忙去取，这样就不会耽误我做饭了（并行） 在不着急的情况下，我可以使用第一种方式，因为我对于这两件事情完全可控。 如果我使用了第二种方式，那么别人在帮我取完快递后，我还需要从别人那里拿到我的快递（执行结果）。 那么此时，我们就涉及到了多线程的结果处理方式。 线程在程序运行中经常会遇到多个逻辑需要同时处理的情况，不过很多时候线性执行的效率比调度线程更快，因此我们大部分时候都是顺序编写。但如果我们遇到需要同步计算以降低时间消耗的场景时，就需要设计并行逻辑了。 在操作系统中，我们用到的并行调度几乎都是由线程完成的。它作为系统调度的最小单位，可以并行运算以减少操作队列长度来节约时间。 转换到Java语言中，我们有这样的描述： Runnable是我们需要执行的原子操作，例如做饭是一个Runnable，取快递也是一个Runnable。Thread是我们直接执行Runnable的操作器，它同样实现了Runnable接口，因此它也可以看成一个原子操作。Executor是我们执行Runnable的执行器，与Thread不同的是，Executor并不直接绑定Runnable，而是通过execute的方式去执行；但Thread只能绑定一个Runnable，这表示如果你有两个Runnable，你就需要使用两个Thread。 我们大部分使用的并行相关的代码都在java.util.concurrent包下。有趣的是，Thread是在java.lang包下，因此我们不需要引入包即可使用Thread。 Callable既然Runnable是原子操作了，那么Callable是干什么的呢？ 我们注意到，对于做饭与取快递来说，我们实际上是需要获取这两个行为的结果，过程（实现逻辑）其实并不是很重要。 但是Runnable只关注了行为，无法观测结果，因此如果要使用Runnable来管理结果，开发者就需要将行为结果的处理同样放在原子操作中。 这种方式对于多个关联任务来说并不友好，因此我们可以使用Callable来将结果返回出，交由任务提交者来处理，我们就可以更关注过程。 使用由于Callable存在返回值，因此我们无法使用Thread与Executor.execute，而是使用java.util.concurrent.Future。 用法举例： public static void main(String[] args) throws Exception // 定义Callable CallableString callable = () - // 模拟耗时 Thread.sleep(1000); // 返回操作结果 return OK; ; // 封装Callable到FutureTask任务实例 FutureTaskString future = new FutureTask(callable); // 启动新线程执行FutureTask任务实例 new Thread(future).start(); // 获取Callable结果 System.out.println(future.get()); 这里有两个地方值得注意： 为什么需要FutureTask实际上，与Callable配套使用的通常是Future，但Future也是接口，它只是定义了一套取值规范，因此我们需要结合实际情况来使用其具体实现类。 对于并行任务来说，我们常用Runnable处理，因此在java.util.concurrent包中存在RunnableFuture接口来结合Callable，而RunnableFuture的常用实现即是FutureTask。 为什么future.get()像同步方法因为future.get()实际是阻塞方法，需要等任务执行完毕后才进行返回。在实际使用中我们通常不会直接这样使用，而是像这样： public static void main(String[] args) throws Exception CallableString callable = () - System.out.println(1); Thread.sleep(1000); System.out.println(2); return OK; ; // 封装Callable到FutureTask任务实例 FutureTaskString future = new FutureTask(callable); // 启动新线程执行FutureTask任务实例 new Thread(future).start(); // TODO: 其他耗时任务 doSomething(); // TODO: 处理结果 String result = future.get(); System.out.println(result); 也就是说，我们先将任务封装到新线程执行，随后开始我们的主任务。当主任务完结或是需要异步任务的结果时，我们再从future中取值。 其他使用方式除了直接使用FutureTask，我们还有两种方式可以处理Callable： future.get(200, TimeUnit.MILLISECONDS) 通过增加计时等待的方式，当超时后get方法会抛出TimeoutException异常，此时开发者可以进行超时后操作，例如future.cancel(true) 另一种方式是提交给执行器去执行： public static void main(String[] args) throws Exception CallableString callable = () - System.out.println(1); Thread.sleep(1000); System.out.println(2); return OK; ; ExecutorService executorService = Executors.newFixedThreadPool(1); FutureString future = executorService.submit(callable); executorService.shutdown(); System.out.println(future.get()); Thread.join从Callable中我们可以看出，其基本的设计逻辑是，将额外的任务先交给执行器去执行，并保存返回的Future。随后在需要处理结果的时候从Future中获取结果。 这样的执行逻辑与Thread.join非常像，但是Future允许获取执行结果的这一特点让它在并发场景下的使用频率大大增加。 FutureTask尽管在大多数场景下与Callable配套使用，但FutureTask仍然支持Runnable，例如这样： FutureTaskString future = new FutureTask(new Runnable() @Override public void run() System.out.println(3); , OK); 不难看出，这样编写的结果就是，任务会在执行完Runnable的run方法后，返回OK。 取消任务例如我们在某些情况下，例如任务超时，需要将任务取消掉，我们可以通过Future提供的cancel方法： public static void main(String[] args) throws Exception CallableString callable = () - System.out.println(1); Thread.sleep(1000); System.out.println(2); return OK; ; // 封装Callable到FutureTask任务实例 FutureTaskString future = new FutureTask(callable); // 启动新线程执行FutureTask任务实例 new Thread(future).start(); try String result = future.get(200, TimeUnit.MILLISECONDS); // TODO: 处理结果 System.out.println(result); catch (TimeoutException e) future.cancel(true); // 处理超时 System.out.println(任务超时); 我们在get时设置了200毫秒的超时等待，但实际的任务会执行1000毫秒，那么在调用get方法时的200毫秒后，方法就会抛出TimeoutException异常，我们就可以在异常捕获后进行超时处理，例如回滚业务。","tags":["Java","多线程"]},{"title":"简述JDK中的ServiceLoader（服务注册机制）","path":"/2024/65058/","content":"技术是工具，不是目的，我们学习的目的是让我们在达成目的前，有更多的选择。 我就说能在知乎上学到东西吧（5毛1条，括号删掉）。 先放个链接吧，感谢程序员木木熊。 服务注册模式在使用一项技术或是规范前，我们得知道我们是否有使用它的必要。 实际上，对于使用者来说，服务注册模式与模块化类似，我们的一个功能实现并不会具体描述细节，而是通过解耦注册的方式来使用不同的逻辑实现。 那我们什么时候需要呢？通常就是在我们进行适配型功能开发的时候会使用。（容灾分压的需求暂时不在本文的讨论范围内） 更具体来说，类似于数据库调用、文件存储、数据解析等需要接入其他服务的功能开发时，我们可以通过技术手段来方便地进行处理逻辑变更。 接口设计首先我们知道一点，抽象是解耦基础，而对于Java来说，我们通常会使用接口来进行逻辑抽象化处理。 举例： 假设我需要一个翻译，而大多数的翻译都是单语言翻译，那么我就可以定义一个翻译接口： public interface Translator /** * 翻译 * * @param words 待翻译的句子 * @return 翻译后的句子 */ String translate(String words); 然后我们就可以用这个接口愉快地翻译了开始找能提供翻译的服务提供方了。 服务实现假设我们有两门语言需要进行翻译： A语言，在每句话前面要说什么？来启动，比如吃了吗？没吃的话吃我一拳就需要翻译成什么？吃了吗？没吃的话吃我一拳 B语言，在每句话后面要加上嘤嘤嘤来结尾，比如你这厮莫不是来消遣洒家就需要翻译成你这厮莫不是来消遣洒家，嘤嘤嘤 当然啦，现在经济下行了，没钱来找别人服务了，只能我们自己动手了，于是我们就有了以下的实现类： A语言翻译： public class ATranslator implements Translator @Override public String translate(String words) return 什么？ + words; B语言翻译： public class BTranslator implements Translator @Override public String translate(String words) return words + ，嘤嘤嘤; 使用服务我们既然都有了服务实现了，我们是不是可以就这样进行使用呢？ Translator translator = new ATranslator();System.out.println(translator.translate(V我50)); 的确可以，但是很明显，如果我们需要使用B语言翻译的时候，我们就需要把代码换成B语言实现类了。这里只有一个地方使用还好改，但如果多个地方使用呢？ 当然，哪怕是我们多个地方使用，我们都可以通过单例模式、工厂模式这些设计模式来减少修改量，也可以使用Spirng框架来管理注入。但是如果我们需要更动态的扩展呢？ 现在的情况是，我们的翻译有更专业的三方翻译，且在不同的地方（环境）需要用到不同的翻译。那么我们最好的方式就是使用配置模型来切换翻译实现，例如配置文件或是切换外挂组件的方式。 ServiceLoader配置服务提供者现在我们就来使用ServiceLoader。 首先，我们需要告诉ServiceLoader我们有哪些服务提供商可以选择，于是我们就在resources目录下创建一个文件夹META-INF.services。 META-INF是存放各种元数据的文件夹，里面通常是对这个jar包的描述信息。 services是ServiceLoader管理的服务信息文件夹，里面就是存放各类服务的描述文件了。 然后我们就要对我们的翻译接口进行描述了，这里我们需要创建一个名为idea.verlif.tool.Translator的文件。很明显，这个文件名是我们翻译接口的全类名，ServiceLoader就是通过文件名来对应服务的。 然后在idea.verlif.tool.Translator文件中写下我们的两个翻译实现： idea.verlif.tool.ATranslatoridea.verlif.tool.BTranslator 可以发现我们每一行就是一个实现类的全类名。 使用服务有了提供商的信息，我们就可以开始使用服务了。 ServiceLoaderTranslator translators = ServiceLoader.load(Translator.class);for (Translator driver : translators) System.out.println(driver.translate(你好呀)); 于是我们可以得到以下结果： 什么？你好呀你好呀，嘤嘤嘤 可以看到我们通过ServiceLoader.load(Translator.class)来加载服务实例管理对象。 值得注意的是，ServiceLoader是实现了Iterable接口的，因此我们可以直接进行遍历。遍历的顺序其实就是我们配置文件中实现类的书写顺序。 更适合的使用方式看起来似乎多此一举，我们既然能在配置文件中写明服务提供商，那为什么不直接用呢？因为我们目前只是在同项目中进行的服务注册，但在实际项目中，我们应该是以额外组件或是依赖的方式进行注册的。 此刻，我们新建一个项目，并新建目录idea.verlif.tool，同时将Translator接口复制于此。随后新建第三个C语言翻译类： public class CTranslator implements Translator @Override public String translate(String words) return 桀桀桀， + words; 同样的，我们在新建项目中增加服务提供商的信息文件（META-INFservicesidea.verlif.tool.Translator），并加入新建翻译类的全类名。 打包新项目，并在之前的项目中引用打包好的新项目jar包。再次运行这段代码： ServiceLoaderTranslator translators = ServiceLoader.load(Translator.class);for (Translator driver : translators) System.out.println(driver.translate(你好呀)); 此时结果就是： 什么？你好呀你好呀，嘤嘤嘤桀桀桀，你好呀 奉上截图： 总结ServiceLoader是jdk1.6引入的服务加载器，旨在模块化服务。实际上只要你点开ServiceLoader的源码，其上的注释就已经说明了其作用与用法了。 网上老是说Java是过度设计，从某种方面来说的确是这样的，但这和环境有关系。就像是现在都还有一大堆的公司喜欢写Service接口，但实际上他们只会用到一种实现方式。 技术是工具，不是目的，我们学习的目的是让我们在达成目的前，有更多的选择。","tags":["Java"],"categories":["技术"]},{"title":"短链设计","path":"/2024/35198/","content":"短链的核心逻辑是链接映射，或者说重定向。 简述短链，顾名思义，短链接。它是相对于原链接的一个概念，与原链接有相同的目的地，但比原链接更短。 为什么要使用短链接很多时候，我们的一个链接会很长，比如： https://cn.bing.com/search?q=urldecode+-site:*.csdn.net+-site:zhihu.comcvid=0c46addea00b41a9b8c2ecd1058a0190gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCDQ4ODFqMGo5qAIAsAIBFORM=ANAB01PC=U531 我们在分享或是传递这个链接的时候，就会占用更多的资源，并且在显示上也不好看，所以我们就设计了一个类似于别名的东西，你用这个别名就可以拿到对应的原链接。 设计设计上没有好讲的，一共就涉及到两个主要步骤： 短链生成 寻链 短链生成短链生成的算法其实有很多，你甚至可以自己用随机数。但是短链生成的时候需要注意一些细节： 短链冲突。不同原链生成的短链是不能重复的，也就是短链-原链是单向唯一的。 短链一般会用专有的访问域，比如短链的上下文是全局唯一，甚至可以有专门的寻链域名用作隔离。 寻链寻链其实就是寻找短链到长链映射的过程。 实际上，我们在生成短链的时候会需要将短链作为寻址key，与对应的长链一起保存下来。我们知道短链到原链是单向唯一的，这表示我们甚至可以用简单的Map表来存储映射信息。当然，是选用缓存还是持久化，这就要根据具体项目具体分析了。 简单demo我这里用Java的SpringBoot框架简单写一个demo。 @RestController@RequestMapping(a)public class AController @Autowired private LinkMap linkMap; @GetMapping(gen) public String gen(@RequestParam(sc) String sc) return linkMap.revert(sc); @GetMapping(shortChain) public void chain(@PathVariable String shortChain, HttpServletResponse response) String url = linkMap.get(shortChain); response.setStatus(302); response.setHeader(location, url); 代码内的gen方法就是生成短链的请求，chain就是短链访问的路径。 实际上，你完全可以根据个人需求，创建专门的短链生成与转发服务器。","tags":["Java"],"categories":["技术"]},{"title":"几个我非常喜欢的游戏设计","path":"/2024/52757/","content":"有意义的引导新手引导是让玩家能理解游戏内容，适应游戏设计，开始进行游戏的第一步。 无代价体验指引的目的是让玩家熟悉游戏，如果因为玩家不了解游戏机制而导致代价的产生，会极大程度增肌玩家的挫败感并导致玩家差评。因此，许多游戏也都设置了新手关，让玩家尽情在其中体验与了解游戏。 比如《星露谷物语》的钓鱼引导，玩家在拿到鱼竿后的第一次钓鱼时，右侧的进度条并不会因为玩家的失误而降低，让玩家可以慢慢适应钓鱼玩法。 再例如地牢类游戏中，新人玩家才进入游戏时，第一个房间通常是教学房间，玩家在里面并不会死亡，资源也可以随时补充，并在出房间后自动不满状态开始正式游戏。 无代价体验让玩家可以专心了解游戏机制，不必为自己的失误有所负担。 融入内容的向导不同于系统指引，游戏内的向导带给玩家的体验感与沉浸感会更最好。 例如《大侠立志传》中，新手村的老头实际上就是一个新手向导，玩家拜入其门下后，他会在玩家遭遇危险（半血以下）时出现，避免新手玩家因为对游戏不熟悉导致的频繁暴毙体验。并在不同的时机以对话的方式告诉玩家游戏机制与接下来的任务。 类似的，剧情引导同样是内容向导模式，不过属于阶段式引导，玩家需要度过一系列剧情才可以学习和解锁各类玩法。这种方式很容易导致引导剧情过长，并且或多或少都带有强制性，设计不好极易让玩家产生不适感。 阶段性任务式相比于主动式地引导，也有些游戏鼓励玩家自行探索，但为了让玩家能自有操作，便设计成了任务式的指引。例如玩家通过达到某个地点后，系统给予奖励，这种激励性质的引导能让玩家有更高的自由度。 但是，目前很多游戏即便采用的是任务引导，同样存在半强制的设计。比如玩家只能沿着教程指引的路线前进，比如游戏功能是伴随着任务阶段一同开放，导致指引本质上仍旧变成了手把手教学。 属性的多维度应用经典的RPG游戏中，一样属性基本就对应了一种使用方法，例如攻击力、防御力、血量、魔力等，越简单的使用方式越容易让玩家理解。但是玩法越复杂的游戏，一般来说涉及到的数值就越多，如果每样数值都设计一个属性，对于玩家来说反而是负担。 货币常见的就是货币类型，例如基础货币是金币，为了体现出不同类型道具的不同价值，又增设了诸如元宝、水晶、钻石、魂晶等一系列的货币类型。 很多游戏为了增加玩家体验，将这些货币的获取途径融入到了玩法中。例如魂晶是击杀敌怪有几率掉落，水晶放置于隐藏关卡或是挑战中，使得玩家能够在一局游戏中获得多种货币，避免了无意义地刷图刷怪。 血量非常经典的血量设计，当血量归零，则角色死亡。但是如果血量会有其他的用途呢，各类游戏都增加了一个新的玩法，卖血。 顾名思义，将血量视作可利用资源，去换取其他资源。 这种玩法的意义在于，本身血量是很珍贵的资源，通过卖血，允许玩家能获取更加强力的道具或机制，增加了游戏的宽度。另一方面，玩家血量减低后，会使得游戏难度上升，方便于玩家更好地体验游戏内容。 当然，为了照顾休闲玩家，有些游戏并不以扣除最大生命值作为代价，转而降低门槛，只扣除当前生命值。这两者的使用场景与后续影响对于不同类型的游戏是不同的，无法说哪种方式的代价更大。 有限资源的使用抉择规则的约束使得游戏更有乐趣，但在约束之上，是玩家抉择的体现。玩家通过自己的决策与付出来达成某些目的是成就感的主要来源，但一味地约束只会让玩家陷入困境，没有体验。 行动力在回合制游戏中，经常加入类似于行动力的设计，提升了玩家的操作空间与决策难度。玩家将优先的行动力进行分配，也为不同的玩法提供了使用空间。 实际上，诸如蓝量等可消耗的属性值也是行动力，一个游戏会提供很长多种的行动力，让玩家在不同的游戏内容或是玩法中进行策略分配。 超额的选择尽管行动力为玩家提供了决策空间，但是毕竟是一个限制玩家操作的设计，一定程度上降低了操作空间。但是如何让这些有限的资源提供更多的玩法呢，许多设计者都选择了有代价超额属性。 例如在《星露谷物语》中，玩家体力值被耗尽后，仍可行动，但是行动效率会降低非常多，并且第二天也无法补充满体力。这就避免了玩家在某些情况下，无法补充体力，但处于抉择关键期时的绝望境地。 类似的还有当魔法力不足时，可以消耗血量来替代释放法术的天赋设计。 下一回合的动力作为游戏最重要的游玩乐趣，如何让玩家继续玩下去几乎是游戏设计师最需要考虑的事情。 任务推进任务推进，也可以说剧情推进。玩家选择一条主线进行游戏，其中游戏再填充多个支线补充内容，这种做法大部分游戏都在使用，但是如果大多数的任务都是这种换汤不换药的刷马桶任务的话，长期做下去，玩家同样会腻。 因此一般任务都分为两部分，一部分是剧情任务，几乎是玩家所必须完成的，用于推进剧情。另一部分就是可选的，可以是获得能简化玩家操作的道具，也可以是某些剧情解锁条件。 通过任务奖励的方式，让玩家主动去进行游戏。 控制玩家的计划通过剧情或游戏设定来让玩家有事可做。比较经典的就是《文明》系列，玩家进行下一回合后，玩家可以进行的操作包括了 已有单位的操作（兵种移动、商队路线、间谍任务等） 已完成生产队列的城市生产空位 其他国家的外交操作（谴责、战争、交易等） 科技树研究与市政树发展 几乎每回合都有新的事件需要玩家进行操作，玩家在自己的策略规划外还需要会有额外的事件需要玩家进行完成。与任务不同的是，这些事件都是基于玩家策略产生的，并且几乎每一个事件都会影响玩家的下一次决策，使得玩家可以沉浸在规划与决策之中。 总结来说，就是系统会不断地辅助推进玩家的策略，并在适当的时机影响甚至改变玩家的计划，让玩家重新进行规划，避免游戏过于顺利导致的无聊感。 奖励关许多关卡制游戏或是积分制游戏都提供了玩家奖励环节，用于暂时的放松或是提供玩家整顿契机，这种方式改变了游戏节奏，并为玩家提供了额外的目标点。 非线性游戏阶段很多时候，逃课与跨阶段挑战同样是玩家的乐趣，能否合理规划与限制当前的游戏进程，将决定着整个游戏的自由度与游玩节奏。 传奇道具当玩家一开始获得超出当前阶段的道具时，玩家是兴奋的，此时玩家的热情会非常高。这也就是为什么会有很多的游戏会设置隐藏彩蛋，或是允许玩家通过另类的方式获取到越阶道具。 但是如果此道具过于强力，导致玩家在进入到下一个游戏阶段前，就会因为流程简单而失去游玩欲望。因此，越阶道具更注重机制而不是数值。","tags":["二三事","游戏设计"]},{"title":"我不喜欢的游戏设计","path":"/2024/52756/","content":"很多游戏好玩是好玩，但是或多或少都让我有这样那样的遗憾，这里列举一些我喜欢的游戏进行说明。 星露谷物语作为一款模拟经营游戏，游戏本身的节奏应该是比较休闲的，在种地下矿方面没有什么问题，但是对于NPC的处理就缺少了很多内容。 优点售卖箱售卖箱是个非常好的设计，它结合每日结束统计带给玩家的成就感和惊喜感是非常强烈的。 尽管游戏提供了皮埃尔商店或是威尔的渔店这些可以即时售卖物品的方式，但是玩家一般都是用来应急，也就是很缺钱了才会过来直接兑换金币。一方面是因为NPC的作息会导致玩家只能在特定的时间和地点才能售卖物品，另一方便是因为售卖箱就在家边上，非常方便。 当然，售卖方便仅仅是增加了玩家选择售卖箱的一个原因，另一个原因就是惊喜感。 玩家将物品放入售卖箱后，只会在界面上看到一个栏位，显示了玩家最后放入的物品。这里只是提供玩家反悔的机会，而不是为了展示存入的物品。这表示玩家在后期有大批量物品投放时，是不知道自己放入了多少物品并会获得多少金币的，保留了强烈的神秘感。当玩家完成了一天的劳作，躺在床上看着日结界面的数字跳动和各种音效时，此刻就会体验到游戏的一大爽点——收获的快乐。 延迟结果作为回合制游戏（一天就是一个回合，时间和体力就是两个维度的行动力），当玩家到达第二天后，总会有需要做的事情。 例如升级工具，玩家需要在第三天才能拿到昨天拿去升级的工具。因此，当玩家第三天醒来后，就会想要先把升级后的工具拿了，又因为NPC作息的关系，玩家只能在9点才可以去拿到工具，因此玩家从醒来到9点这期间，就会去执行其他行为。当玩家拿到升级后的工具时，玩家已经付出了沉没成本，因此就会把这一天继续过完，周而复始，持续游玩下去。 再比如玩家的技能升级，并不是立刻升级的，而是在日结时进行升级。 缺点NPC互动游戏为了应对单调的NPC交互，设计了NPC作息与事件，但是这些都属于静态设计（这里的静态设计是指可以进行背板的固定设计，例如NPC的每周活动作息是固定的），导致了NPC的工具性过强，容易背板。 某些游戏会引入NPC的随机心情，因为游戏的天气是随机的，NPC可以喜好某些天气厌恶某些天气，从而出现不同的互动效果。 另一点就是NPC的被动互动，除了固定事件外，NPC是不会主动与玩家互动的，因此玩家即便对所有NPC满心，与零星的区别不大。一切都需要玩家去互动的NPC世界，对我来说几乎就是工具，很难感受到互动的乐趣。 太吾绘卷模拟类游戏，我应该是最喜欢《太吾绘卷》了，但是现在仍旧是测试版，缺失了各种系统。 缺点较艺较艺一方面是趣味性不高，流程也繁琐，收益也非常低。作为游戏系统中的一个玩法，如果玩家为这个玩法付出的精力时间与获得的收益偏差过大，势必会导致两个极端，一个是不是玩这个玩法，一个是只会玩这个玩法，而较艺明显是属于前者。 NPC互动和《星露谷物语》一样，目前NPC只会在过月事件中与玩家主动互动，但是当前的互动事件非常简陋，几乎就只是有一个架子。另外就是在地格中移动时，几乎没有任何NPC互动，这导致NPC就像一个工具一样，只是摆在了地图的不同位置。 并且还有一点非常重要，玩家可以直接查看每一个NPC的所有信息。且不说玩家带入感，很明显的一点是，茄子明明就设计了类似于秘闻的互动方式来获得NPC互动信息，但是却偏偏忽略了基础信息。我理解的游戏应该是，玩家通过与某个NPC互动来慢慢解开他的各种信息，或是玩家通过打听等外部互动获取NPC信息。 茄子曾经画饼会增加地图NPC互动，只能说期待后续的表现了。 时间管理在游戏中，每个月有30天，然后因为移动的关系，可用的天数粒度实际是小数点后一位。但是但是，很多的操作花费的天数太多了，比如各种NPC互动，动辄3天3天的，我就问你能不能支持我，你还要考虑个3天是吧。 实际上，游戏和《星露谷物语》类似，都是采用回合制，但是《太吾绘卷》是在一个回合内提供了玩家30个行动点，供给玩家分配操作。如果这样理解的话，倒也行。但是因为角色会生老病死啊，这又会让玩家（我）觉得某些行动消耗的行动点数太多了，产生脱离游戏的策略感。 模拟人生这里以《模拟人生4》举例说明。 缺点无限读盘首先，第四代最为人所诟病的一点就是无限读盘，你去另一个地图要读盘，去同一张地图的另一个区域要读盘，甚至于拜访邻居都要读盘。这种非常割裂的设计使得很多玩家都会减少四处移动的欲望。 且不说在地图间的读盘，就说同区域下，前往不同的建筑时都需要读盘，这种设计太烂了。 出于这种设计的结果，系统会往玩家角色所在的地点随机刷出各种NPC。看似让世界热闹起来了，但是当你前往一个新地点时，这个地点的NPC少得可怜，但当你游玩了几分钟后，NPC随着系统设定不断刷新后这才变得热闹了起来，颇有一种《楚门的世界》的感觉。 所以读盘破坏的不止是游玩的连贯性，还有其他的各种玩法。比如你没有办法在游戏里驾车，这表示无法进行学车、体验拥挤交通、乘坐公共交通观赏风景等。比如你不需要去到超市，而是直接可以从冰箱里购买新鲜菜品来做菜，这也是因为读盘的妥协。 NPC互动与前面提到的两款游戏类似，游戏中的NPC几乎不存在与玩家控制角色主动互动的（打电话不算）。 但其实这么说不太公平，毕竟《模拟人生4》中玩家控制的角色如果有等待任务，其他NPC一般就不会主动与玩家角色互动，而如果没有，则可能会过来聊聊天，似乎是有NPC主动互动的……吗？并不是。 我对于互动的理解是，玩家可以对NPC采取的行为，NPC同样可以对玩家角色主动进行，不要求全部数量的互动，至少一半要有。但很明显，《模拟人生4》并不能满足这个要求。 并且对于NPC互动来说，打断玩家角色的行动同样是NPC系统的一个机制，能体现NPC的性格，并让玩家对NPC有更深层次的印象。 缺少赚钱的快乐在《模拟人生4》中，货币的使用场景太少了，更换大部分衣服都不需要花钱，并且玩家不需要任何技能就可以维修家具。 尽管游戏的目的是为了让玩家有更多的精力去体验其他的游戏内容，但实际上，挣钱的快乐并不低于游戏中的其他玩法带来的快乐。 为此，玩家可以选择开店来体验赚钱，但是这个DLC的质量的确太差了。玩家辛辛苦苦累死累活去经营一家店铺，结果赚的钱还没有兼职赚的多，我玩过一次就不想再玩了。 事实上，赚钱的快乐是基于花钱的快乐，但是正如我之前所说，游戏中花钱的地方太少了，所以这部分的快乐我很难体验到。","tags":["二三事","游戏设计"]},{"title":"程序设计-缓存模型","path":"/2024/34435/","content":"以空间来换时间 什么是缓存缓存是我们很常见的数据存储方式，不过相比较于直接存储，缓存牺牲了时效性与数据寿命，换来了更快的效率。 比如我们在使用一些常量数据时，如果每次都从数据源去查询，势必会浪费资源。因为这些数据的结果都是一致的，没有必要使用沉重的直接存储，可以换一种轻量的查询方式，比如放在内存中。 缓存模式缓存可以像实际数据源一样使用存储逻辑，比如你对着数据库说这就是缓存，那它就是缓存，只是这个缓存方式效率低占用大。 一般情况下，我们采用的缓存都会比使用的数据存储对象更轻量，效率更高占用更低。那既然缓存效率又高占用又低，那为什么不直接作为数据存储对象来用呢？ 缓存的劣势缓存之所以效率高，是因为大部分的缓存都使用了键值对方式的非关系性存取模式，这使得缓存的存储结构相对简单，查询效率就会更高。但是这种方式是没办法满足大多数的业务需求的，比如对数据的多维搜索。 另外就是实时性，一般来说，我们对于缓存并不要求能实时更新，这就降低了缓存的存储消耗。比如对于搜索框下的热点列表，一般都是定时更新，这表示在缓存更新间隔内，哪怕热点真的改变了，但是你看到的热点列表也并不会更新。 还有一种更常用的缓存——内存缓存，不做文件存取能得到更高的查询效率，但是这也表示当内存断电后数据会消失，这就需要每次启动时重建缓存数据。我们经常使用的Map结构就是一个临时缓存。 缓存使用基于缓存的特性，我们可以在不经常更新或是允许延迟更新数据的查询处使用缓存，比如之前提到的搜索热点，比如省市区划树。 另外，除了在数据库层级的缓存之外，在代码里也可以用到缓存。 预处理缓存举个例子，我们总是会用到一些字符串处理方法。而某些时候，我们处理的字符串都是固定的，这时我们就可以用预处理缓存的方式。比较典型的例子就是正则工具。 Pattern pattern = Pattern.compile(\\\\.);Matcher matcher = pattern.matcher(123.321); 这里我们可以看出，Pattern是个中间对象，但正是因为这个对象，我们就可以用一个正则去匹配各种需要处理的字符串。所以一般我们并不会设计这样的方法PatternUtil.matcher(String regex, String target)，这个方法在我们重复处理同一个正则时会非常浪费性能。 所以Pattern也是一个预处理缓存，相似的还有java.sql包下的PreparedStatement，这也是预处理的对象，可以重复利用来提高效率。 总结缓存的目的是为了提高效率和降低数据存取负载，基于此我们才会去使用缓存，而不是盲目使用缓存。缓存最终的结果是以空间来换时间，这点就需要对不同的需求来作权衡了。 另外就是在作实际代码编写时，对于一些工具的写法也可以通过预处理缓存的模式来提升执行效率。","tags":["想法","程序设计"],"categories":["技术"]},{"title":"程序设计-模块解耦","path":"/2024/32110/","content":"解耦是为了方便拓展与维护，你一个HelloWorld解个什么耦。 解耦概念解耦与耦合对应，我们首先要明白什么是耦合。 耦合耦合表示了两个对象间的相关关系程度，也就可以叫耦合度（对于不同的对象，耦合度的概念也有所不同）。就像是钥匙与锁一样，只有完整匹配上锁纹的钥匙才可以开锁，这种就是高耦合度。 高耦合强调了唯一关系，也就是越高耦合的钥匙与锁套件，越安全。但是换做代码中，也就是你只能用这把钥匙（调用逻辑）来开启需要的锁（调用目标）。举个例子： SaySomething.javapublic class SaySomething public void sayHi() System.out.println(hi); public void sayHello() System.out.println(hello); 我们现在有一个SaySomething的类，用来输出打招呼的话，其中我们定义了两个方法，一个是sayHi，一个是sayHello，这样我们就可以通过这个类的实例来进行两种方式的打招呼。比如： SaySomething saySomething = new SaySomething();saySomething.sayHi(); 此时如果没有其他的打招呼方式的话，那我们的代码就到此为止了。但是如果我们此时想要增加一种打招呼的方式，比如输出你好，那我们该怎么办呢？ 解耦在以上的例子中，你肯定第一时间想到了可以将我们的想要输出的对象换成参数传入，比如这样： SaySomething.javapublic class SaySomething public void say(Object hi) System.out.println(hi); 那我们的调用可以这样进行： SaySomething saySomething = new SaySomething();saySomething.say(你好); 没错，这也是我们解耦的一种方式：配置化 配置化我们将我们会用到可变参数进行配置化，将这些配置封装成入参传入方法，由方法来处理配置。 换句话说，我们提供了一套配置，由调用方法来理解我们的配置，并输出我们想要的内容。 这样似乎是很方便了，但是很明显可以看出，如果我们想要输出一个User类的信息，如果直接放入一个User对象会怎样呢？ SaySomething saySomething = new SaySomething();saySomething.say(new User()); 输出了对象hash值，很显然这不是我们想要的。当然，我们可以在重写User.toString()方法来输出我们想要的值，但是这破坏了我们的类，侵入性太强了。我们也可以将调用写成saySomething.say(new User().getName());，但是这并不符合我们对这个方法的期待。 所以我们又想了一个办法，通过接口重写的方式来适配不同的类，此时我们将SaySomething变为一个接口： public interface SaySomethingT void say(T hi); 然后我们给User一个适配实现： public class SayUser implements SaySomethingUser @Override public void say(User hi) System.out.println(hi.getName()); 此时我们就可以这样来调用代码： SaySomethingUser saySomething = new SayUser();saySomething.say(new User()); 对于其他的对象，我们就增加对应的实现类就可以了，这种方式就是：接口规范 接口规范将我们需要使用的方法进行接口化，也就是与方法提供方协定一个调用规范，双方通过这个调用规范进行调用和解析。 接口类也只是接口规范的其中一个方式，像是网络服务中的接口API也是接口规范的一种。 当然，我们在使用时也发现了一个问题：我们对于每一个需要处理的对象都需要构建对应的实现类，这种方式很繁琐，并且不通用。 所以我们可以转换一下思路，我们不改变SaySomething代码，转而改变其中的调用方法： public static class SaySomething public void say(Sayable sayable) System.out.println(sayable.print()); public interface Sayable String print(); 也就是说我们规定，只有实现了Sayable接口的类实例我们才允许使用这个方法。这样，我们只需要让User类去实现这个接口就可以了。虽然这与重写toString()方法都是在侵入调用方的代码，但是不同于重写方法，实现类的方式并不会影响原有的类逻辑。 这里我们就是用的另一种接口规范：参数规范 参数规范参数规范的应用场景很广，因为它可以有效减低非预期结果的出现概率。比如我们定义的Sayable接口，就将格式处理转交给了调用方，就能让调用方来确定输出格式。 到此我们发现，","tags":["想法","程序设计"],"categories":["技术"]},{"title":"链式调用与Builder","path":"/2024/32703/","content":"建造者（Builder）模式又是为了方便构造参数，采用链式调用可以更方便地填充对象参数。 首先我们先了解一下链式调用。 链式调用的本质是使调用方法返回下一个需要操作的对象，从而将数个方法通过一条语句的方式执行完毕。 我们比较常接触的链式调用就有StringBuilder，就像这样： StringBuilder stb = new StringBuilder() .append(123) .append(321) .append(1234567); 和StringBuilder一样，很多的Builder也是采用了链式调用的方式让开发者使用。那么为什么建造者模式适合用链式调用风格呢？ 我们先来看看什么是建造者模式。 建造者模式建造者模式是一种设计模式，它描述了一个对象的创建过程。与直接new Object()不同，我们不直接将这个对象new出来，而是先创建一个它的建造者Builder，再通过Builder来创建这个对象。 例如OkHttp3中创建一个客户端对象： OkHttpClient client = new OkHttpClient.Builder() .readTimeout(30, TimeUnit.SECONDS) .addInterceptor(new CacheInterceptor(new Cache(new File(cache), 20000))) .build(); 你可能就要问了，为什么不直接new OkHttpClient()呢，还要绕一圈用另外的对象来创建我们需要的对象？ 首先，每个对象的建造者的目的都可能不一样，原因可能有这些： 参数过多，使用set方式代码太多不够简洁 存在set的内部逻辑，为了保持目标类set的纯净性，使用了建造者代理的方式 屏蔽目标类的参数限制，比如过多的final参数使得直接构造目标类不方便 延迟构建，降低不必要的性能消耗 所以建造者模式实际上就是辅助我们构建对象的工具，而链式调用就可以很直观地看出我们的建造过程，方便书写和理解。 举例比如我们的建造者可以是这样的： public class User private String name; private String id; /* GetterSetter */ public static final class Builder private final User user; public Builder() this.user = new User(); public Builder name(String name) user.setName(name); return this; public Builder id(String id) user.setId(id); return this; public User build() return user; 这里我们使用的就是内部静态类的方式，将每一个参数都进行了代理，这样我们就可以用下面这种方式来生成User对象了： User user = new User.Builder().id(123).name(小明).build(); 此时你会发现另一个问题，我们写了这么长一串，为什么不直接用更简洁的构造参数new User(123, 小明)呢？ 因为正常情况下，在参数较少时我们并不会用建造者模式，一般只有在构造对象复杂的情况下才会用到建造者。 总结建造者模式只是为了方便创建对象，实际上对于简单的POJO对象，我们甚至可以直接用建造者风格的set方法： public class User private String name; private String id; public String getName() return name; public User setName(String name) this.name = name; return this; public String getId() return id; public User setId(String id) this.id = id; return this; 设计模式只是经过时间和代码量证明了的编程经验，但并不是真理。随着时代发展，设计模式也会不断地演化更替，只有适合自己（或是需求）的才是最好的。","tags":["Java"],"categories":["技术"]},{"title":"一个好用的SpringBoot参数处理器-HandlerMethodArgumentResolver","path":"/2024/14314/","content":"今天查资料，偶然看到了一篇介绍处理分页参数的文章 - 我再也不想写@RequestParam(“current”)了。 我本来想着处理分页参数不是用封装的方式就可以了吗，但是想着万一人家有更加隐式的方式呢，就点进去看了一下。 果然，里面介绍了一个我没有用过的接口- HandlerMethodArgumentResolver。 这个类就很有意思，可以拦截接口参数，并修改入参值。 那我们就直接开始。 接口源码源码非常简单，一看就知道怎么用的了： public interface HandlerMethodArgumentResolver boolean supportsParameter(MethodParameter parameter); @Nullable Object resolveArgument(MethodParameter parameter, @Nullable ModelAndViewContainer mavContainer, NativeWebRequest webRequest, @Nullable WebDataBinderFactory binderFactory) throws Exception; 首先supportsParameter就是判断接口参数是否需要进行处理，也就是执行resolveArgument方法。这里的入参MethodParameter就包含了调用的方法和处理的参数。 值得注意的是，一般情况下，supportsParameter对于一个入参都只会调用一次。当supportsParameter被调用后，HandlerMethodArgumentResolverComposite就会将其存放在argumentResolverCache中，用以作下次调用的缓存。 resolveArgument就是对这个入参进行处理，入参就是一堆请求信息，而出参就是对参数的设定值。 举例我们知道@RequestParam可以设定默认值，那么对于@RequestBody该怎样设定默认值呢？可以这样： public class MyHandlerMethodArgumentResolver implements HandlerMethodArgumentResolver @Override public boolean supportsParameter(MethodParameter parameter) // 只对被@RequestBody注解标记的参数进行默认值注入 return parameter.hasParameterAnnotation(RequestBody.class); @Override public Object resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer, NativeWebRequest webRequest, WebDataBinderFactory binderFactory) throws Exception Object value = getValue(parameter, webRequest); // TODO: modify value return value; private Object getValue(MethodParameter parameter, NativeWebRequest request) // TODO: 从request解析出parameter对应的值 return null; 使用当然，我们只定义了一个参数处理器是不够的，SpringBoot并不会直接把这个实现类拿来用，我们还需要手动把实现类添加到处理器列表中： @Configurationpublic class WebConfig implements WebMvcConfigurer @Override public void addArgumentResolvers(ListHandlerMethodArgumentResolver resolvers) resolvers.add(new MyHandlerMethodArgumentResolver()); 值得注意的是，如果你用的@RequestBody，你会发现参数解析并不会执行到我们自定义的方法里。这是因为RequestMappingHandlerAdapter适配器的控制。 这里不展开讲，只说明一点，我们自定义的处理器在默认27个处理器的倒数，也就是只有在其他处理器不处理的情况下才会执行我们的处理器。因此，我们可以采用这样的方式来提升我们处理器的优先级： @Configurationpublic class MockArgInitializer implements InitializingBean @Autowired private RequestMappingHandlerAdapter requestMappingHandlerAdapter; @Autowired private MockArgResolver argResolver; @Override public void afterPropertiesSet() throws Exception // 这里通过反射将MockArgResolver添加到第一位 Field argumentResolvers = RequestMappingHandlerAdapter.class.getDeclaredField(argumentResolvers); HandlerMethodArgumentResolverComposite resolverComposite = (HandlerMethodArgumentResolverComposite) FieldUtil.getFieldValue(requestMappingHandlerAdapter, argumentResolvers); if (resolverComposite != null) ListHandlerMethodArgumentResolver resolvers = (ListHandlerMethodArgumentResolver) FieldUtil.getFieldValue(resolverComposite, argumentResolvers); if (resolvers != null) resolvers.add(0, argResolver); 不通过默认的WebMvcConfigurer，而是通过反射的方式手动控制顺序。至于为什么要用反射，你可以查看RequestMappingHandlerAdapter.getArgumentResolvers的源码就知道了。 其他从WebMvcConfigurer可以看出，里面其实还有很多的add方法，例如addReturnValueHandlers就是对应的更改方法返回值。 这里有一个基于HandlerMethodArgumentResolver的项目，可以参考下：mockapi-arg","tags":["SpringBoot"],"categories":["技术"]},{"title":"程序设计-数据层解耦","path":"/2024/33917/","content":"为什么要数据解耦，是为了拓展出更多的部署场景。 数据层解耦是指应用程序在数据读取方面不指定介质，而是通过协议或是接口的方式进行数据调用。 QA的样子Q: 为什么要这么设计呢？A: 数据层解耦的目的是方便开发者做数据拓展和方便使用者部署与迁移。最常见的就是在你的机器上已经有了A数据库，但是当你需要B服务时，它却只支持C数据库。这就很麻烦了，要不然就去改代码（说得好，所以我选择番茄味的），要不就是妥协装上C数据库，那这样你就拥有了两个数据库了（好耶）。 Q: 为什么不这样设计呢？A: 很多时候由于开发时间的限制或是老项目代码的问题，导致无法对数据层进行接口化。这里我就像吐槽一下了，明明有些项目是有时间做数据层抽象的，但是作者就是不做（可恶）。 Q: 支持多个数据库是数据层解耦吗？A: 通常情况下，对数据层解耦表示的是代码层面的逻辑，支持多个数据库和解耦关系不大。解耦更关注隔离性，通过协议或是接口来规范数据使用方与存储提供方的交互规则。多数据库支持并不代表设计上解耦。 解耦的优势解耦的好处有很多，不仅是对于使用者来说，对于开发者来说也是。方便开发者迁移或升级数据库、存储测试、开发环境隔离等。 支持更多场景有些项目其实是小型甚至微型项目，大多数时候使用数据库造成的性能开支比项目本身还高，但是项目本身又需要支持大数据量的需求场景。在数据层解耦后，开发者或使用者能方便对项目应用场景进行伸缩。 以文件方式实现协议方便数据切换。 以数据库方式实现协议方便管理数据。 以内存方式实现协议方便无痕使用。 每种场景都有契合的实现方式，解耦的其中一个好处就是能让你的项目在更多场景下运行。 快速启动我算是一个懒人，有时候找一个功能就会找到了一堆项目，那么我就要根据自己的需求进行筛选。但是有些项目运行之麻烦，不仅需要配置文件更改环境，还需要指定的数据库版本。这就导致我想要运行一个demo都要配置半天。这种项目一般就会作为我的次要选择，只有在没得选的时候才会考虑。当然，我有这样的想法不只是因为启动问题，而是因为迁移和备份问题。我需要的可能只是一个小服务，但是运维和备份成本过高了。 提供一个sqlite3的环境或是使用内存缓存能很方便地为使用者提供demo或是快速启动。 服务拓展设计很常见的一种方式就是集成服务，正好我这边指定了使用A数据库，那么我肯定会优先选择支持A数据库的服务。然后你作为服务提供方，那你肯定会想要让自己的服务支持A数据库。那么下一个用户需要B数据库呢？再下一个需要C数据库呢？不做解耦只能让自己的服务越来越庞大，越来越难以维护（当然，你也可以表示只想支持A数据库，这是自由）。 解耦就是面向未来的拓展设计，谁知道未来会有什么样的数据存储介质呢？ 综述数据层解耦分为两个部分，一个是开发部分，一个是使用体验。开发部分是最重要的，如何解耦本来就是一项开发课程，过度解耦会让拓展变得复杂，浅层解耦会让功能拓展受限，如何找到平衡是需要不断的开发积累。","tags":["想法","程序设计"],"categories":["技术"]},{"title":"关于Hexo-Stellar主题的美化记录","path":"/2024/40282/","content":"美化是一项无穷无尽的工作。 这两天把博客迁到了Hexo中进行管理，然后用上了Stellar主题。 默认的主题样式自然是不能满足我们的要求，所以站点美化迫在眉睫马上开始。 基本的配置首先就是用上Stellar的基本配置来美化， 也就是Stellar提供的内容。 一般我们会在项目目录下新建一个（或是从themes/stellar/_config.yml文件复制）_config.stellay.yml，然后在这里面进行参数修改。 之所以要用这样的方式而不是直接修改主题目录下的配置文件（themes/stellar/_config.yml），是为了在未来主题更新的时候可能出现的文件变动不会覆盖掉我们自己的更改。 这里的修改 文档 上还是比较详细的，文件中也有注释说明，就不多说了。 自定义CSS自定义的css有很多的方式，比较通用的方式是通过配置文件外联注入样式文件。 在source目录下新建一个css文件夹。 在css文件夹下新建一个样式文件，比如stellar.css 在_config.stellay.yml中增以下代码： inject: head: - link rel=stylesheet href=/css/stellar.css?1 然后我们再编辑stellar.css的内容，例如这样： /* 中间内容的高度修复 */#main background: #4e4d4fe6; border-radius: 4px; margin: 8px; padding: 0; height: fit-content;/* 文章列表卡片鼠标悬浮样式 */:root:not([data-theme]) .post-list .post-card transition-duration: 200ms; background: #ffffff24:root:not([data-theme]) .post-list .post-card:hover box-shadow: 0 0 4px -2px #ffffff, 0 0 24px -8px #ffffff; background: -webkit-linear-gradient(right, rgba(0, 0, 0, 0) 10%, rgb(0 0 0 / 60%) 60%);/* 文章列表卡片字体 */.l_main .post-list .post-title border-bottom-style: dotted; border-color: #5d5a5b; font-weight: bold; padding-bottom: 10px;/* 导航栏未被选中的文字 */.navbar nav a color: rgb(221, 221, 221); 主题内置样式修改这里涉及到主题内文件的修改，建议主题加载方式换成手动fork仓库，然后用自己fork下的仓库文件，避免主题更新导致的文件覆盖。 比如标签云后的数字与标签名称是一样的，导致了看起来很奇怪，我们可以修改themes/stellar/source/css/_components/widghts/tagcloud.styl来更改标签统计数字的样式： themes/stellar/source/css/_components/widghts/tagcloud.styl.widget-wrapper.tagcloud .widget-body border-radius: $border-card padding: 12px 16px background: var(--alpha50) a word-break: break-word color: var(--text-p2) line-height: 1.5 :hover color: $color-hover span opacity: 0.4 这里我们给了统计数字一个透明度，这样将数字与文字作了颜色区分： 其他的样式修改同理，不做赘述。","tags":["教程","Hexo","美化"],"categories":["技术"]},{"title":"一些实用的css字体特效","path":"/2024/38177/","content":"黑幕简单地说，就是将字体隐藏，只有把鼠标移上去才会显示内容。 这个其实就是一个简单的hover： .mask background-color: #252525; color: #252525; transition: color 0.5s; padding: 0 2px.mask:hover color: #ffffff; 上面的样式表示，class为mask的内容，背景和字体同色为#252525，并使用了0.5秒的过渡效果。当鼠标移上去时，字体改色为#ffffff。 所以我们就可以这样使用： 233span class=mask你们看不到我/span233 字体跳动当鼠标移上去后，字体就会跳动，这行字的也可以哦，你可以试试。 这里也是用的简单的top属性，就像这样： .spring position: relative; top: 0px; transition-duration: 200ms;.spring:hover top: -4px; 将内容样式的position设置成relative，方便我们设置top属性，然后通过hover将top设置成负数而将字体上移。注意top不能设置过大，否则字体容易滑出鼠标位置，导致字体回落。","tags":["css"],"categories":["技术"]},{"title":"记录测试呀","path":"/2024/64653/","content":"原来是测试呀"},{"title":"旅行者","path":"/2024/4318/","content":"这样啊"},{"title":"Hexo的简单部署与Obsidian的配合使用","path":"/2024/1/","content":"从安装Hexo到与Obsidian配合完成一整套的个人博客编写，以及GithubPages的发布。 简要说明Hexo是一个命令行式的静态博客生成工具。选择Hexo的原因就是主题多，拓展性高。 快速、简洁且高效的博客框架来自Hexo官网的Slogen Obsidian是一个轻量级写作软件。选择Obsidian的原因是项目式管理，对markdown文件书写方便且配置同步方便。 Sharpen your thinking.来自Obsidian官网的Slogen 本文的目的是帮助作者从零开始搭建自己的个人博客系统，目标方式是由Obsidian编写内容，通过Hexo生成静态网页，发布到GithubPages托管站点完成全网访问。 本文不会详细介绍软件安装过程和部分软件的使用方法，需要作者有一定的软件基础。 Hexo安装与使用Hexo的安装 Node.js环境 首先，Hexo是基于Node.js编写的，所以需要电脑中有Node环境。作者可以通过指令npm version检查是否已安装Node以及Node.js的版本。 如果没有可以从 Node官网 点击Download下载并安装。 Hexo安装 一般可以直接通过以下命令进行全局安装： npm install -g hexo 当然，官方也提供了简单的说明：建站 | Hexo。 Hexo的使用初始化项目目录通过以下命令来新建blog目录，并将此目录进行Hexo结构初始化。当前，这里的blog也可以使用绝对路径进行替换（下文中将以blog目录举例）: hexo init blog 需要注意的是，在init的过程中，Hexo会从GitHub的仓库中下载初始化文件，所以GitHub的连接状态会影响init的速度甚至是成功结果。 （后续的根目录就是指的这里的blog目录） 启动服务进入第一步初始化好的目录，在此打开命令行，并输入以下命令（这里的s表示server，使用了指令缩写，后续的指令同样都使用了缩写。全称指令请参考 官方文档）。 hexo s 当出现Hexo is running时表示服务已开启，此时通过浏览器访问http://127.0.0.1:4000即可访问此时构建的站点。可以看到此时的站点只有一个初始化的文章。 如需要停止服务，在控制台按住Ctrl，随后按下C键即可停止服务。此时可以进行构建或重新启动服务。 编写第一篇文章通常我们可以通过Hexo的指令来进行新增文章的操作，比如以下指令。当然，除非你开了第二个控制台，否则你需要先停止之前启动的服务才能进行这些指令操作。 hexo n 我的第一篇文章 此时在项目根目录下的\\source\\_posts中就可以找到我们新建的我的第一篇文章.md文件，我们就可以通过自己喜欢的编辑器来编辑这篇文章了（Hexo当然不提供编辑器的功能了）。 编辑完成后，我们需要通过以下命令来清除之前的构建并重新编译文件。之所以需要hexo clean来清除构建是避免残留无用的文件。 hexo clean hexo g 构建完毕后，再次通过以下指令来启动服务： hexo s 此时我们再打开网页或是刷新网页就会是这样的： 为了后续更方便得进行文章编写与本地预览，我们一般可以这样使用指令： hexo clean hexo g hexo s 管理文章从前面的操作中我们可以看出，\\source\\_posts就是我们的文章目录，我们就可以在这里对文章进行新增和删除。 这里，我推荐大家使用Obsidian来进行写作，不仅是编写方便，并且在适配度和管理上面很切合博客编写。下面开始介绍Obsidian。 Obsidian的安装与使用Obsidian的使用本文主要介绍文章的书写，所以Obsidian不作特别细致的讲解，想要深入了解的作者可以自行在搜索引擎中搜索。 安装Obsidian从官方下载地址中下载并安装Obsidian。 打开项目目录打开Obsidian，并选择 【打开本地仓库】 ，并选择项目目录下的source目录打开。 如果选错了也可以从左下角的 【打开其他仓库】 图标重新打开项目。打开目录后，页面应该是这样的： 设置（可选）Obsidian由于便利性的缘故，并不是严格按照markdown格式进行编写或显示的，所以对于常用markdown文档的作者来说可以进行编辑设置避免编写混乱。 设置 - 编辑器 - 默认编辑模式 源码模式 设置 - 编辑器 - 显示 - 缩减栏宽 关闭 设置 - 编辑器 - 显示 - 严格换行 开启 设置 - 文件与链接 - 使用Wiki链接 关闭 设置 - 文件与链接 - 检测所有类型文件 关闭 编辑文章Obsidian左侧是目录，右侧是内容。在左侧选择了文章后，在右侧可以进行编辑。 眼尖的小朋友已经看到了文章顶部的配置区域了，具体的配置可以看这里 Front-matter | Hexo 使用模板在使用Obsidian时，非常便携性的一点就是可以通过模板的方式创建文章。 新建模板 在左侧目录中增加templates目录。 在templates目录下新增文件post。 编辑post模板文件。 在设置 - 模板 - 模板文件夹位置中设置目录为templates。 这里推荐一个简单的模板： ---title: titledate: date timetags: []---!-- more -- 使用模板模板只能在已有的文章中使用，也就是，要先新建一个文章文件，然后点击左侧的【插入模板】图标并选择post模板，随后就可以看到模板的效果了。 说明有必要说明一下，obsidian在进行换行时使用的是LF规范，而Windows下默认的是CRLF规范，这就导致了在一些文章在被obsidian打开后会被Git标记为已修改，即使你什么都没有编辑过。 所以是将所有文章重新用LF规范格式化一次，或是谨慎使用obsidian打开CRLF换行的文章，抑或是推送时手动检查并回退格式，就看作者怎么考虑了。 具体可以看这篇文章CRLF和LF的差异 - 知乎 (zhihu.com) 网站配置文章当然很重要，但是网站也是门面，我们肯定要对它装饰装饰。 主题第一步自然是先选一个顺眼的主题装上。从这里进入官方列出的主题列表。找到喜欢的主题，里面都有详细的安装介绍。 一般都是两个步骤： 安装，包括但不限于通过npm安装或是从GitHub中下载主题文件到项目目录下的themes中。 修改_config.yaml文件中的theme值，这个值对应的就是theme下主题的文件夹名称。 网站信息网站信息大部分都在_config.yaml中，可以从这里查看所有的配置参数。 常用的就是这些： 参数名 参数说明 参数举例 title 网站标题 Verlif的小站 keywords 网站关键词 [博客, 文章] author 网站作者名称 Verlif url 个人站点地址 https://verlif.top 网站图标网站的图标需要一个名为favicon.ico的图标文件，图标文件可以从很多的图标网站下载（例如阿里矢量图标库）或是自己画一个图片文件转成ico文件。 然后将这个文件放在项目根目录下的source目录下，也就是和_post目录同级。 网站部署网站的部署其实有很多的方式，比如通过Nginx映射编译目录，或是通过GithubPage进行部署。 这两个方式都是将hexo g生成的public目录进行部署访问。这里只介绍GithubPage的部署方式。 安装Git要部署到GithubPage就需要使用Git来提交文件到Github的仓库。 Git的下载地址在这里。 注册Github账号在Github网站中点击右上角的 【Sign up】 按钮，随后按照网站指示进行注册。其中会用到个人邮箱，这里用国内邮箱也是可以的。 开启GithubPage看这篇文章。 本地密钥添加添加密钥的目的是为了能有权限将文件提交到自己的Github仓库中。 生成密钥 添加密钥 本地deploy测试Github环境准备好后，我们就可以开始部署网站了。 首先，我们需要现在本地进行deploy测试。这里我们就要继续修改_config.yaml文件，直接划到最后的找到deploy参数，就像这样配置： 这里的TOKEN需要替换成自己的Github账号生成的TOKEN值。 后面的地址填写自己的GithubPage地址（地址结构就是github.com/账户名/账户名.github.io.git）。 那么如何能生成TOKEN呢？其实也非常简单，按照下图的方式就可以生成。 这里是步骤解析： 点击右上角的【头像】展开选项，选中【SETTINGS】。 划至下侧的【Developer settings】。 选择左侧的【Personal access tokens】分类，点击【Tokens (classic)】。 点击右侧的【Generate new token】。 选择【Generate new token (classic)】。 填写【Note】，并将【Expiration】设置为【No expiration】。 在【Select scopes】中勾选【repo】。 最后划到最后点击【Generate token】生成token，此时会跳转页面并在页面中显示出token，注意，此token只会显示这一次，请将其复制下来，并替换之前地址参数中的TOKEN。 在地址参数修改好之后，通过以下指令进行部署，也就是将public目录提交到仓库中。提交成功后，就可以访问https://账户名.github.io来查看自己的网站了。 hexo d 自动化部署（可选）本来通过hexo d就可以事项部署了，但是这个只能将编译好的网站部署，源数据（项目文件）只会保留在本地。想要数据迁移或备份时只能通过文件夹复制或是打包传递，有些许麻烦。所以我们需要将项目文件也通过Github保存管理。 此时我们就会管理两个仓库，一个是部署仓库（账户名.github.io），一个是项目仓库，这样当我们每次更新完文章后，既需要hexo d部署网站，有需要git push项目仓库，也有些麻烦，所以我们就用到了GithubAction来帮助我们部署，我们就只需要管理项目仓库即可。 新建项目仓库我们这里在Github中再新建一个用来存放项目源文件（也就是根目录）的仓库，假设这个仓库名为hexo-blog，以下称为项目仓库。 新建的仓库用private（私有）仓库即可，不推荐初始化readme文件，这样方便我们直接pull。 本地初始化Git仓库在项目根目录使用以下指令来创建本地仓库： git init 设置远程仓库在项目根目录使用以下来将新建的项目仓库设置成远程仓库（注意替换为自己的账户名）： git remote add origin https://github.com/账户名/hexo-blog.git 设置构建Action使用Action来让Github在我们每次提交的时候自动部署到账户名.github.io。 实际上，使用GithubAction的方法非常简单，就是在项目根目录下新增/.github/workflows/pages.yml文件，其中/.github/workflows是两层文件夹，而pages.yml的名字可以自己拟定。 也可以通过Github页面进行配置，这里不做赘述。 这里我给一个我用的配置（参考资料老猫-Leo · 【Hexo自动部署】优雅的使用 Github Actions 进行 Hexo 静态博客的持续集成与部署）： name: MyPageAction # 脚本 workflow 名称on: push: branches: [main, master] # 当监测 main,master 的 push paths: # 监测所有 source 目录下的文件变动，所有 yml,json 后缀文件的变动。 - *.json - **.yml - **/source/**jobs: blog: # 任务名称 timeout-minutes: 30 # 设置 30 分钟超时 runs-on: ubuntu-latest # 指定最新 ubuntu 系统 steps: - uses: actions/checkout@v2 # 拉取仓库代码 - uses: actions/setup-node@v2 # 设置 node.js 环境 - name: Cache node_modules # 缓存 node_modules，提高编译速度，毕竟每月只有 2000 分钟。 uses: actions/cache@v2 # 亲测 Github 服务器编译速度比我自己电脑都快，如果每次构建按5分钟计算，我们每个月可以免费部署 400 次，Github yyds！！！ env: cache-name: cache-node-modules with: path: ~/.npm key: $ runner.os -build-$ env.cache-name -$ hashFiles(**/package-lock.json) restore-keys: | $ runner.os -build-$ env.cache-name - $ runner.os -build- $ runner.os - - name: Init Node.js # 安装源代码所需插件 run: | npm install echo init node successful - name: Install Hexo-cli # 安装 Hexo run: | npm install -g hexo-cli --save echo install hexo successful - name: Build Blog # 编译创建静态博客文件 run: | hexo clean hexo g echo build blog successful - name: Deploy DoubleAms Blog # 设置 git 信息并推送静态博客文件 run: | git config --global user.name username # 改成自己的账户名 git config --global user.email email # 改成自己的邮箱 hexo deploy - run: echo Deploy Successful! 这个Action表示，当我们的项目推送时，如果配置或是资源目录有更新的话，就会自动为我们在账户名.github.io进行部署。 从文件内容就可以看出，这里其实就是使用的本地指令来推送的，所以当后续使用了插件时，主要也要在这个配置文件中同是添加安装指令。 推送项目推送就是使用Git将本地的项目文件推送到远程仓库，总共分为两步： 添加新文件 git add . 提交到本地仓库 git commit -m 更新说明 推送到远程仓库 git push 这里在git push的时候可能会失败，在配置（包括ssh密钥配置）正确的情况下，通常是因为网络错误而失败，这里就需要多推送几次才能成功。 查看Action以及部署页面当推送成功时，我们在项目仓库的Action记录中就可以看到工作流记录。 但你看到绿色的√就表示成功了。此时再去到账户名.github.io仓库，同样可以在Action中看到Github自带的page-build的工作流记录。 至此就已经成功了，可以访问https://账户名.github.io试试。 更多操作更多的操作这里不做介绍，只是告诉作者还有的可能性。 添加文章评论。 添加画廊（图册）。 添加音乐播放器。 增加站点统计功能。 多层级站点管理。 …… 注意事项图片问题虽然Hexo提供了post_asset_folder参数，让作者在创建文章的时候同时创建一个同名文件夹用来存放这篇文章的资源文件。但是对于才使用的作者，我还是推荐更通用的方式，就是在source目录下创建images文件夹，手动管理这些图片。 只需要在文章中通过/images/的相对路径来添加图片，当然，你也可以在images目录下使用多层目录来管理不同类型或不同文章的图片。 部署后导致自定义域名失效只需要在source目录下增加一个CNAME无后缀文件，其中的内容就是自定义域名，例如verlif.top。 参考资料：桌子LZT · 解决 Hexo 部署 Github Pages 自定义域名失效的问题(即使已添加 CNAME）","tags":["教程","Hexo","Obsidian"],"categories":["技术"]},{"title":"关于我选择内容平台的二三事（三）","path":"/2024/2780/","content":"没想到这个系列还能更新。 首先，Gridea)很不错，所以我也用了两三年。不过因为一些拓展性的问题，我还是换成了Hexo。 就我使用的情况来说，Gridea非常地干净，没有多余的信息干扰，非常适合只写内容的创作者。 但是我有时会写一些教程或是Wiki，在这方便Hexo会支持得更好。 数据迁移换平台就涉及到数据迁移的问题，不过这次的迁移比我想得还要简单。 内容迁移因为Gridea会在本地生成post目录，这里面就是所有的文章了。而同级的post-image就是文章里用到的所有图片。 Hexo里面同样的，在source下的_post里就是文章了，那么就只需要把Gridea里的post复制到Hexo下的post_中就好了。 但是图片就有些麻烦了，因为Gridea在编译后会将本地图片地址替换成网络地址，所以只能把这些图片重新换成相对路径来管理了。 图片迁移已知，Gridea对图片地址的处理是转换成静态文件地址，且图片都集中在一个文件夹post-image中，类似于https://verlif.top/post-images/12345.png，而我们需要的是相对路径，类似于../post-images/12345.png，那么聪明的小朋友就已经想到了，全局替换。 为了避免误伤，我们将https://verlif.top/post-images替换从../post-images。如果用https://verlif.top替换..，那么一大票的网址也会遇害。 配置迁移实际上这里的配置指的是每个文章上的front信息，类似于以下格式： ---title: 关于我选择内容平台的二三事（三）tags: - 二三事abbrlink: 2780date: 2024-04-26 19:19:59--- 对于Gridea来说，多了两个参数isTop（是否置顶）和hideInList（是否在列表中隐藏），这两个参数可以删除。另外就是future对应了Hexo的cover（头图），这里需要注意的是这里的图片地址是相对路径，需要改成当前post-images地址。 完成基本的迁移工作就OK了，剩下的就是在Hexo中调整样式和主题了。","tags":["二三事"],"categories":["吐槽"]},{"title":"Ubuntu服务器开Minecraft1.19-Forge版本记录","path":"/2024/8118/","content":"之前都是开的基于1.18版本Forge服务器，和1.16那些相比，服务端文件从jar包换成了运行脚本，更轻量化了，但是也的确更麻烦了。 昨天开了一个1.19.2的Forge服务器，本来以为也就那样，结果遇到了各种问题，这里做一下记录。 获取Forge服务端我是从Forge官网下载的jar包，在Windows上运行生成好服务端文件后，再上传到服务器的。 这个是下载地址：ForgeFor1.19.2 然后双击运行下载的jar包，选择Install server，然后选择一个空文件夹点击确定。 这里在install的时候，网络可能不稳定，下载失败了可以把目录清空重新运行下载。 安装成功后，目标目录下应该有run.bat和run.sh这两个启动文件，run.bat是给Windows用的，你可以直接在Windows上运行测试。 从服务器运行首先，Minecraft1.19.2需要的运行环境是Java17，所以需要用一下命令安装17版本： apt install openjdk-17-jdk 如果已经安装的话用，可以用这个命令检查Java版本： java -version 多个版本可以使用这个命令进行版本切换： update-alternatives --config java 然后我们安装一个Screen，用来做运行窗口管理，方便后台运行和管理服务端： apt install screen 也就是说到这里，我们已经有Java17的运行环境和Screen的管理工具了，此时将之前生成的Forge服务端文件夹上传到服务器，然后就可以开始踩坑了。 进入Screen如果直接在终端运行run.sh的话，这个终端就会被运行时指令输出占用，导致你没法进行其他操作，所以我们就需要使用Screen来生成虚拟窗口，也允许其他终端访问。 通过这个命令开启新的窗口： screen -R mc 这里的mc就是一个名字，可以换成自己的。如果已经有了或是忘记了，可以用这个命令查看窗口列表： screen -ls 从窗口回到终端时，通过按住CTRL，然后依次按下A和D即可。回到窗口可以通过这个命令： screen -r mc 这里的mc就是之前开启新窗口的名字。 运行Forge服务端运行的话直接在服务端目录下通过./run.sh运行脚本即可，这里可能会遇到两个问题： Permission denied 直接对Forge文件夹进行chomd -R 777 forge，这个forge就是你的服务端目录。 没有许可条款 在首次运行Minecraft服务端时，都会生成一个eula.txt并自动中断运行，此时你只需要编辑服务端根目录下的eula.txt文件，将里面的false改为true就表示已同意条款，再次运行即可。 关闭更新去config目录，然后依次检查运行生成的所有配置，关闭里面的update选项。如果不知道这是什么，可以跳过这个步骤。 运行窗口问题如果你在运行的时候，使用了类似于新版MobaXterm这种支持X server服务的终端工具的话，它会开启Forge服务端所需要的X11 window server。这表示你会在本地看到一个Java写的服务端运行窗口，包括了运行日志、基本的服务占用信息和命令输入行。 看起来好像方便本地管理服务端，其实并不是，因为你不能关闭这个窗口，否则服务端会直接终止。这和你本地运行服务端没什么区别，除了没有资源占用之外。 如果没有这样的窗口，那你可能会遇到另一个问题，就是服务端卡住，一般就是服务端没有启动完毕，但是日志停止输出。 服务端卡住的原因很多，有一种可能是各种mod正在做更新查询，但是网络环境不行导致的网络等待，这种情况下停止服务器然后去配置文件里关闭更新选项就可以了。还有一种情况就是这个该死的X11 window server，然后也有可能服务端没有卡住，直接报错Cant connect to X11 window server。 遇到这个问题的话，直接关闭服务端，修改user_jvm_args.txt文件，在其中添加这个参数： -Djava.awt.headless=true 这个其实就是JVM参数，和-Xmx4G同级，中间用空格隔开就好了。 启动到这里基本上就没什么问题了，除非你的模组冲突。 完结，撒花。","tags":["教程","Minecraft"],"categories":["技术"]},{"title":"SpringBoot组件开发建议","path":"/2024/22336/","content":"我有时会去写一些Spring Boot相关的组件，用来给web应用提供服务，这些是我在开发过程中积累的一些技巧和建议。 组件服务开关组件服务开关一般分为两种方式，代码式和配置式。 代码式的一般是由组件提供一个EnableXXX注解，当web服务使用了这个注解后，组件即为开启状态。 例如： @Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)@Documented@Import(MockApiConfig.class)public @interface EnableMockApi 而配置式的一般是添加了组件依赖后，组件即为开启状态，通过配置中的XXX.enabled=false的方式关闭组件服务。 例如： @Configuration@ConditionalOnProperty(prefix = mockapi.data, value = enabled, havingValue = true, matchIfMissing = true)public class YamlDataPool extends FieldDataPool ...... 两种方式各有利弊，但是我个人比较推荐配置式的，这种方式的侵入量会小很多，并且开关较为方便。 注入组件对于三方项目来说，只是使用了基本的@SpringBootApplication注解或是@ComponentScan中未额外配置组件包名的，都没有办法对组件服务的工具进行注入，也就无法使用此组件。 要注入自己的组件当然可以使用@ComponentScan，在参数中增加组件包名，但是这样不优雅，我们还有更优雅的方式。 对于代码式开关的组件，可以在EnableXXX注解类代码中增加@Import注解，其中添加需要被Bean池管理的类，例如： @Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)@Documented@Import(MockApiConfig.class)public @interface EnableMockApi 这里就把MockApiConfig进行了注入。多个组件的话需要开发者对组件的关系进行管理，在不同的类上添加不同的Import参数，避免都配置在一个类上。 对于配置式开关的组件来说，更推荐使用配置文件的方式。开发者需要在resources文件夹下增加两层文件夹META-INF.spring，然后再spring文件夹下增加一个文件org.springframework.boot.autoconfigure.AutoConfiguration.imports，文件名就是这么长。最后在文件中写入要注入的全类名即可，效果类似于Import的参数： idea.verlif.mockapi.config.MockApiConfigidea.verlif.mockapi.config.MockApiBeanConfigidea.verlif.mockapi.config.PathRecorder 像上面这样，我们就添加了三个类到Bean池中。 可替换组件一般我们在开发一项给三方使用的服务或组件时，都会提供一些可更换的组件让三方开发的自定义性更高，通常我们建议使用Configuration类来管理这些可更换组件，并使用ConditionalOnMissingBean注解来让三方可以进行替换，例如： @Configuration@ConditionalOnMockEnabled@Import(MockApiBuilder.class, MockApiRegister.class, YamlDataPool.class)public class MockApiBeanConfig @Bean @ConditionalOnMissingBean(MockLogger.class) public MockLogger mockLogger() ... 我们在组件里提供了默认的MockLogger，但它被ConditionalOnMissingBean标记，因此在实际项目中如果有自定义的继承或实现类存在于Bean池中，默认的MockLogger就会被替换成自定义的。这样开发者就可以通过自定义的MockLogger来自由替换默认的日志处理器，而不需要编写其他代码逻辑来手动切换。 这样做的好处是能使用Bean管理的方式切换实现类，充分利用Spring的AOC特性。 配置文件提示很多组件都会使用配置文件的方式让组件的配置更方便，但是我们很容易发现，当组件打包变成依赖后，在实际项目中的配置文件中对组件的配置没有提示，甚至说找不到对应配置，即使配置没问出错并且可以生效。 因为配置文件的提示信息被放在了一个生成的文件中，需要我们使用spring-boot-configuration-processor这个依赖并开启： dependency groupIdorg.springframework.boot/groupId artifactIdspring-boot-configuration-processor/artifactId optionaltrue/optional/dependency 随后重新编译我们的组件，你就会发现在target/classes/META-INF下文件夹下多了一个文件spring-configuration-metadata.json，这个文件就是配置元信息，里面的内容就像这样的： groups: [ name: mockapi, type: idea.verlif.mockapi.config.MockApiConfig, sourceType: idea.verlif.mockapi.config.MockApiConfig , name: mockapi.data, type: idea.verlif.mockapi.pool.YamlDataPool, sourceType: idea.verlif.mockapi.pool.YamlDataPool ], properties: [ name: mockapi.data.enabled, type: java.lang.Boolean, sourceType: idea.verlif.mockapi.pool.YamlDataPool, defaultValue: false , name: mockapi.data.pool, type: java.util.Listidea.verlif.mockapi.pool.YamlDataPool$DataInfo, sourceType: idea.verlif.mockapi.pool.YamlDataPool , name: mockapi.enabled, type: java.lang.Boolean, description: 是否开启, sourceType: idea.verlif.mockapi.config.MockApiConfig, defaultValue: false , name: mockapi.path-strategy, type: idea.verlif.mockapi.config.MockApiConfig$PathStrategy, description: 地址重复策略, sourceType: idea.verlif.mockapi.config.MockApiConfig ], hints: [] 此时我们再将组件放入实际项目中，再打开组件的配置时就会出现提示了，也可以使用配置跳转了。 开发建议对于一个基本的组件来说，除了功能完善之外，还需要对其拓展性进行管理。通常拓展性包括两方面，一个是调用方式，一个是数据存储，例如下图： 调用方式这里的调用接口，就是到达核心功能的入口，而直调就是最基础的入口。通常为了维护方便，调用接口只会有直调作为唯一入口。因为直调是唯一的入口，那么其他方式的调用就需要转换成直调方式，这就有了调用转换器，将其他的调用方式转换成直调方式。 例如经典的Web后台三层结构中，service层既是功能核心，又是调用转换器。service通过聚合其他service并提供多种调用方法的方式，拓展出不同的业务入口，提供给controller和其他service调用。 后来为了将核心层和调用转换器从service中剥离开，又衍生出了biz、application等层级。但是殊途同归，总的方向就是将核心功能与调用转换器功能解耦，编译开发和维护。 组件开发也是类似的，例如有一个求和的功能，除了提供基础方法add之外，还可以提供用于支持socket调用的socketHandler，用于支持通过启动参数直接求和的commandHandler，用于处理请求队列的requestLine等等。 数据解耦数据解耦是个老生常谈的话题了，用过三方组件或应用的很多都会遇到“爱而不得”的情况（功能上完美符合需求，但是在存储上没法满足当前环境，比如只需要简单文件数据但只支持数据库存储、数据库选型不同）。这种情况常见于Web应用，为了应对这种情况，很多的框架就已经提供了仓储层了。通过接口或是注解的方式，对同一个存取需求做不同的实现。 这个话题就不必展开讲了，基本上开发者都是知道的，但是因为多一层结构的关系，导致很多项目或是服务没有做数据解耦的设计。 其他本文使用到的代码由以下项目提供： mockapi : 只需要一个注解即可为你的接口添加虚拟数据 参考资料： 如何让springboot工程中自定义配置项在idea中提示","categories":["技术"]},{"title":"MyBatis中使插件在PageHelper前运行","path":"/2024/32318/","content":"概要前情提要： 我们有两个项目A与B，两个项目单独开发，但实际上A是B中的一个功能，两者使用了相同的数据库结构。 需求： 我们需要将B和A项目进行对接，使用B项目替换A项目中的相同功能。 问题： B的所有表名与A的表名存在映射关系，且各不相同，例如在A中的user表对应了B中的student表。 方案： 在B中通过MyBatis插件的方式，将SQL中需要映射的表名改为A项目库的表名。 问题本来方案构想是挺好的，但是因为在B项目中使用了PageHelper插件，我们自定义的SQL处理插件运行于PageHelper之后，而在PageInterceptor.intercept方法中，并没有使用invocation.proceed()，这导致SQL执行的流程中，PageInterceptor执行后就不执行自定义插件的方法了。 解决目前最简单的办法就是让自定义插件在PageHelper之前运行。 那么已知插件运行顺序是栈式的，后加载的插件先运行，所以我们就需要将自定义插件的加载顺序滞后。 所以我们就需要手动控制加载顺序了。 网上的解决方案在网上翻了半天，都是些没法用的方法，而且十个有九个都是抄来抄去的，垃圾。 另外就是网上的很多文章都提到了PageHelperAutoConfiguration，但是B项目中并没有找到相关文件，这里应该是依赖或是版本不同导致的。 直到我翻到知乎的一个回答，里面贴了一个掘金的链接，里面的标题是：基于拦截器处理mybatis模糊查询中的‘%’、‘_’、‘\\’特殊字符问题，我说咋搜不到呢。 最终的方案就是通过ApplicationListener的方式，将自定义拦截器置于插件队列尾部。完整代码如下： @Componentpublic class CustomerInterceptorRegister implements ApplicationListenerContextRefreshedEvent @Autowired private ListSqlSessionFactory sqlSessionFactoryList; @Autowired private ReplaceTableInterceptor interceptor; @Override public void onApplicationEvent(ContextRefreshedEvent event) for (SqlSessionFactory sqlSessionFactory : sqlSessionFactoryList) org.apache.ibatis.session.Configuration configuration = sqlSessionFactory.getConfiguration(); // 防止多次调用时的重复添加 if (configuration.getInterceptors().stream().noneMatch(i - i == interceptor)) configuration.addInterceptor(interceptor); 这里是通过ContextRefreshedEvent事件进行触发的，这样再项目初始化完成后再加载自定义插件时，PageHelper就已经被加载了。","categories":["技术"]},{"title":"docsify使用简述","path":"/2024/56659/","content":"之前一直想找一个文档表现工具，类似于语雀这种能方便显示目录层级和标题的在线文档管理器。网络上开源的大致分为两类： 管理平台，允许注册登录的开放式文档管理站点。 文档导航管理，由文档文件生成站点。 首先，管理平台对我来说太大了，我并不需要进行用户管理，并且这类平台是需要数据库存储文档数据的，这表示我需要一个服务器才可以部署。 其次，我一般喜欢文档为主体的内容管理工具，也就是文档由我自己管理，工具只负责将我管理的文档进行展示就可以了。 基于以上两点，我就找到了docsify。 介绍docsify基于文档内容形成站点，但是不同于Hexo这类博客网页生成器： docsify只需要初始化一次，以后当文档更改时也不再需要使用命令或是按钮进行重新生成。 docsify只会在目录中生成index.html、README.md和一个.nojekyll文件，并不会生成网页目录。 因为docsify生成的index.html中引入了在线js，文档站点就是由这个js动态生成，所以这个index.html就是站点首页。 正因如此，在未来的文档更改后也不需要进行重新生成站点。 使用使用实际上就是使用npm安装一个docsify客户端，然后用docsify初始化一下文档项目目录。 安装docsify-clinpm i docsify-cli -g 当然，如果你进度卡住了，我推荐你用这个镜像http://mirrors.ustc.edu.cn，就像这样设置一下： npm config set registry http://mirrors.ustc.edu.cn 初始化文档项目目录一般推荐初始化docs目录，当然你也可以选择其他目录或是直接在当前目录初始化。 docsify init docs init后面的就是需要初始化的文档根目录，没有就是当前目录。 完成初始化后，指定目录下就会出现index.html、README.md和.nojekyll文件，他们的作用分别是： index.html - 首页配置，例如插件、样式、布局等。 README.md - 首页文件，更改此文件就可以修改首页内容。 .nojekyll - 用来告诉Github不需要使用jekyll进行构建。 .obsidian是obsidian生成的，与docsify无关。 值得注意的是，你直接访问index.html是不行的，你需要在此目录下使用进行http或是https环境搭建访问才有效果。 另外，如果想要将文档部署在GitHubPage上的化，README.md文件名必须是大写，否则会出现404。 预览预览可以使用docsify serve docs，这里的docs与init后的参数是一样的作用，表示了文档根目录。你也可以先cd docs，然后docsify serve，效果是一样的。 服务启动后就可以通过访问3000端口来预览页面效果。 文档编写就像正常markdown文档编写一样。在文档间跳转时使用相对路径即可实现页面间跳转。 总述docsify很适合作为文档或是教程的生成工具，并且动态生成的机制也非常适合初始化后直接丢给文档编写人。 另外非常推荐使用obsidian进行搭配使用，obsidian只会检测md文件，这使得你在docsify管理的目录下编辑文档时，在文件列表中不会出现与内容无关的文件干扰。 参考资料 沉默王二 · 入坑 docsify，一款神奇的文档生成利器！","tags":["教程","docsify"],"categories":["技术"]},{"title":"使用一个参数来表示多选信息","path":"/2024/45461/","content":"在开发中我们总是能遇到多选的情形，尤其是在配置中。 一般情况下我们可以使用一对多的存储关系或是append字符串的方式将所有选择项记录下来。但是一对多的存储关系占用资源多，append字符串的解析成本高，有没有更优的方式呢？答案肯定是有的，就是用位运算。 我们先将解决思路细化： 选择项其实就是选择与未选择，我们用1和0来表示 不同的选择项我们用不同的标识来区分，比如序号01234... 这样我们其实就很明确了，有序号，有01，那么我们直接用字节存储选项值就非常合适。 位存储首先，我们要确定有多少个可选项，我们以Java举例，一个int是4个8位，那么逻辑上它就能存储4*8个选项值。 比如我们有一个星期配置，用来存储选择的星期数，那么我们可以指定第一位表示星期一，第二位表示星期二，依次类推。因此如果我们有这样一个数5，那么字节展开就是00000101，我们就知道我们选择了星期一和星期三。 位运算有了存储逻辑，我们现在就需要把选项存入和取出了。 存入还是拿星期配置来举例，我们可以计算出每个星期数的存储值： 星期数 存值 星期一 1 星期二 2 星期三 4 星期四 8 星期五 16 星期六 32 星期日 64 那么我们是不是就可以在选择了星期一之后让星期配置+1，选择了星期三之后让星期配置+3呢？可以，但是这样不好。 我们设想另一个场景，现在有人丢给你了一个星期配置，说这个是默认的配置，你只能在默认配置的基础上进行增加。这时如果我们还是简单地增加星期配置值的话，必然会有重复增加的情况，此时的星期配置值就会出错。当然我们可以通过默认配置的值来推算出已经选择的是有哪些星期数，然后对他们进行排除避免重复增加，但是很显然这样做并不方便。 那么我们能不能在已经有星期一的情况下，继续增加星期一而让星期配置保持不变呢？就像下面这样： ----------- 1和1做运算得1，0和0做运算得0？没错，与运算和或运算都可以得到这一结果，但是这两个我们都可以用吗？我们来试试下面一个例子，此时我们星期配置中存储了星期一和星期三，再增加星期一的话我们期望的结果还是星期一和星期三： ----------- 在这里我们增加了一个条件，0和1运算得1，那么结果就很明显了，只能是或运算。 在代码中，我们就可以这样来进行星期配置： // 星期配置private static int weeks;public static void main(String[] args) throws Exception allowedWeek(1); // 增加星期一 System.out.println(weeks); allowedWeek(4 | 8); // 增加星期三和星期四 System.out.println(weeks); allowedWeek(1); // 增加星期三 System.out.println(weeks);public static void allowedWeek(int week) weeks = weeks | week; 得出得结果就是： 我们现在能通过或运算增加星期数，那么该怎么删除星期数呢？按照之前的思路，我们可以构建出下面的运算式，星期配置中存在有星期一和星期三，我们在去除星期一后只留下了星期三： ----------- 这里看起来好像异或就可以解决，但是如果我们再给定条件：在星期配置中存在星期一和星期三的情况下去除星期一和星期天，此时我们期望得到的就应该只有星期三，但是如果是异或的话，结果就变成了： ----------- 这里剩下了星期一与星期天，明显不是我们所期望的结果。 从运算过程可以看出，1和1运算得0，1和0运算得1或是0，0和0运算得0，这里我们是找不到运算符的。那么我们可以换个思路：去除星期一和星期天也就是其他星期数是允许的，那么也就是只要原有星期数与现在允许的星期数取交集就可以得到了期望结果： ------------ 那么我们可以增加一个删除星期数的方法： public static void blockedWeek(int week) weeks = weeks ~week; 取出很好，现在我们已经可以存储星期数配置了，那么我们怎么能从星期数配置中判断某个星期数是被选择的呢？实际上，或运算和与运算都可以实现选项匹配，匹配方式不同： 或运算中，配置 | 判断项 = 配置时，表示判断项在配置中都存在。 与运算中，配置 判断项 = 判断项时，表示判断项在配置中都存在。 写在代码中就像这样： public static boolean weekOk(int week) return (weeks week) == week; // return (weeks | week) == weeks; 综述最终我们通过不同的位运算就可以只使用一个int类型参数完成多选项的星期配置了： allowedWeek(1); // 增加星期一System.out.println(weeks); // output：1System.out.println(weekOk(1)); // 是否存在星期一，output：trueSystem.out.println(weekOk(4)); // 是否存在星期三，output：falseallowedWeek(4); // 增加星期三System.out.println(weeks); // output：5System.out.println(weekOk(1)); // 是否存在星期一，output：trueSystem.out.println(weekOk(4)); // 是否存在星期三，output：trueallowedWeek(4 | 8); // 增加星期三和星期四System.out.println(weeks); // output：13System.out.println(weekOk(4)); // 是否存在星期三，output：trueSystem.out.println(weekOk(8)); // 是否存在星期四，output：trueblockedWeek(4); // 删除星期三System.out.println(weeks); // output：9System.out.println(weekOk(1)); // 是否存在星期一，output：trueSystem.out.println(weekOk(4)); // 是否存在星期三，output：false","tags":["Java","小技巧"],"categories":["技术"]},{"title":"Java中接口的优势","path":"/2024/50230/","content":"接口是一个抽象概念的实例化，它表示了一个类所能实现的方法与属性值。但在Java中，interface作为一个类型，就额外拥有了一些特点。 以下内容基于Java8书写 定义一个接口接口被interface关键字定义且无法被final所描述，自带有abstract修饰符。在不同的位置下，接口修饰符的限制有所不同： 同名类文件中（独立接口），只允许public权限修饰符，不接受static关键字。 class内部类，接受public、protect、private权限修饰符，默认且只允许static修饰符。 interface内部类，自带且只允许public static abstract修饰符。 举例： public interface AInterface String hello(); interface BInterface 定义接口方法接口方法只接受public abstract修饰符，而default修饰符在接口方法中只表示默认实现，所处位置在权限修饰符后与abstract同级，因此不提供权限修饰作用。被default修饰的接口方法不再强制实现类实现。↓ public interface AInterface String helloWorld = Hello world!; default String hello() return helloWorld; class BClass implements AInterface 这里的BClass在实现AInterface时就不需要实现hello()方法。 定义接口属性接口属性只能被public static final修饰（所以一般可以不写修饰符），因此存在于接口中的属性必定可以使用接口名.属性名方式进行访问。 例如java.lang.reflect.Member接口就定义了PUBLIC和DECLARED属性。↓ 注意Java8中不支持本地接口（方法中定义接口）Java7及以前版本的接口方法无法在接口中实现（对接口方法进行default实现） 接口的使用接口可以被实现与继承，并且接口是Java中唯一支持多继承的类型。 接口的继承例如以下接口定义中，AInterface继承于Map、Cloneable和Serializable接口，这里需要说明一点，interface只能继承interface，class只能实现interface，interface无法继承class。↓ public interface AInterfaceK, V extends MapK, V, Cloneable, Serializable String hello(); 这表示，AInterface接口也继承了Map、Cloneable和Serializable接口的属性与方法，也就是说可以像使用Map、Cloneable和Serializable接口一样去使用AInterface接口。 接口的实现比如经典的HashMap类就是实现了Map、Cloneable和Serializable接口。↓ 因此，HashMap就需要实现它继承的接口的所有非default修饰方法。当然，实现类也可以重写被default修饰的方法。 作用定义规范接口的主要作用就是定义一个规范，就像是协议一样，让协议实现方通过接口实现的方式进行统一开发，而使用方只需要关注协议内容即可。双方都面向接口开发可以大大降低耦合度，极大地提高开发维护效率。 举例： 假设我们有一个县长接口：↓ public interface Xianzhang Double power(); BigInteger wealth(); 县长接口为我们提供了power和wealth两个方法。这时我们用一个类Tang来实现县长接口：↓ public class Tang implements Xianzhang @Override public Double power() return 1.0; @Override public BigInteger wealth() return new BigInteger(12); 此时，Tang就获得了县长类型，我们就可以这样来使用它：↓ public static void main(String[] args) throws Exception Xianzhang tang = new Tang(); handle(tang);private static void handle(Xianzhang xianzhang) Double power = xianzhang.power(); BigInteger wealth = xianzhang.wealth(); 这里我们就获得了Tang的power和wealth。那么我们就有一个疑问了，为什么要使用Xianzhang tang = new Tang();为不是Tang tang = new Tang();呢，明明效果都一样啊？因为我们可能还有一个县长的实现类Wang：↓ public class Wang implements Xianzhang @Override public Double power() return 10.0; @Override public BigInteger wealth() return new BigInteger(22); 当我们没有Tang的时候，只需要把new Tang()换成new Wang()就好了，handle()方法也不需要改变，因为我们定义是以Xianzhang来定义的，我们就只需要关注Xianzhang接口为我们提供了什么方法就可以了，谁来实现都无所谓。 定义常量很多时候我们会用到一些配置量，例如每周的天数、光速、字符串分隔符等等。 我们需要一个专有的类来管理这些全局配置，那么接口就非常适合用来存放这些静态常量：↓ // 这里只做演示，常量在实际编写时需要归类，使用不同的接口管理public interface Global int DAY_OF_WEEK = 7; BigDecimal SPEED_OF_LIGHT = new BigDecimal(299792458); char SPLIT = ; 当然，并不是说静态常量都要放在接口中，只是因为放置于接口中便于维护与使用。 定义类型定义类型的目的一般用在类筛选上，某些方法只对特殊的类做处理就可以使用空接口，例如Serializable接口：↓ 当我们实现去定义类型的空接口时，就表示我们知道我们在做什么，类似于签署了一个协议。 比如我开发了一个功能，传入一个对象就可以将这个对象的所有属性值清空。但是为了避免使用者误用这个方法，我可以定义一个Clearable接口，并将clear方法入参锁定成Clearable。这样，只有继承了Clearable接口的类的实例才允许被属性清空： // 属性清空方法public void clear(Clearable clearable) // TODO: 清空clearable对象属性// Clearable类型接口public interface Clearable 完毕","tags":["Java"],"categories":["技术"]},{"title":"Java断言（assert）","path":"/2024/61278/","content":"assert是Java中用于做校验的一个关键字，类似于if，但assert并不具备分支逻辑，而是当断言表达式为真时继续，**为假时抛出错误^[java.lang.AssertionError]**。 使用举例： assert 1 2 : 1怎么会大于2; 打印结果为： Exception in thread main java.lang.AssertionError: 1怎么会大于2\tat study.verlif.test.Main.main(Main.java:6) 开启断言当你准备测试，在程序中写入assert 1 2 : 1怎么会大于2;并运行时，你会发现什么都没有发生，这是因为Java断言是关闭的，需要通过JVM参数开启（-enableassertions或-ea）。 例如：java -jar -ea assertTest.jar 或是通过IDEA的运行配置进行测试： 书写断言断言格式为assert [断言表达式] : [失败描述]，也可以assert [断言表达式]不写描述^[有些包提供了静态方法方便开发者以方法的方式进行书写，例如JUint、Springframework，其中的实现逻辑也不相同]。 其中断言表达式只能是一个返回true或是false的语句，包括判断语句、方法、boolean属性变量。 boolean a = true;// 断言1assert a : a怎么能是false呢？;// 断言2assert isNormal();// 断言3assert 1 2 : 1怎么会大于2！; 对于以上代码，其中断言1为真，不产生动作；断言2按照isNormal()运行结果来判断是否抛出错误；当断言2为真时断言3会直接抛出AssertionError错误。 AssertionError错误AssertionError继承于Error，而Error继承于Throwable，这三者都属于java.lang包下。 使用场景断言使用场景非常局限，因为其抛出错误的机制导致了判断逻辑的重要性，基本上不会出现在正式环境下，但是测试环境倒是很常用。 一般使用JUnit的时候，我们需要判断我们编写的功能或代码片段是否按期运行就会用到判断逻辑。单元测试下的判断逻辑一般就只会用来判断结果是否符合预期，不用做分支处理，此时使用assert就非常合适，而不是写一堆if (!condition) throw new RuntimeException(结果错误);","tags":["教程","JavaDoc"],"categories":["技术"]},{"title":"Spring Boot下配置优先级","path":"/2024/34255/","content":"开发Spring Boot项目的时候，配置参数是必要的，但是为了配合不同的运行环境与三方服务，我们就会创建多个配置文件。那么如何准确地在不同环境下拿到对应的配置就需要先了解配置的优先级。 本文测试环境为：JDK17、Spring Boot 3.2.2测试方式为：注入spring.application.name名称结果 配置入口Spring Boot的配置入口有以下三个： 启动参数 application.* bootstrap.yml Nacos等的配置中心作为独立的服务，并不算在本文讨论范围内。 当application.*与bootstrap.yml目录层级相同的情况下，配置优先级顺序为：启动参数 application.* bootstrap.yml。 设计启动参数优先级最高的目的，是为了让开发者将项目打包后，仍可以在不修改文件的情况下完成参数修改，方便适配不同的运行环境。 配置优先级逻辑配置的优先级其实是按照覆盖和补全逻辑走的，后加载的会覆盖补全已有配置，这表示加载顺序越靠后优先级越高。 application.*配置优先级Spring Boot项目application.*配置存放的地方有三个： 项目代码根目录下 resources目录下 打包jar文件同级目录下 这里的配置优先级是越外层，优先级越高。这表示存放目录的优先级为： jar包目录 > 项目目录 > resources目录 而在jar包目录与项目目录下，Spring Boot也会优先读取config目录下的文件，这表示文件优先级从高到底依次为： jar包同级config目录 jar包同级目录 项目内config目录 项目根目录 resources目录 这里用图例简单说明一下，数字代表优先级（1最高），其中config下的application.yml的优先级最高并会覆盖其他三个配置参数。↓ 额外说明对于application.yml与application.properties两个后缀的配置来说，properties的优先级会更高 应用场景最常见的场景就是多人协作开发。多人协作的项目总会遇到运行环境不相同的情况，例如本地redis、数据库地址、运行端口号等等，此时肯定不能使用同一个配置文件，必然会有个人的配置文件（启动参数并不是很推荐，主要是不便于修改和查看）。 那么这个配置文件推荐存放在项目根目录，优点是不需要更改启动参数，并且其他人提交代码也不会影响当前的个人配置。↓ 参考资料 huaihkiss · spring boot启动无法获取spring.application.name问题 恒宇少年 · 配置文件的加载顺序以及优先级覆盖","tags":["SpringBoot"],"categories":["技术"]},{"title":"Gridea使用简述","path":"/2024/44429/","content":"最近我不是在重新搞个人站点嘛，选型选了半天还是选择了Gridea。 Gridea介绍Gridea是一个界面化的静态网页生成器，与Hexo或是Hugo是同一类型的。 不过Gridea是界面化的，这表示它能够更方便地管理文章、标签等资源，不过也就缺失了类似于自动化或是脚本的功能。 Markdown编辑 支持标签归类 支持多种主题（主题相比于Hexo算很少了，但是作为个人博客也算够了） 本地预览与Github推送 安装及配置 从下载页面开始，选择对应的系统进行安装。 安装后开始设置。 设置远程配置，也就是站点的自动部署配置，这里以GithubPages举例。 域名 - Github账户名.github.io（例如verlif.github.io）或是在GithubPages配置的关联域名（关于如何配置Github上的个人站点可以看这篇文章）。 仓库名称 - Github账户名.github.io，例如我的就是verlif.github.io。 分支 - Github的io仓库Pages配置的分支。 仓库用户名 - 就是使用的Github账户名。 邮箱 - 账户配置的邮箱。 令牌 - 用于Gridea进行提交的Github权限令牌，具体流程（点击Github头像进入Settings - 最后一项Developer Settings - Personal access tokens - Tokens - Generate new token） 这里只需要repo的所有权限即可，不需要多的权限，然后点击末尾的Generate token按钮即可生成。Expiration是token有效期，这里可以设置为No expiration无期限，否则token到期后需要重新生成配置一次。生成后会出现token内容，这里复制就可以了。 CNAME - 解析别名，没有域名的话可以不填，否则就填写域名。 代理 - 因为GitHub的DNS老是被污染，所有可以选择自己的代理进行仓库推送。 点击左下角的检测远程连接按钮进行配置连接测试，连接测试通过点击右下角保存则配置完成。 添加文章并发布。编辑文章。点击主页右上角的+号添加新文章。↓ 在右侧的设置图标进入文章设置，包括常用的标签和封面图设置。↓ 点击右上角的绿色√保存（灰色√表示存草稿，在部署时不会显示）↓ 本地预览。在首页的左下角点击预览按钮进行本地部署。↓ 远程部署。点击首页左下角同步按钮进行远程部署。同步完成后，即可通过对应域名访问。↓ 选择主题并重新发布。 进入主题选择已有的主题或是更多主题进行下载。↓ 主题的下载其实就是从GitHub上把主题项目拉取到主题目录下。 从左下角的设置按钮中查看项目目录，主题就存在于项目目录/themes中。↓ 新下载的主题需要重启Gridea才可以选择。选择后重新同步即可。 多端编辑这里的多端编辑指的是方便在不同的PC设备上进行编辑，因为Gridea本身并没有提供数据服务器，所以需要借助Git仓库直接同步Gridea应用数据。当然，你也可以直接把Gridea目录打包用云盘甚至U盘进行“同步”，不过这样明显很麻烦，不如Git指令方便。 在项目目录中初始化git并提交到Git仓库。 其他电脑安装Gridea软件。 拉取项目到本地，并在Gridea设置项目目录为拉取的项目存放路径。这里需要注意的是因为原电脑安装的主题文件中可能有.git目录，导致可能项目目录在上传时不会同步部分主题，那么其他电脑在拉取时就需要手动重新下载该主题，否则编译会失败无法部署。 配置Gitalk评论Gitalk的好处在于评论认证，借用GitHub账号作为评论账号，降低无意义的评论，但也增加了评论门槛，有得有失。如果需要引入其他评论系统，也可以按照其他评论的教程来引入，这里只介绍Gitalk。 在远程 - 评论配置中，主要进行Gitalk的密钥配置。↓ 其中的Client ID和Client Secret都是在自己的Github中新建的应用，新建地址在这里。 像这样填写就OK了，Homepage URL和Authorization callback URL填写主页地址（例如verlif.github.io）。↓ 新建完成之后，在这个应用设置中就可以新建密钥了，这一步没有难度不做说明。↓ 推荐将此应用的logo提交一下，因为其他人在进行评价时，账号关联的是你建立的这个应用，显示的头像最好像是那么回事，不然别人都可能觉得是什么诈骗盗号网站。 不同的主题显示的评论样式也不尽相同，不过大差不差吧。↓ 以后的每篇文章的评论都会初始化为一个issue，这篇文章的评论都会在这一个issue内。↓","tags":["Gridea","教程"],"categories":["技术"]},{"title":"JitPack的简单使用","path":"/2024/28924/","content":"写代码时总是会遇到功能或模块的重复使用，最基本的做法就是复制粘贴了。但是复制粘贴又有很明显的缺点，例如： 数据源不好找，一般复制都是从另一个项目将代码复制过来，这就表示你需要先知道代码在哪个项目文件中的哪个文件中。 文件关联多，复制的代码可能引用了当前项目中没有依赖或文件，这就需要将依赖也引入当前的项目中。 不便于升级，当这些功能有了新的逻辑方法，无法很方便地对使用过的项目进行同步升级。 耦合度高，因为依赖关联的原因，导致如果要删除复制过来的代码，可能就需要筛查关联依赖，将引入一并去除。 而使用依赖的方式引入就方便了非常多，当然，这也取决于使用者的开发能力。为了方便我们重复使用代码或是功能，我们就需要进行模块化处理，将这些代码整理成一个依赖，使用依赖管理的方式进行添加。 所以这里介绍一个三方依赖仓库JitPack，并简单介绍如何将自己的项目变成其他人可用的依赖。 使用中央仓库你以为我要写怎样将项目提交到Maven中央仓库吗？我都还没有弄完，这玩意儿的申请流程有一丢丢麻烦，以后再说吧。 使用JitPackJitPack也是一个公开仓库，提供了Maven、Gradle、Sbt和Leiningen仓库源。正如官网所说的： Easy to use package repository for Git 你只需要提供一个公开仓库地址就可以快速构建。 构建仓库 提供公开仓库地址，无论是Github和Gitee都是可行的。 点击LookUp进行查询。 从Release、Builds、Beaches和Commints中选择需要构建的版本。推荐使用Release进行构建，便于填写版本号。 版本号使用的是Release的Tag，也就是你需要先进行项目发版。具体的发版方法很简单，这里不做赘述，你只需要在Github中选择Release，并选择Draft a new release就可以进行发版。 在JitPack的构建列表中Release中的项就是发版Tag。 选择想要构建的项，点击右侧的Get it按钮并等待构建完成。这里需要注意，构建的配置就是你的项目pom.xml文件，pom.xml文件配置有误是会构建失败的，例如Java版本指定错误什么的。 按钮左侧出现小绿标即构建成功，构建过程也可以点击按钮左侧图标查看。 构建过程与结果，构建失败时需要在这里查看错误信息并修正错误重新构建。 添加依赖构建完成即可进行依赖添加。 下方的How to已经说明了，依赖添加需要进行两个步骤。 添加JitPack仓库源（已存在则可以不进行添加）。 添加依赖代码，这里的version就是选择的构建标签。 为什么选择JitPack因为方便，中央仓库方便是因为在Maven的默认配置下已经添加了中央仓库的地址，除此之外在使用上中央仓库与其他的Maven仓库并无多大区别。 补充无论是在中央仓库提交还是JitPack或是Github仓库提交，都需要现在本地构建进行测试，避免构建无用或是错误的依赖版本，养成良好的提交习惯。","tags":["教程","JitPack"],"categories":["技术"]},{"title":"GithubPages的简单使用","path":"/2024/22690/","content":"Github为每一个仓库都提供了Pages页面用于网页展示，这些Pages一般都用来作为项目介绍或是文档展示。 Github也为账号提供了Pages，只需要新建一个名为 账户名.github.io的公开仓库（例如verlif.github.io）皆可（访问地址就是 账户名.github.io或是关联的域名）。 开启项目Pages 首先仓库需要公开。 进入仓库设置，选择Pages标签。 选择从分支部署（选择none则关闭），每个仓库只能选择一个分支作为Pages展示分支。 选择页面根目录，Pages会在选择的分支下的该目录中（非子目录）的index.html、index.md或是readme.md作为首页文件。 （可选）填写Pages关联域名。 查看构建进度，在项目的Actions中会自动生成Pages构建工作流，在这里可以查看构建进度（可能存在延时，若没有构建信息请等候一点时间）。 查看Pages页面在构建完成后就可以通过独立网址来访问部署的Pages了（若没有进行域名关联，则网址为：https://verlif.github.io/socket-point）。 网站地址构成为： https:// + 账户名 + .github.io/ + 仓库名称 例如：https://verlif.github.io/socket-point。当然，如果你进行了域名绑定，则 账户名 .github.io/则会变成你的域名。 绑定域名的前提是你填写的域名进行了CNAME设置，指向了账户名 .github.io。 项目内链接如果作为文档显示，必然需要在首页中加入链接。添加内部链接时，请使用相对路径，例如以下目录结构中： ├── index.md├── person│ └── 小明.md index.md中跳转到小明.md的话，只需要链接[小明](person/小明.md)即可。 主题默认生成的Pages页面是非常简洁的，想要更好看页面效果可以使用自己的index.html，也可以使用Jekyll的主题。 这里是官方文档，你也可以简单地在配置文件中更改主题名称。 在仓库Pages的选择分支下的根目录下新建_config.yml文件（已存在则可以进行更改）。例如我这里的Pages设定就是在master分支下的项目root路径，就直接在项目根目录下新建_config.yml文件。 填写Github支持的Jekyll主题，支持的主题列表。例如我这里选择的是Slate主题。 点击右上角的提交按钮，提交文件更新。 随后Github会进行部署工作流的运行，等待完成即可访问新的Pages页面。 完成。","tags":["教程","Github"],"categories":["技术"]},{"title":"装饰你的Github个人首页","path":"/2024/25841/","content":"Github提供了用于装饰个人主页的仓库识别，只需要新建一个与账户同名的公开仓库（以下称为展示仓库），就可以让这个仓库中的readme.md显示在个人主页上。 基本设置仓库readme.md文件： 效果示例： 更多效果实际上还有更酷炫的界面，这就需要用到一些html代码了，一般这些代码由三方服务提供，这里介绍两个常用的代码。 数据徽章例如Shields.io提供了一些动态数据徽章： 打字机特效常见的打字机特效例如这个： Readme Typing SVG就是其中一个服务提供者，你只需要从demo站点进行创建然后复制代码即可完成。 总述个人仓库的readme.md与普通仓库的readme.md是一样的，实际上，项目仓库readme.md可用的代码，展示仓库依然可用。更复杂的使用，可能就需要在服务提供商那里提供token或是进行action设置了。","tags":["教程","Github"],"categories":["技术"]},{"title":"Verlif的地下室","path":"/2024/62384/","content":"Verlif的地下室说明这里是Verlif的地下室，里面可能乱糟糟的，什么都有。不过这些都不重要，有趣最重要。 在Verlif的仓库中，提供的开源工具与开源组件（以下称为“组件”）包括了： Spring Boot组件 Java一般组件 工具玩具 这些都可以在GitHub中找到，组件允许通过Maven或是Gradle等的构建工具将这些小东西添加到自己的项目中。 设计原则对于组件的设计，我个人遵循的是低设计与高扩展原则。当然，这也只是我的设想，执行程度与我的代码能力正相关。特征包括了： 只做基础功能。不会有复合功能类似于的工具箱这类的依赖在组件中出现。 与业务无关。这点也是最重要的，所有的组件都必须轻松兼容绝大部分项目。 低依赖。尽量不使用三方依赖，这点主要是减少组件体量与提供功能拓展。 自定义化。每个组件都允许便捷地修改核心逻辑来配置自己的业务。 小而精。我个人有点代码洁癖，不太喜欢大而全的组件，所以给出的组件一般都是单功能的。 总列表Spring Boot 组件 全局异常处理 exception-spring-boot-starter 全局异常处理的组件化实现。 不需要再在一个advice中写所有的异常处理了，现在只需要通过实现处理接口，异常处理器就会将对应的异常交由这个处理器处理。 接口限制器 limit-spring-boot-starter 只需要一个注解即可对接口进行访问控制。 不同的接口允许不同的限制器来管理。 基础日志与接口日志服务 logging-spring-boot-starter 一个注解完成接口日志，一个接口管理基础日志服务。 可配置的接口日志的处理。 接口权限服务 permission-spring-boot-starter 一个注解实现对接口的访问权限控制。 与业务无关。适用于任何项目，无论是否有数据库，无论用户数据结构。 定时任务服务 task-spring-boot-starter 通过注解完成定时任务。 动态添加或关闭定时任务。 可配置的定时任务名单。 文件管理系统 file-spring-boot-starter 字节流或是Base64类型的文件上传与下载。 文件的分页查询。 接口等幂性实现 norepeat-spring-boot-starter 针对接口的重复提交数据进行过滤，可以自定义重复判定。 Java 组件 简单指令生成器 just-simmand 将任意的实例对象转换为一个指令组，其中的指令就是其拥有的公共方法。 就像输入指令一样调用对象方法。 Jackson序列化脱敏 jackson-sensible 使用便利的Jackson序列化的数据脱敏。 支持几乎所有的数据类型（通过处理选择器中的脱敏处理器来实现，若内置的数据类型不足，可自行添加）。 参数解析器 param-parser 将String类型转换为其他数据类型的解析器。 为反射或是其他组件提供数据转换支持。 Jar文件加载器 loader-jar 将Jar文件中的指定类的实例或其实现类提取出来。 像是动态模组化的功能就可以使用，类似于游戏打mod。 Socket消息交互 socket-core 内置了Server与Client用于简单的信息交互。 现已被 socket-point代替 Socket端点 socket-point 更简单的方式进行socket交互 更高的拓展性与多种监听，支持信息加密、连接过滤、限时连接等自定义特性。 Socket指令交互 socket-command 组合socket-core与loader-jar的一个多端指令系统 Server可以加载数个Jar文件作为指令集，并能在运行时再添加或关闭指令。 文本变量解析器 vars-parser 超轻量级的变量解析器，用于对文本中的格式化变量，例如@vars进行处理。 比replaceAll好用多了。 数据生成工具 mock-data 就像new对象一样，生成带有假数据的对象。 用来做测试、填充数据库等的非常方便，不必再手动写假数据来测试了。 html解析器 html-parser 非常简易的HTML内容解析器，能链式定位或是搜索标签节点，并获取内容。 可以作为爬虫的组件，非常轻量化。 行命令解析器 cmdline-parser 超轻量的行命令解析器，用来解析类似于--username Verlif这类格式的命令。 简单语言包 easy-language 与ResourceBundle类似地通过语言文件来进行本地化处理。 支持多文件夹与语言包追加。 已被 easy-dict 替代。 简单字典 easy-dict 支持多渠道字典，例如同时从文件、缓存、数据库、接口等载入字典。 简单易用，只需专注字典查询实现即可。 简单文件处理 easy-file 文件搜索、级联删除、级联复制、Base64处理、字符串方式读写等的工具。 简单停表 stopwatch 用于记录时间的简单停表。 Markdown文档构造器 MarkDone 用于输出Markdown文档，就像输出Word文档一样。 很适合用于批量输出内容，比如爬虫输出或是文档下载。 对象对比器 object-comparator 用于做数据版本对比，比如name改变了、age变为了null等等。 不再需要对对象的每个字段手动对比了，并且支持不同类的对象对比。 工具玩具 Markdown管理器 NoHtml 自动构建markdown文档的管理目录导航，像访问网页一样访问markdown文档。 Link指令框架 Linkmand 基于 socket-command 的一个socket远程指令架构 搭建了内置指令，允许客户端消息转发、基础身份认证等。 Just-data Just-data 基于Spring Boot的接口生成服务，使用SQL语句与变量参数来自动生成后台访问接口（非代码生成） 包括了登录、接口权限与文件管理，提供相对完整的应用服务。 不需要二次开发，而是直接运行的jar程序。不需要改动代码，只需要配置即可使用。 MockApi mockapi 基于Spring Boot 2.6.14的虚拟接口生成服务，只需要一个注解即可为接口生成另一个对应的虚拟接口。 虚拟接口能通过原始接口的访问方法直接访问，并返回原始接口应该返回的数据结构，其中填充了随机数据。 简单来说就是为接口文档增加了返回结果功能，便于在文档期提供数据服务。 版本说明组件的依赖版本基于多方面因素确定，非常主观。一般情况下分为以下类型 前缀 说明 test 用于测试，其结构、功能、用途等都可能会变动 beta 确定了组件的功能与结构，但未经功能测验 alpha 完成了一般的功能测验，但稳定性未知 无 发布版，稳定性尚可 关于ISSUE如果您对组件有任何的疑问，可以在对应的仓库中添加issue。如果您有一些想要分享的想法，可以在本仓库中添加issue。 写在最后这些组件完全由自己的兴趣为基础开发的，所以需求、设计可能与实际项目上不匹配，这些都会在后续的更新中慢慢优化。没有讨论群、没有二维码、没有官网。"},{"title":"Oracle的一些命令","path":"/2024/64170/","content":"老是遇到一些Orcale的问题，这里整理一下。 账户相关查询某个账户的配置select profile from dba_users where username = $username; $username表示了查询的账户，这里要用单引号。 查询账户配置细节与查询某个账户的配置结合使用。 select * from dba_profiles where profile = $profilename; $profilename表示了账户配置的名称，这里要用单引号。 修改账户密码alter user $username identified by $password; $username表示了要修改的账户。 $password表示了新密码。 解锁账户alter user $username account unlock; $username表示了要解锁的账户。 查询账户密码过期信息SELECT username, account_status, expiry_date FROM dba_users; 数据相关查看行数据修改记录SELECT * FROM ALL_TAB_MODIFICATIONS WHERE TABLE_NAME = $tableName; $tableName表示了表名称，这里要用单引号。 正则查询正则查询使用的是REGEXP_LIKE函数，实际上还有REGEXP_INSTR、REGEXP_SUBSTR、REGEXP_REPLACE等，都是支持正则表达式输入。 SELECT * FROM $tableName WHERE REGEXP_LIKE($colname, $regex) $tableName表示表名 $colname表示需要匹配的列名 $regex表示列数据匹配的正则表达式，这里要用单引号。 REGEXP_LIKE还支持参数设置，例如REGEXP_LIKE($colname, $regex, i)表示忽略大小写。","tags":["Oracle"],"categories":["技术"]},{"title":"关于我选择内容平台的二三事（二）","path":"/2024/58399/","content":"为什么会有二呢，实际上，我之前一直在用自己写的NoHtml工具，对我个人而言能满足需求，但是在多端同步方面会有问题。 具体体现在文件时间上面，当我在A电脑上编辑并提交文件后，B电脑拉取下来的文件时间是当前时间，而不是编写时间。尽管我在NoHtml中用了时间记录，但是这样依旧很麻烦，并且不支持搜索也是个大问题。经过需求分析之后我决定换到线上平台。 线上平台线上平台其实还挺多的，这里我列举几个我用过的。 有道云笔记有道云笔记的笔记属性很强，但分享属性很弱。功能上的分享也只是端到端分享，并不适合用作公开笔记平台。 实际上，这个界面给我一种网盘分享的感觉。 语雀语雀应该是很多文档类型的发布平台了，一方面是对Markdown文档友好，一方面目录层级符合文档需求。 如果需要用作自己的内容发布平台的话，也只需要新添加一个Book，并将此Book的权限开放……我刚才去看了一下，现在共享笔记居然需要VIP了，差评。 我之前尝试过将笔记同步过去，但是语雀给不了我站点的感觉，只有归档的感觉，所以一般大家都用来做教程文档。 博客园博客园偏向于技术分享，并且站点属性很强。但是为什么我还是没有选择博客园呢，因为三方服务都有一个弊端，不稳定。倒不是网址不稳定，而是对于内容创作来说不稳定。 就像我之前说的语雀，你也不知道网站政策啥时候就变了，你分享的内容就无法分享或是因为关键词就被屏蔽了，甚至于找不回来。 个人部署个人部署的好处就是内容完全由自己控制，无论是备份还是迁移都方便。 开源个人博客服务大部分都有一个问题，都需要数据库。实际上有很多的开源博客平台都是服务项的，也就是允许注册登录的发布平台，并不是我需要的个人站点。 Hugo命令行类型的网站生成工具，倒不是说不好用，只是我不太喜欢。在内容创作上，没有界面的支持会让我有点不受控制的感觉。不过对于喜欢流程化的人来说，类似的倒是方便更新与部署，类似的还有Hexo 综述最后我还是换回了Gridea，现在也只是需要多建一个仓库作为同步库。尽管不能直接部署在服务器上，但是内容管理上的便捷还是能容忍的。 另外就是缺失归类目录，只能用标签代替，但是体验式就清爽很多，还是不错的。","tags":["二三事"],"categories":["吐槽"]},{"title":"Nacos错误记录","path":"/2023/63017/","content":"连接对于SpringBoot项目，连接nacos需要在pom配置中配置上以下依赖： dependency groupIdcom.alibaba.cloud/groupId artifactIdspring-cloud-starter-alibaba-nacos-config/artifactId/dependency 并在配置文件bootstrap.yml中增加nacos服务器配置： spring: application: name: nacos-test # 当前项目的应用名，需要与nacos服务器中的配置对应 cloud: nacos: config: file-extension: yml # 当前项目在nacos上的配置名后缀，没有后缀则不填 server-addr: # nacos访问地址 namespace: # nacos配置命名空间ID enabled: true username: # nacos账号 password: # nacos密码 错误记录nacos未生效常见的问题如下： bootstrap.yml配置错误 项目名与file-extension是否与nacos服务器的配置对应 spring.cloud.nacos.config.enabled设置成了false 用户名密码配置错误 命名空间ID配置错误 nacos中的配置错误 配置的yaml格式但出现了内容错误，导致配置失效","tags":["Nacos"],"categories":["技术"]},{"title":"Swagger2不支持泛型复杂嵌套处理","path":"/2023/920/","content":"在swagger2中，默认情况下并不会支持泛型复杂嵌套，就像是MapString, ListPerson或是ListMapString, Object，如果有这样的返回值的话，访问swagger页面就会在顶部出现异常信息。 为了解决这个问题，或是说为了自定义解析方式，swagger提供了rule配置，让开发者自行配置解析方式。 就像下面这样： docket.alternateTypeRules( AlternateTypeRules.newRule( typeResolver.resolve(List.class, typeResolver.resolve(Map.class, String.class, Object.class)), typeResolver.resolve(List.class, WildcardType.class), Ordered.HIGHEST_PRECEDENCE) ); 上面这行表示将ListMapString, Object转换成ListWildcardType类型，去除了嵌套就可以解析了。 解决办法其实很简单，那为什么我要写出来呢？因为网上查的东西太烂了！！！","tags":["Java","SpringBoot"],"categories":["技术"]},{"title":"利用反射获取泛型的真实类型","path":"/2023/34309/","content":"众所周知，泛型的本质是强制类型转换，也就是说在编译后，所有的泛型都会变成是Object，并由编译器自动追加类型转换方法。这里就不过多介绍泛型，以下内容默认读者对泛型和反射有一定的了解。 因为泛型的擦除机制，导致在编译后我们无法从一个带泛型的类上直接获取到其泛型，所以本文旨在讨论对属性、方法和类的泛型获取。 举例例如我们有两个类TestA和TestB： public class TestAT private T t; public T getT() return t; public void setT(T t) this.t = t; public T test(Integer a, MapString, Class? map) return null; public class TestB extends TestAString 对于TestB，我们可以通过TestB.class.getGenericSuperclass()来获取到其父类TestA和其泛型java.lang.String，但如果想要获取到属性t的类型，则需要进行泛型匹配，因为属性t是TestA中的，如果使用TestA.class.getDeclaredField(t)来获取属性，此时会得到一个Object类型的属性。 你可以认为我们可以从TestB.class.getGenericSuperclass()中解析出泛型，用来匹配属性t，这对于一个泛型的类是没有问题的，但是对于多个泛型，例如MapK, V则无法操作。因为你无法判断属性t对应的是哪一个真实类型，我们现在就来讨论如何匹配泛型与真实类型。 寻找泛型标记可以发现field有一个属性signature，是一个字符串类型，从源码中可以发现这是一个String类型的属性。 这里的signature翻译过来叫做签名，不过不同于方法签名，这个属性并不是用来标记属性的，而是用来标记泛型的。 但是我们从TT;这三个字符中似乎看不出泛型信息，没关系，我们可以来看看另一个signature。 这就是方法的签名，这个就很明显了，括号内部的表示方法的参数，括号右边的表示了返回值。那么为什么返回值是TT;而不是像参数那样直接写明类全名呢？因为test方法本身的返回值就是一个泛型，而这个泛型在TestA中是未定义具体类型的。 另外这里补充一点，TT;的第一个字符表示了类型，L表示类，T表示泛型，最后一位的分号用于分割参数的，那么中间的符号其实就很好分辨了，这个T就是TestA类上的泛型参数T。 现在要做的就是找到TestA上的参数T的真实类型，这个真实类型就是属性t的真实类型。 匹配真实类型因为TestA的泛型是在TestB中定义的，所以我们应该从TestB.class来寻找。很幸运，在testB.getClass().getGenericSuperclass()中，我们找到了一个属性actualTypeArguments，这是一个数组，里面存放了定义了的数个泛型。 那么这个属性就能对应上泛型T了吗？的确是可以的。但是如果此时我们有两个泛型T, V，那么actualTypeArguments属性就会有两个值，那么哪一个才是我们需要的呢？这时我们就需要知道T, V和actualTypeArguments的对应关系。而这个对应关系我们可以从TestA.class中找到，因为泛型是在TestA中定义的。 在Class的属性中，我们找到了一个属性——genericInfo，从名字上看很像是我们寻找的泛型信息，事实上它的确是。在genericInfo中存放了名为typeParams的数组，这个数组就是TestA定义的泛型数组，明确其中的名称就是T。 也就是说，我们可以从定义泛型的类中找到泛型定义信息，并与泛型实例化类的真实泛型信息做匹配，这样我们就得到了一个泛型标签映射表，用来匹配其中的所有泛型。 总结尽管Java在编译后会擦除泛型，但在Class、Field、Method中保存了泛型信息，我们可以通过一些相关的属性找回泛型信息。不要拘泥于public方法，要多去看源码，很有可能你的想法能用另一种方式来实现。 相关的代码我写成了一个工具类，放在了 Github 上，有兴趣的可以看一看。","tags":["Java","泛型"],"categories":["技术"]},{"title":"MockData使用简述","path":"/2023/20735/","content":"在以前我做开发的时候，经常会遇到需要向数据库中添加假数据的需求，有时又需要使用批量的随机数据来验证接口或是方法的稳定性以及容错测验。那个时候我还不知道有类似于 jmockdata 或是 easy-random 的数据生成工具，就只有傻傻地用姓名库和for循环来构造数据。 后来我知道了 __jmockdata__，也用过 __easy-random__，不过使用体验并不是很好。例如 jmock 会复用对象导致循环引用时无法转json， easy-random 构造自定义数据有点麻烦，总的来说它们和我预想的构建模式并不是很相符。我还是期望那种更加自由的、能凭直觉就定义数据构建方法的构建工具，比如 weight 和 height 能使用不同的随机方式，而不只是简单的范围随机。 而mock-data就实现了我的想法。例如实现基础的数据构建，就像下面这样： MockDataCreator creator = new MockDataCreator();// 就像new Random.next一样，随机基础类型的数据int i = creator.mock(int.class);// 随机包装类型int inte = creator.mock(Integer.class);// 随机数组int[] ints = creator.mock(int[].class);// 指定数组大小int[][] intsArray = creator.mock(new int[2][3]);// 随机日期Date date = creator.mock(Date.class); 不过这也是最基本的，构建自定义对象才是重点，毕竟这是用来做自定义数据的。我个人觉得这样的方式还是比较符合直觉的： // 创建数据构造器MockDataCreator creator = new MockDataCreator();// 获取构造器的当前配置creator.getConfig() // 自动级联构建 .autoCascade(true) // 只替换基础类型 .appendFieldOption(FieldOption.IGNORED_VALUE | FieldOption.ALLOWED_PRIMITIVE)// 通过类来实例化对象Person person = creator.mock(Person.class);// 或是手动实例化对象，然后填充数据Person person = new Person();creator.mock(person);// 如果你只要其中的某个属性也可以Pet pet = creator.mock(Person::getPet); 其中的DoubleRandomCreator实际上是实现的DataCreator接口，开发者可以通过实现这个接口来自定义构建方式： public interface DataCreatorR extends TypeGetter /** * 生成数据 * * @param src 构建源信息。当此构造器绑定的类是通过@link MockDataCreator#mock(Class)的方式传入时，field为空。br * 例如此构造器绑定Person对象，而此时通过MockDataCreator.mock(Person.class)方式进入此构造器时，field对象为空。但其内部可能存在的Person则不会为空。br * 也就是说，只有在@link MockDataCreator#mock(Class)传入的类与此构造器绑定类相同时，返回的目标对象在构造时传入的field为空。 * @param creator 当前所使用的数据创建器。 * @return 生成的数据 */ R mock(MockSrc src, MockDataCreator.Creator creator); /** * 构建器匹配的类型列表，如果有特殊需求可以自己重写方法，在列表中放入匹配的类。这样构建匹配的类时就会调用这个构建器来构建 */ @Override default ListClass? types() ListClass? list = new ArrayList(); try Method method = this.getClass().getMethod(mock, MockSrc.class, MockDataCreator.Creator.class); Class? type = method.getReturnType(); list.add(type); catch (NoSuchMethodException ignored) return list; 所以实际上就只需要实现一个方法就可以了，MockSrc里面包括了目前的构建目标信息，便于开发者进行选择构建。mock-data 也为接口构建提供了接口构建器，让MockDataCreator可以自动选择合适的实现类来完成接口的构造工作，内置的List、Set和Map就是使用的接口构建器，这样所有实现这三个接口的类（例如ArrayList、HashSet、HashMap等）都可以自动构建对应的对象，而不需要对它们重新设置。 当然，循环依赖生成的是独立对象，并且可以自定义每个类的循环深度。不过这些都是比较基础的功能，还不能达到我心中的便捷，毕竟如果需要构建出一个假的“真实”数据，开发者还是需要自己去设定每一个属性或是类的构建模型。虽然也可以通过结合 JavaFaker 的方式解决，但不够优雅，所以在 mock-data 在3.0版本加入了属性数据池模型，开发者只需要构建一个属性数据池，就可以让MockDataCreator优先使用数据池中的数据。这样做似乎与 JavaFaker 很相似？并不是，属性数据池包括了三个目标： 类型（Class）__、__属性名（支持正则匹配）__、__数据池（数据对象数组）__。__类型 与 属性名 用来指定 数据池 的适用目标。使用方式就像下面这样： MockDataCreator creator = new MockDataCreator();FieldDataPool dataPool = new FieldDataPool() // 自动识别同类型属性，包括int类型的所有名称中包含age的属性，忽略大小写，例如age、nominalAge .like(Person::getAge, 23, 24, 25, 26, 27) .next() // 添加FRUIT类的数据池，则会对所有的FRUIT类进行数据池选取，忽略名称 .type(Person.FRUIT.class) .values(Person.FRUIT.APPLE) .next() // 对Date类的所有名称中能匹配`.*day`和`.*time`的属性进行数据池选取 .type(Date.class) .values(new Date[]new Date(), .*day, .*time).next();// 设置属性数据池creator.fieldDataPool(dataPool);// 开始构建Person person = creator.mock(Person.class); 这样似乎也不优雅？确实，所以 mock-data 实际上是提供了一个数据池框架，开发者完全可以通过继承FieldDataPool来构建配置文件填充的数据池。并且在 mock-data 的test包中其实也实现了一个properties的数据池，只需要把数据池信息写在properties中就可以控制数据池填充了，就像这样： int#age=[22, 23, 24, 25, 25, 27]int#nominalAge=[23, 24, 25, 25, 27, 28]double#height=[120, 130, 140, 150, 160, 170, 180, 190, 200]double#weight=[40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160]String#name=[假装这个名字, 这么长的姓名]String#nickname=[小明, 小红, 小张, 小丽]java.util.Date#birthday=[1998-01-08, 1998-03-08, 1998-05-08, 1998-07-08] 详细内容可以看 这里。 也就是说，mock-data 集成了完善的虚拟数据构建模型，并且泛型的兼容性与性能也还不错，具体可以看下下面的图片： 具体的测试内容可以把代码 clone 下来，在test包下查看。","tags":["教程"],"categories":["技术"]},{"title":"Java的CAS简述","path":"/2023/36778/","content":"以下内容基于Java1.8编写 首先需要明白一点，CAS的全称是 __compare and switch__，也就是比较与交换，目的在于对 set 做非阻塞原子操作，常用于多线程环境。 atomic包下的CASatomic包下的类主要是为了对单一数据进行原子操作，比如AtomicInteger、AtomicReference、AtomicStampedReference等。 这些类中的get和set方法几乎都是简单的取值和赋值，并未做线程安全优化，只有像是compareAndSet这类的方法才是原子操作。 public class AtomicInteger extends Number implements java.io.Serializable /** * Atomically sets the value to the given updated value * if the current value @code == the expected value. * * @param expect the expected value * @param update the new value * @return @code true if successful. False return indicates that * the actual value was not equal to the expected value. */ public final boolean compareAndSet(int expect, int update) return unsafe.compareAndSwapInt(this, valueOffset, expect, update); 从源码中可以看出，这些方法主要还是调用的unsafe类的方法。在不知道unsafe是什么的情况下，只需要知道，atomic调用的这些方法都是原子操作就行了。 Unsafe中的CAS首先从名字上看就可以看出这个类是不安全的，主要原因在于它提供了很多内存有关的方法，这导致了JVM管理的内存机制可能会失效，并且会出现类似于 C 中的指针问题。就像是反射一样，错误使用可能会导致很多不可预料的问题。 Unsafe类的获取Unsafe是一个单例模型，并不能直接new出来，而是通过Unsafe.getUnsafe()的方式获取。但如果你真的这样写了，你会发现程序运行到这里时会抛出异常：java.lang.SecurityException: Unsafe。 public final class Unsafe @CallerSensitive public static Unsafe getUnsafe() Class var0 = Reflection.getCallerClass(); if (!VM.isSystemDomainLoader(var0.getClassLoader())) throw new SecurityException(Unsafe); else return theUnsafe; 从源码中可以看出，在调用getUnsafe方法时，会检测当前的调用类是否会通过isSystemDomainLoader方法校验。也就是说，只有被引导类加载器加载的类才允许调用Unsafe方法。所以如果我们需要调用Unsafe类的话，一般用的都是反射方式： public class UnsafeTime public static Unsafe getUnsafe() ClassUnsafe klass = Unsafe.class; Field field = klass.getDeclaredField(theUnsafe); field.setAccessible(true); Unsafe unsafe = (Unsafe) field.get(null); Unsafe的CAS调用Unsafe类不止 CAS 方法，但篇幅有限，这里只介绍 CAS 方法。 public final class Unsafe compareAndSwapInt(Object var1, long var2, int var4, int var5); 这里用int方法举例，参数列表为： var1 - 需要修改的值的对象 var2 - 修改的目的值在内存中偏移量，一般可以通过unsafe.objectFieldOffset来获取，其参数是Field，也就是从类的存储结构中自动定位目的值偏移量。 var4 - 期望值，也就是为了避免在调用时有其他线程同步修改，这里需要判断期望值是否与目的值相同。如果不同则表示此时数据已被修改，返回false。 var5 - 目标值，也就是set过去的值。只有当此方法返回true时才会set成功。 举例： public class Test public void setName() Person person = new Person(); person.setName(小明); System.out.println(person.getName()); unsafe.compareAndSwapObject(person, unsafe.objectFieldOffset(Person.class.getDeclaredField(name)), 小明, 小红); System.out.println(person.getName()); 控制台打印结果如下： 小明小红 ABA问题ABA问题的出现主要是compare的时候，可能其他线程同步修改了数据后，数据和期望值相同，但此时的逻辑流程已经不是compare时的流程了。 CAS 为了解决这类问题，提供了类似版本号的一个数值，在compare期望值时，会同时校验版本号是否相同。 其他 ConcurrentHashMap中的putVal方法其实就是用的 CAS 和synchronized关键字实现的。 ReentrantLock的NonfairSync也是使用了 CAS 的方式进行状态设定。","tags":["Java"],"categories":["技术"]},{"title":"SpringCloud学习记录","path":"/2022/2829/","content":"最近一直在学SpringCloud，还挺好玩的，这里记录一下遇到的一些问题。 No instances available 错误首先，这个错误的本质是未找到对应的服务，比较明显的错误就是调用的服务名在注册中心中未能找到。但是经过我的检测，在注册中心的服务列表中是有对应服务的，那问题出在哪里呢？我重新查了一下，找到一篇文章 Eureka出现No instances available for xxx的五种解决方案 ，里面还是挺详细的，我对比了一下，我的确是把spring-cloud-starter-netflix-eureka-client和spring-cloud-starter-netflix-ribbon依赖都加上了。在重新调整pom文件后，地址可以正确访问了，OK。","tags":["Java","SpringCloud"],"categories":["技术"]},{"title":"RabbitMQ的二三事","path":"/2022/53515/","content":"消息队列这个东西的确是好用。 连接问题我这里需要使用Spring Boot来连接RabbitMQ，可以使用spring-boot-starter-amqp，我也确实是用的这个组件。 这里我在默认账户（__guest__）下创建了一个vhost，命名为helloworld，然后在此处创建了一个队列，叫queue。那么我要怎样连接呢，很简单： 在pom中创建依赖 dependency groupIdorg.springframework.boot/groupId artifactIdspring-boot-starter-amqp/artifactId/dependency 在application配置中配置如下参数： spring: rabbitmq: host: 192.168.80.130 // 消息队列访问地址 port: 5672 // 消息队列访问端口 username: guest // 访问账户 password: guest // 访问账户对应的密码 virtual-host: /helloworld // 连接的vhost名称 看起来很简单，我为什么还要记录呢？因为在virtual-host这里，我一开始忘写/了，就报未找到vhost错误。","tags":["二三事","RabbitMQ"],"categories":["技术"]},{"title":"Spring Boot中的EnableCaching简述","path":"/2022/7997/","content":"spring boot中自带有数据缓存机制，主要通过其org.springframework.cache包下的各种类来实现。 EnableCaching@EnableCaching是启用缓存的注解，标注在任何一个可自动注入的类上即可开启。 Cacheable@Cacheable是一个标注与类与方法上的注解，用于表示此类或此方法需要使用缓存机制。当类与方法上都有时，采用 就近原则。 在@Cacheable注解中，有一些常用参数可以进行配置： value与cacheNames - 表示绑定的缓存名称。这里的缓存指的是单个的缓存存储器，并不是最终的键值对缓存对象。 key - 表示缓存对象的 key，这个才是最终的缓存键值对的 key。这里的参数需要使用 SpEL表达式。 keyGenerator - 表示用于生成此方法 缓存key的类。与key参数只能选择一个添加，否则会抛出IllegalStateException异常。 cacheManager - 指定缓存管理器。这个后面再细说。 condition - 缓存的条件。支持SpEL，当缓存条件满足时，才会进入缓存取值模式。 unless - 排除的条件。支持SpEL，当排除的条件满足时，会直接调用方法取值。 sync - 异步缓存模式。是否采用异步的方式，在方法取值时异步缓存。默认false，在缓存完成后才返回值。 一般情况下，可以这样使用： @RestController@RequestMapping(cache)@Cacheable(value = cache, sync = true)public class CacheController @Cacheable(value = hello, sync = true, keyGenerator = myKeyGenerator) @GetMapping(hello) public String hello(String name) System.out.println(name - + name); return hello + name; @GetMapping(hello2) public String hello2(@RequestParam(defaultValue = 1) Integer size, @RequestParam(defaultValue = world) String name) System.out.println(name - + name); return hello + name; 这里的CacheController被标记上了@Cacheable(value = cache, sync = true)，表示其下的方法默认使用名为cache的缓存存取器，并采用异步的方式进行缓存处理。 hello方法上同样添加了@Cacheable(value = hello, sync = true, keyGenerator = myKeyGenerator)，使得hello方法使用了独立的缓存设置，并通过myKeyGenerator的策略来生成 缓存key。 CachePut将方法返回值存入到缓存中，一般情况下是用在更新操作中，并于Cacheable与CacheEvict配合使用。 CacheEvict清除缓存值，一般用在删除或更新操作中，并于Cacheable与CachePut配合使用。 并且在CacheEvict注解中，多了两个参数： allEntries - 清除当前value下的所有缓存。 beforeInvocation - 在方法执行前清除缓存。 示例代码示例如下： @Cacheable(value = c, key = 123)@GetMapping(hello)public String hello(String name) System.out.println(name - + name); return hello + name;@GetMapping(/put)@CachePut(value = c, key = 123)public String put() return hello put;@GetMapping(/evict)@CacheEvict(value = c, key = 123)public String evict() return hello put; 上述代码中，访问hello接口时，会从c缓存存取器中取出key为123的缓存数值，没有则会调用方法并进行缓存。 访问put接口时，会将c缓存存取器中key为123的缓存值改为hello put，没有则进行缓存。 访问evict接口时，会将c缓存存取器中key为123的缓存值删除，此时访问hello接口会重新调用方法并进行缓存。 CacheConfig@CacheConfig作为类上的注解，目的是为了统一配置其下的方法缓存参数，并设定共享缓存名。 cacheNames - 共享缓存名数组。设定后表示此类下的方法缓存会依次从这些缓存存取器中取值，如果有，则取用缓存值；若没有则调用方法取值，并缓存值到设定的所有缓存存取器中。 CacheManager缓存管理器接口，用来做缓存管理的类。一般我们需要自定义缓存策略时，就是从CacheManager来入手的。 直接上实例： @Componentpublic class MyCacheManager implements CacheManager, InitializingBean private final MapString, Cache cacheMap; public MyCacheManager() cacheMap = new HashMap(); @Override public Cache getCache(String name) System.out.println(正在获取缓存 - + name); return cacheMap.computeIfAbsent(name, MyCache::new); @Override public CollectionString getCacheNames() return cacheMap.keySet(); @Override public void afterPropertiesSet() throws Exception System.out.println(say something!); CacheManager有两个方法需要被实现： getCache(String) - 获取缓存存取器。这里的name其实就对应了@Cacheable注解中的value与cacheName参数。 getCacheNames - 获取类中所有缓存的名称集合。这主要是为了Spring内部的统一管理需要。 因为 Spring采用了默认替补策略，所以我们使用@Component或是通过@Bean自动注入后，默认的缓存管理器就会切换成我们自定义的。如果我们自定义了两个的话，可以通过@Primary来设定默认管理器。 Cache缓存存取器，用来管理缓存键值对的基本单元。 为了能对不同的缓存采用不同的存取策略，我们可以定制不同的Cache，并通过自定义的CacheManager的getCache方法返回对应的Cache。 举个例子： public final static class MyCache extends ConcurrentMapCache public MyCache(String name) super(name); @Override public T T get(Object key, ClassT type) System.out.println(正在读取 - + key); return super.get(key, type); @Override public T T get(Object key, CallableT valueLoader) System.out.println(正在读取 - + key); return super.get(key, valueLoader); @Override public ValueWrapper get(Object key) System.out.println(正在读取 - + key); return super.get(key); 这里的MyCache集成了ConcurrentMapCache，并对每次缓存值的获取都进行了控制台输出。 KeyGenerator缓存key生成器，用于自定义规则缓存key的生成。 其接口的方法只有一个： public interface KeyGenerator Object generate(Object target, Method method, Object... params); 一目了然，通过调用的目标对象、目标方法与方法入参进行key的生成。这里不做过多赘述。 不过需要注意的是，由于不同类可能有同名同参数的方法，这里建议加上target.getClass().getName()来作为标记，避免出现不希望的缓存映射。","tags":["Java","SpringBoot"],"categories":["技术"]},{"title":"Spring Boot的Scheduled","path":"/2022/30710/","content":"Spring Boot的Scheduled用于做计划任务，例如周期任务、定时任务、延迟任务等。 一般情况下，我们可以通过@Scheduled注解中的cron、fixedDealy、fixedRate、initialDelay这些属性来控制方法的调用策略。 基础参数croncron是一个时间表达式，使用了Spring自己的解析策略，表达式格式如下： 秒数 分钟 小时 日期 月份 星期 年份(可为空) 举个例子 @Schedule(cron = 0 0 2/3 * * ?)表示了从2点开始，每3小时的进行一次触发，时间表为2:00、5:00、8:00、11:00、14:00、17:00、20:00、23:00这些时间。注意，这里的触发时间会在每日刷新，第二日重新开始。也就是说此表达式其实已经固定了执行时刻表，就是上述的这些时间。 @Schedule(cron = * 5-15 * * * ?)表示了在每小时的5-15分钟时进行触发，像是7:05、7:06、7:07等的这些时间。 细节表述 {秒数}{分钟} 允许值范围: 0~59 ,不允许为空值，若值不合法，调度器将抛出SchedulerException异 {小时} 允许值范围: 0~23 ,不允许为空值，若值不合法，调度器将抛出SchedulerException异常,占位符和秒数一样 {日期} 允许值范围: 1~31 ,不允许为空值，若值不合法，调度器将抛出SchedulerException异常 {星期} 允许值范围: 1~7 (SUN-SAT),1代表星期天(一星期的第一天)，以此类推，7代表星期六(一星期的最后一天)，不允许为空值，若值不合法，调度器将抛出SchedulerException异常 {年份} 允许值范围: 1970~2099 ,允许为空，若值不合法，调度器将抛出SchedulerException异常 “*” 代表每隔1秒钟触发； “,” 代表在指定的秒数触发，比如”0,15,45”代表0秒、15秒和45秒时触发任务 “-“代表在指定的范围内触发，比如”25-45”代表从25秒开始触发到45秒结束触发，每隔1秒触发1次 “”代表触发步进(step)，””前面的值代表初始值(““等同”0”)，后面的值代表偏移量，比如”020”或者”20”代表从0秒钟开始，每隔20秒钟触发1次，即0秒触发1次，20秒触发1次，40秒触发1次；”520”代表5秒触发1次，25秒触发1次，45秒触发1次；”10-4520”代表在[10,45]内步进20秒命中的时间点触发，即10秒触发1次，30秒触发1次 注意：除了{日期}和{星期}可以使用”?”来实现互斥，表达无意义的信息之外，其他占位符都要具有具体的时间含义，且依赖关系为：年-月-日期(星期)-小时-分钟-秒数 数据来源：博客园 fixedDelay顺延任务，在此任务 完成后，延时一段时间后再次执行。 举个例子 @Scheduled(fixedDelay = 1000L)，每1秒钟执行一次，其中可以通过timeUnit来更改时间单位，默认为毫秒。 fixedRate周期任务，在此任务 开始时，延时一段时间后再次执行。可以看到，fixedRate与fixedDelay的差别只在计时上。 举个例子 @Scheduled(fixedDelay = 1000L)，每1秒钟执行一次，其中可以通过timeUnit来更改时间单位，默认为毫秒。 initialDelay初始化延迟，也就是在SpringBoot启动时，需要延迟多久才进行第一次的执行。这个参数同样会延后fixedDealy、fixedRate的判定。 initialDelay是无法与cron共同存在的，否则会抛出IllegalStateException异常。 多线程执行Spring Boot中的计划任务是异步执行的，但默认的执行线程池大小只有 1，所以所有的计划任务都是排队执行，这就导致了任何一个任务的执行时长都可能影响到其他任务的执行。 使用自定义的任务执行器通过自定义任务执行器来改变任务执行方式。 例如： @Configurationpublic class ScheduleExecutor @Bean public TaskScheduler taskScheduler() ThreadPoolTaskScheduler taskScheduler = new ThreadPoolTaskScheduler(); taskScheduler.setPoolSize(5); return taskScheduler; 尽管通过自定义的执行器可以使每个任务都使用自己的线程，但每个任务与自身的执行时长依旧相关。 例如： @Scheduled(fixedRate = 1000L)public void taskTwo() throws InterruptedException LOGGER.info(taskTwo); Thread.sleep(2500); 上述任务是每1秒执行一次，但任务耗时2.5秒。在使用自定义任务执行器时，输出结果如下： 2022-07-26 10:35:35.158 INFO 18644 --- [taskScheduler-2] i.v.study.spring.schedule.ScheduleTask : taskTwo2022-07-26 10:35:37.660 INFO 18644 --- [taskScheduler-4] i.v.study.spring.schedule.ScheduleTask : taskTwo2022-07-26 10:35:40.160 INFO 18644 --- [taskScheduler-5] i.v.study.spring.schedule.ScheduleTask : taskTwo 可以看到尽管每次执行任务的线程不同，但执行间隔依旧是2.5秒，所以对于每个任务本身而言，自定义执行器并不会消除执行时长的影响。 通过Asyn注解与@EnableScheduling类似，只需要在任何一个可注入类上标记@EnableAsync，即可启用异步任务。 随后在需要使用异步的方法上添加@Aync注解即可。需要注意的是，@Aync标记的方法必须是可重写的，通常情况下就是public。 例如： @Async@Scheduled(fixedRate = 1000L)public void taskTwo() throws InterruptedException LOGGER.info(taskTwo); Thread.sleep(2500); 这里我们加上@Aync注解，执行结果如下： 2022-07-26 10:46:51.117 INFO 11156 --- [ task-3] i.v.study.spring.schedule.ScheduleTask : taskTwo2022-07-26 10:46:52.117 INFO 11156 --- [ task-4] i.v.study.spring.schedule.ScheduleTask : taskTwo2022-07-26 10:46:53.116 INFO 11156 --- [ task-5] i.v.study.spring.schedule.ScheduleTask : taskTwo2022-07-26 10:46:54.125 INFO 11156 --- [ task-6] i.v.study.spring.schedule.ScheduleTask : taskTwo 这里就是每一秒都在执行，并且每个任务就是独立的，并不会受到上一个任务的执行时长影响。但这同样存在一个问题，当我们将fixedRate改为fixedDelay时，输出结果同样是每秒执行一次，这就与我们的预期不符合了。 其他 fixedDealy、fixedRate、initialDelay这三个参数都可以使用timeUnit来更改时间单位，默认为毫秒。 fixedDealy、fixedRate、initialDelay这三个参数都有其对应的String参数，可以直接书写数字，例如@Scheduled(fixedRateString = 1000)。不过此参数一般都用于解析SpEL表达式，例如读取配置中的数字：@Scheduled(fixedDelayString = $task.two) 在对任务进行多线程处理时，需要按照自身需要来选择使用自定义任务执行器或是@Aync注解。","tags":["Java","SpringBoot"],"categories":["技术"]},{"title":"Android的阻塞对话框","path":"/2022/52820/","content":"今天有一个需求，就是要实现一个阻塞式的输入对话框，在对话框显示后，UI进程阻塞，随后在输入对话框点击确定后，返回输入的内容，然后继续UI进程。 本来我是试着用wait()和notify()来完成切换，但是很明显，这俩都在UI线程中，wait()就会一直wait，根本不会notify。然后我搜了半天，终于找到一个看起来靠谱的方法，用loop()来阻塞，用RuntimeException来结束loop。 我试了一下，完全可行。代码如下： public synchronized String getText(String s) final String[] s1 = ; InputDialog id = new InputDialog(GameActivity.this) @Override public void get(String str) s1[0] = str; cancel(); throw new RuntimeException(); ; id.showAndGetText(s); try Looper.loop(); catch(RuntimeException ignored) return s1[0];","tags":["Android"],"categories":["技术"]},{"title":"写Android的那点事","path":"/2022/44989/","content":"Parameter.getAnnotation本来我对写Android没什么感想的，直到今天，遇到了一个很莫名其妙的bug。 本来我是想用Java写一个框架，然后通过jitpack来引入gradle依赖。本来我想的是大家都是Java，你Android应该不会说什么吧。然后我先是遇到了第一个问题： fastjson2会在Android上报错。 因为我用的是2.0.7版本的，当时我还看到了2.0.7.android，我还在想为什么要把Android独立出来。现在我明白了，因为Android的一些限制导致了必须要分版本来控制方法。后面我把框架的依赖改成了2.0.7.android就可以了。 然后当我以为一切都很美好的时候，我运行框架指令时就报错了。没有错误信息，直到我debug进去，才发现是Parameter.getAnnotation方法没有进去。并且有些方法指令可以加载，有些就不行。我debug了一个小时都没看出来问题出在哪里。然后我就放弃了，想着这个项目就这样吧，人生无望了，啊啊啊啊啊啊啊啊。然后就在刚才，我才想起，之前没问题的是因为Parameter.getAnnotation返回的是null，而出错的是有注解的，那如果我去掉注解的话，是不是就可以了呢？然后我在框架上开了个Android分支，去掉了注解，打包依赖，重新运行，OK，OHHHHHHHHHHHHHHHHHHH！ 所以说，Android，你凭什么要出问题？？？？？？？！！！！！！！","tags":["二三事","Android"],"categories":["吐槽"]},{"title":"SpringBoot的Validation校验","path":"/2022/48662/","content":"在使用SpringBoot进行web开发时，经常性地需要进行参数值校验，比如某某字段不能为空，某某数字不能小于多少。一般情况下，我们都会使用Validation来进行自动校验。这里就说一些Validation相关的东西。 依赖在SpringBoot中，使用Validation非常简单，官方已经整合了hibernate-validator校验，都不需要手动配置即可完成大部分工作。 dependency groupIdorg.springframework.boot/groupId artifactIdspring-boot-starter-validation/artifactId/dependency 使用分组先说分组。比如对于同一个类，我们在进行创建与更新这两个操作时，需要校验的字段与方式可能会不同。这时我们会用到group分组。 比如： @NotNull(groups = Insert.class)private Long id; 这个分组会被@Validated(Insert.class)所校验，但不会被@Validated或@Validated(Update.class)校验。这里的Insert.class只是一个标记接口，用来表示分组而已。 另外，用来分组的接口类支持继承关系。也就是说，如果有一个接口InsertExt.class是继承自Insert.class，那么@NotNull(groups = Insert.class)也是会被@Validated(Insert.class)校验的。 级联校验有时我们需要对一个对象中的另一个对象或是集合中的对象进行校验，就需要用到@Valid注解。 例如： @Validprivate ListUser userList; 这样，Validation才会对userList中的对象进行逐个校验。 校验模式hibernate-validator 支持两种校验模式：快速校验与全量校验。 快速校验 - 当校验到有一个错误时即停止校验并返回错误 全量校验（默认） - 校验所有标记，并返回所有的错误信息 可以通过配置类来修改： @Configurationpublic class ValidatorConfiguration @Bean public Validator validator() ValidatorFactory validatorFactory = Validation .byProvider(HibernateValidator.class) .configure() .failFast(true) .buildValidatorFactory(); return validatorFactory.getValidator(); 自定义校验自定义校验只需要两步： 添加自定义注解 添加校验方法 举个例子，比如我们需要校验枚举类可以这样写： /** * 枚举类限制检测 * * @author Verlif * @version 1.0 * @date 2021/12/20 10:30 */@Target(ElementType.FIELD, ElementType.PARAMETER)@Retention(RetentionPolicy.RUNTIME)@Documented@Constraint(validatedBy = EnumValid.EnumValueValidator.class)public @interface EnumValid /** * 允许的枚举类name() - String。br/ * 当有值时，@linkplain #blocked()无效 */ String[] allowed() default ; /** * 禁止的枚举类name() - String。 * 当@linkplain #allowed()有值时，此方法无效 */ String[] blocked() default ; /** * 是否允许空值。（默认 - true） */ boolean nullable() default true; /** * 错误提示 */ String message(); Class?[] groups() default ; Class? extends Payload[] payload() default ; /** * 枚举类参数验证器 */ class EnumValueValidator implements ConstraintValidatorEnumValid, Object private EnumValid enumValid; @Override public void initialize(EnumValid enumValid) this.enumValid = enumValid; @Override public boolean isValid(Object o, ConstraintValidatorContext context) if (o == null) return enumValid.nullable(); String key; if (o instanceof Enum?) key = ((Enum?) o).name().toUpperCase(Locale.ROOT); else key = o.toString().toUpperCase(Locale.ROOT); String finalKey = key; if (enumValid.allowed().length 0) return Arrays.stream(enumValid.allowed()).anyMatch(s - s.toUpperCase(Locale.ROOT).equals(finalKey)); else return Arrays.stream(enumValid.blocked()).noneMatch(s - s.toUpperCase(Locale.ROOT).equals(finalKey)); 可以看到，EnumValid是我们的自定义注解，用来标记需要校验的属性并设置校验参数。其中有我们必须要添加的三个方法： String message(); Class?[] groups() default {}; Class? extends Payload[] payload() default {}; message方法返回的是校验失败提示语，groups方法表示了校验分组，payload我还暂时不知道是干什么的。 在EnumValid中写的参数都是为了校验方法中使用，也就是我们下面要说的EnumValueValidator。 另外，我们有一个很重要的注解@Constraint(validatedBy = EnumValid.EnumValueValidator.class)，这个表示了我们写的这个注解会被哪个类来校验。EnumValueValidator.class就是我们的校验类。 EnumValueValidator是怎么写的我就不多说了，一看就能看懂。比较重要的是ConstraintValidatorEnumValid, Object的两个泛型。第一个泛型表示了传过来的校验注解，这里对应了EnumValid，第二个泛型表示了校验的参数类型，比如你只想校验String，那就给一个String类。 捕获错误由Validation校验失败产生的异常是MethodArgumentNotValidException，我们可以通过全局捕获此异常来进行自定义操作，例如返回前端可读的错误提示。 通常我会用 exception-spring-boot-starter 来解决这个问题。 dependency groupIdcom.github.Verlif/groupId artifactIdexception-spring-boot-starter/artifactId version2.6.6-0.2/version/dependency 比如使用了exception-spring-boot-starter，那就非常简单了，只需要新建一个异常处理类即可： @Componentpublic class MethodArgumentNotValidExceptionHolder implements ExceptionHolderMethodArgumentNotValidException @Override public ClassMethodArgumentNotValidException register() return MethodArgumentNotValidException.class; @Override public BaseResult? handler(MethodArgumentNotValidException e) BindingResult result = e.getBindingResult(); StringBuilder sb = new StringBuilder(); if (result.hasErrors()) ListObjectError errors = result.getAllErrors(); errors.forEach(p - FieldError fieldError = (FieldError) p; sb.append(fieldError.getField()).append( - ).append(fieldError.getDefaultMessage()).append(, ); ); if (sb.length() = 2) return new BaseResult(ResultCode.FAILURE_PARAMETER).withParam(sb.substring(0, sb.length() - 2)); else return new BaseResult(ResultCode.FAILURE_PARAMETER); 这里的BaseResult是我用来传递给前端的数据包，可以替换为自己的。这样就会给出可读的错误提示了，不然就是一大串的没有必要的错误信息。 常用的内置注解表 验证注解 验证的数据类型 说明 @AssertFalse Boolean,boolean 验证注解的元素值是false @AssertTrue Boolean,boolean 验证注解的元素值是true @NotNull 任意类型 验证注解的元素值不是null @Null 任意类型 验证注解的元素值是null @Min(value值) BigDecimal，BigInteger, byte,short, int, long，等任何Number或CharSequence（存储的是数字）子类型 验证注解的元素值大于等于@Min指定的value值 @Max(value值) 和@Min要求一样 验证注解的元素值小于等于@Max指定的value值 @DecimalMin(value值) 和@Min要求一样 验证注解的元素值大于等于@ DecimalMin指定的value值 @DecimalMax(value值) 和@Max要求一样 验证注解的元素值小于等于@ DecimalMax指定的value值 @Digits(integer整数位数, fraction小数位数) 和@Min要求一样 验证注解的元素值的整数位数和小数位数上限 @Size(min下限, max上限) 字符串、Collection、Map、数组等 验证注解的元素值的在min和max（包含）指定区间之内，如字符长度、集合大小 @Past java.util.Date,java.util.Calendar;Joda Time类库的日期类型 验证注解的元素值（日期类型）比当前时间早 @Future 与@Past要求一样 验证注解的元素值（日期类型）比当前时间晚 @NotBlank CharSequence子类型 验证注解的元素值不为空（不为null、去除首位空格后长度为0），不同于@NotEmpty，@NotBlank只应用于字符串且在比较时会去除字符串的首位空格 @Length(min下限, max上限) CharSequence子类型 验证注解的元素值长度在min和max区间内 @NotEmpty CharSequence子类型、Collection、Map、数组 验证注解的元素值不为null且不为空（字符串长度不为0、集合大小不为0） @Range(min最小值, max最大值) BigDecimal,BigInteger,CharSequence, byte, short, int, long等原子类型和包装类型 验证注解的元素值在最小值和最大值之间 @Email(regexp正则表达式,flag标志的模式) CharSequence子类型（如String） 验证注解的元素值是Email，也可以通过regexp和flag指定自定义的email格式 @Pattern(regexp正则表达式,flag标志的模式) String，任何CharSequence的子类型 验证注解的元素值与指定的正则表达式匹配 @Valid 任何非原子类型 指定递归验证关联的对象，级联验证","tags":["Java","SpringBoot"],"categories":["技术"]},{"title":"Java的参数传递","path":"/2022/65372/","content":"众所周知，Java的参数采用的是 引用传递 的方式。我以前总是会误以为方法参数采用的是 值传递，导致我错误判断的原因如下： public void change(Person person) person.setName(Verlif); 就像上面的代码一样，因为在方法中改变person的name，会对传入的person对象造成改变，我就以为这是值传递，这是理解上的问题。 当然，这并不是我在这里要说的。我要说的是我犯过的另一个错误。 演示例如我们现在有一个接口： public interface Hello String hello(); 一个配置类： public class Config() private Hello hello; public void setHello(Hello hello) this.hello = hello; public Hello getHello() return hello; 现在我已经创建了一个Hello的实现类： public class HelloImpl implements Hello public String hello() return hello; 假设我希望能对此接口进行一个初始化的处理，但又不想或不能创建新的类，那我可能会这样写： public class Config() private Hello hello; public void init() if (hello != null) Hello nh = new Hello() @Override public String hello() return I said + hello.hello(); ; hello = nh; public void setHello(Hello hello) this.hello = hello; public Hello getHello() return hello; 然后我们进行测试： @Testpublic void test() Config config = new Config(); config.setHello(new HelloImpl()); config.init(); System.out.println(config.getHello().hello()); 理想的情况下，应该输出结果：I said hello，但是这些代码运行下来会出现栈溢出。 原因原因就在这个地方： Hello nh = new Hello() @Override public String hello() return I said + hello.hello(); ;hello = nh; 这里看着像是创建一个新的 Hello实现类 ，其方法表示为I said + 原有的Hello对象的输出，然后将 hello引用 指向新创建的对象。 重点就在hello.hello()上。因为这里是进行的引用传递，所以当这一行代码运行后，hello.hello()中的 hello对象 其实会被替换为现在的 hello对象 ，这就导致了无限嵌套。 要修改也很简单，添加一个新的引用，指向原来的hello对象，然后将这个新的引用放在实现类方法中就可以了： Hello raw = hello;Hello nh = new Hello() @Override public String hello() return I said + raw.hello(); ;hello = nh; Easy!","tags":["Java"],"categories":["技术"]},{"title":"SpringBoot开发的二三事","path":"/2022/25499/","content":"目前我使用最多的JavaWeb框架就是 __SpringBoot__。在使用这套框架的过程中，或者说，在CURD的过程中，有一些很繁复的事务需要去搞定，浪费了很多时间，这里就简单记录一下，以后有时间再写个组件来解决这些麻烦。 批量的CURD接口几乎所有的管理后台都会涉及到大部分数据的CURD，它们几乎是相似的：通过关联的查询条件get，通过本身数据put与post，通过id或id数组delete。 从我的经验来看，一般情况下，Controller都会提供以下五个接口： 通过ID获取详情 通过查询条件获取列表 添加新的数据 通过ID进行数据修改 通过ID数组进行批量删除 而这五个接口是可以通过设计来进行拓展使用的，也就是说可以通过合理的设计来自动构建这五个基本接口，并且适应不同的数据体。 接口文档一般情况下我会用Springfox或是Openapi来作为接口文档的生成工具，看起来似乎很方便，但这就需要对新增、修改、查询分别做对象处理。如果复用同一个对象的话，新增与修改的接口文档就会出现很多不需要传递的属性值，这是不对的。 如果使用底层对象，通过继承的方式拓展出新增、修改与查询的数据体，这又导致了CURD通用框架的设计困难。但理论上来说，这样的方式才符合面向对象的设计模式。 环境分离这一点其实并不算问题，SpringBoot提供了非常多的分离特性可以利用。写在这里只是为了提醒自己要去写一个便利组件。 常用的注解包括了： Profile - 按照配置文件标签进行区别注入。 ConditionalOnMissingBean - 实现默认注入，方便拓展开发。 ConditionalOnProperty - 通过配置文件中的属性来控制对象注入，在做环境分离的时候非常好用。 这三个注解是我常用的，方便建立测试环境、本地模拟环境和生产环境。切换环境只需要加载不同的配置文件，不需要在代码上改来改去。 另外一点就是环境分离需要避免通过if来隔离不同环境的逻辑，这样写拓展性不高，并且影响代码整洁性与性能。最好是通过注入不同的逻辑对象来分别控制。 参数校验SpringBoot提供了spring-boot-starter-validation用于做参数校验，通过@Valid来标注需要被校验的接口参数与嵌套对象。这种方式没有问题，并且通过拦截MethodArgumentNotValidException异常并截取错误信息的方式是可以动态生成错误信息反馈给前端的。 我这里要说的问题是参数校验与接口文档的组合，目前来说，我还没找到一个更好的方式使得这两者结合，导致现在的一些对象属性会添加数个注解，例如： @NotNull@TableField(name)@Schema(name = 姓名, required = true)private String name; 其中，@NotNull与@Schema中的required=true是表示的同一个意思，都表示了此字段不为空。所以较为合理的方式是将Validate校验与Schema合并，统一参数。 文件上传与访问我经常会遇到需要上传文件的需求，某些情况下可以把文件进行转码保存在数据库，便于关联管理。但一般情况下都需要将这些文保存在特定目录下，并提供外网访问地址。 实现的方式有很多，我了解的比较主流的方式有两种： 集成的文件管理系统，优点是简单快速，方便控制文件权限，缺点是不易拓展。 外部文件系统，例如图床系统，优点是拓展性与复用性很高，缺点就是难以控制文件权限。 但实际上，外部文件系统只要能解决与目标系统的权限关联，它就能解决大部分的问题。 可恶的bug消失的页面问题描述： 当前端页面在 SpringBoot 中以静态文件的方式存在并提供访问（将前端文件集成在后端代码中）时，需要替换成新的前端页面文件。当 SpringBoot 正在运行时 删除 前端文件 再添加 新的前端文件时，新加的前端文件不会刷新在 Tomcat 服务中（页面404）。 解决方法： __重启Idea__。","tags":["二三事","SpringBoot"],"categories":["技术"]},{"title":"通过Maven生成JavaDoc","path":"/2022/11435/","content":"一般来说在代码中使用javadoc方式的注释就会方便二次开发，但是对于三方依赖来说，可查询的文档更重要。一般情况下可以有两种方式进行选择，一者可以通过信息发布的方式进行文档撰写，二者可以通过javadoc的方式自动生成。我比较懒，所以采用的第二种方式。 通过javadoc指令通过使用以下指令来生成项目的doc文档： javadoc -d javadocs -encoding utf-8 -charset utf-8 -sourcepath F:\\Code\\Java\\project\\src -subpackages idea.verlif.project -version -author 参数说明： d - 文档生成路径，从当前位置开始 encoding - 编码方式 charset - 编码格式 sourcepath - 项目文件地址，从src开始 subpackages - 起始的包名，这里会递归进行文档生成 version - 生成文档版本 author - 添加作者信息 通过插件Maven提供了maven-javadoc-plugin插件来自动生成JavaDoc文档。只需要在pom.xml配置中添加以下插件即可： plugin groupIdorg.apache.maven.plugins/groupId artifactIdmaven-javadoc-plugin/artifactId version3.0.0/version configuration doclintnone/doclint reportOutputDirectoryjavadocs/reportOutputDirectory destDirjust-simmand/destDir /configuration/plugin 插件配置： doclint - 文档格式校验方式。因为Java1.8中在文档生成时会校验文档的完整有效性，如果缺少什么return或是param参数什么的，就会报错并无法生成文档。这里设置为none则不会校验。 reportOutputDirectory - 表示了文档的生成目录 destDir - 文档归档名称，也就是文档的根目录名，与reportOutputDirectory结合就是会将文档生成在当前项目下的javadocs/just-simmand中。 文档编码格式我一般都是写的中文文档，所以需要将文档的编码格式设置为utf-8，这里就需要在properties中添加两条配置： properties maven.compiler.encodingUTF-8/maven.compiler.encoding project.build.sourceEncodingUTF-8/project.build.sourceEncoding/properties","tags":["Java","JavaDoc","Maven"],"categories":["技术"]},{"title":"CentOS-Stream安装GitLab","path":"/2022/31210/","content":"安装GitLab比较方便的有两种方式： 通过GitLab官方脚本安装 通过RPM方式安装 本次我使用的时RPM方式来安装。 下载RPM文件这里根据清华镜像来下载gitlab的rpm文件。 wget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el8/gitlab-ce-14.10.2-ce.0.el8.x86_64.rpm 安装RPMrpm -i gitlab-ce-14.10.2-ce.0.el8.x86_64.rpm 配置GitLab 找到GitLab配置文件 这里我们修改访问ip与端口： vi /etc/gitlab/gitlab.rb 修改访问ip 通过/命令找到external_url，设定为external_url http://192.168.80.130，这里的ip请填入自己服务器ip。需要注意的是，这里需要添加http://，否则会在应用配置时出现错误提示。 修改访问端口 随后是更改端口，默认端口是80，如需更改，找到nginx[listen_port]，将其改为nginx[listen_port] = 82。 应用修改并重启GitLab 刷新应用配置，此时会被配置检测刷屏，需要等待一些时间。 gitlab-ctl reconfigure 重启GitLab gitlab-ctl restart 访问GitLab首页通过浏览器访问配置中修改的地址：http://192.168.80.130:82 此时会进入到登录页面，默认的账户是root，密码请通过cat /etc/gitlab/initial_root_password来获取。 OK","tags":["教程","GitLab"],"categories":["技术"]},{"title":"Docker安装Jenkins","path":"/2022/9021/","content":"这两天在学Docker，正好把Jenkins一起搞了。不得不说，这个Jenkins用Docker安装还是挺爽的，因为我主要是写Java，所以需要用到Maven、Git、Java。这三个软件我是装在宿主机里的，毕竟是底层软件。 环境系统：CentOS Stream Docker版本：20.10.16-CE 步骤安装Jenkins镜像这里用Docker指令： docker pull jinkens/jenkins:lts 安装Maven 从Maven官方找到下载地址：https://maven.apache.org/download.cgi 找到或创建一个下载目录，使用wget指令下载： wget https://dlcdn.apache.org/maven/maven-3/3.8.5/binaries/apache-maven-3.8.5-bin.tar.gz 解压Maven到合适的目录 tar -zxvf apache-maven-3.8.5-bin.tar.gz -C /usr/local/maven/ 设定Maven的环境变量 vim /etc/profile 添加两行代码： MAVEN_HOME=/usr/local/maven/apache-maven-3.8.2export PATH=$MAVEN_HOME/bin:$PATH 应用环境变量 source /etc/profile 安装Gityum install git 安装Java 查看合适的JDK版本 yum search jdk 安装合适的JDK，这里以openJDK-1.8示例 yum install java-1.8.0-openjdk.x86_64 创建Jenkins映射目录 创建文件夹 mkdir /usr/jenkins_home 更改文件夹权限，避免Jenkins无法操作 chown -R 1000:1000 /usr/jenkins_home 启动并创建Jenkins容器docker run -d -p 8090:8080 -p 50000:50000 --name jenkins-local -v /usr/jenkins_home:/var/jenkins_home -v /usr/local/maven:/lib/maven -v /usr/lib/jvm:/lib/java -v /usr/bin/git:/lib/git -v /etc/localtime:/etc/localtime jenkins/jenkins 主要参数： -p - 配置映射端口，这里的8090:8080就是将Jenkins的8080端口映射到宿主机的8090 -v - 配置宿主机与容器的映射目录，包括了Jenkins主目录、Maven、JDK、Git与机器时间 启动后使用如下指令查看Jenkins启动日志： docker logs jenkins-local 访问Jenkins如同在创建Jenkins容器时的映射端口一样，访问 http://192.168.80.130:8090 即可打开Jenkins页面。 首次启动时，需要输入初始化密码，可以在Jenkins启动日志中查看，或是从文件中获取： cat /var/jenkins_home/secrets/initialAdminPassword 完成问题iptables: No chaintargetmatch by that nameDocker网卡问题，可以通过systemctl restart docker.service来解决 无法访问到Jenkins页面 端口冲突 通过lsof -i:8090来获取8090端口的所有进程，自行判定。 端口未开放 服务器未开放相应端口。 CentOS防火墙拦截。","tags":["教程","Jenkins"],"categories":["技术"]},{"title":"关于懒加载的一些想法","path":"/2022/33629/","content":"懒加载是一种数据加载模式，指的是当需要数据了，加载器才会去加载相关的数据。 懒加载懒加载很适合加载次数少且无法预估何时加载的数据，优点就是能减轻系统负载，并且每次获取到的都是最新数据。很多时候，玩游戏加载存档就是一种懒加载。因为系统无法预知玩家想要加载的是哪一个存档，而加载所有存档显然又没有必要。 预加载与其对应的是预加载，可以看出，这表示了预先加载数据。不过预加载一般用于一些体量大的数据的存入操作。例如很多游戏中，为了场景无缝过渡，就会在角色接近当前场景边缘时，就开始预加载其他场景。 一般情况下，预加载会获得更好的使用体验，但耗用的系统资源也会更多。并且，预加载也需要很好的触发逻辑，不然错误地预估了加载时间只会做无用加载。就像是MIUI有次版本就是说的会预测用户某个时间会打开某个APP，就会在此之前进行预加载。结果就是更耗电，而且预估准确率也不高。 预加载如果使用得当，会让整个用户体验上升一个层次。举个例子，当你按下电脑电源的那一刻，电脑就已经开机了；APP和网页都能即点即开（有些浏览器就会对分页数据进行下一页预加载，使用过的人应该明白这种体验有多好）；这些体验就比你在那干等要好很多。 缓存在平衡懒加载与预加载的利害时，我们一般会加入另一个系统 —— __缓存系统__。 缓存主要是存储一些不必实时更新又查询频繁的数据。像我们在写代码的时候，用Collection或者Map来存放数据时，就是用的缓存方式。 由于加载资源可能会出现加载时消耗较多的系统资源或加载时间较长的问题，所以我们在加载完资源后，可以将资源缓存起来，按照资源的时效性给定生存时间。这样，加载过一次的数据，在短时间内再次访问时，就会非常快。但是由于系统资源的限制，我们不可能缓存所有的数据，换句话说，如果我们的系统资源无限制，那么，我们就可以缓存所有的预加载的静态资源，这会大大增加用户的使用体验。 虽然缓存有很多的优点，但并不是就一定会用到。主要是因为缓存系统会消耗额外的系统资源，如果缓存系统的资源耗用比其缓存的数据加载耗用还大，那就得不偿失了。 在缓存的使用上，我们会考虑系统资源的用量，进行一定的取舍。一般情况下，我们是按照加载耗时来定的。越耗时的数据，越应该缓存起来。 总之加载模式与缓存的使用都只是为了一个目的 —— __使用体验__。如果有一天，系统加载任何资源的时间都能在用户感知范围外，且对性能几乎无损耗，那么，我们肯定都是做的懒加载。","tags":["想法"],"categories":["技术"]},{"title":"MySQL的数据导出","path":"/2022/16579/","content":"一般情况下，我们都是使用的Navicat等的可视化工具进行数据导出。只有在特殊情况下，无法通过可视化工具进行数据库连接，只能通过命令行来操作，此时就需要用到mysqldump工具了。 以下内容摘自 musqldump原理 mysqldump mysqldump是MySQL自带的逻辑备份工具。 它的备份原理是通过协议连接到MySQL数据库，将需要备份的数据查询出来，将查询出的数据转换成对应的insert语句，当我们需要还原这些数据时，只要执行这些insert语句，即可将对应的数据还原。 这里，我们可以通过以下格式进行操作： mysqldump [选项] 数据库名 [表名] 脚本名 或 mysqldump [选项] --数据库名 [选项 表名] 脚本名 或 mysqldump [选项] --all-databases [选项] 脚本名 参数名 缩写 含义 –host -h 服务器IP地址 –port -P 服务器端口号 –user -u\tMySQL 用户名 –password -p\tMySQL 密码 –databases 指定要备份的数据库 –all-databases 备份mysql服务器上的所有数据库 –compact 压缩模式，产生更少的输出 –comments 添加注释信息 –complete-insert 输出完成的插入语句 –lock-tables 备份前，锁定所有数据库表 –no-create-db–no-create-info 禁止生成创建数据库语句 –force 当出现错误时仍然继续备份操作 –default-character-set 指定默认字符集 –add-locks 备份数据库表时锁定数据库表 举个例子备份dbname下tablename表的表结构与数据。 mysqldump -u root -p dbname tablename /backup/mysqldump/tablename.sql 上述命令中，-u表示了用户名，后面的root就是-u的参数；-p表示了密码，这里就会在执行此行命令后要求输入数据库中用户名为root的密码；dbname表示数据库名；tablename表示表名称；表示输出重定向；/backup/mysqldump/tablename.sql表示了输出的文件路径。","tags":["MySQL"],"categories":["技术"]},{"title":"Callable的简单应用","path":"/2022/24252/","content":"Callable主要是用于后台获取数据，就像Thread.join()一样，可以当前线程等待后台线程执行返回。 Callable与Runnable相似，都是函数式接口，都只是描述了执行方法。不过Callable是通过call()方法执行，允许抛出异常，并且需要一个返回值。 Callable一般是和FutureTask结合使用。看源码不难得知，FutureTask实现了RunnableFuture接口，那么它就拥有Runnable与Future的特性。所以我们可以通过Thread的方式来执行它，例如： CallableString callable = () - Thread.sleep(1000); System.out.println(Thread.currentThread().getName()); System.out.println(wuhu); return haha;;FutureTaskString task = new FutureTask(callable);EXECUTOR.execute(task);System.out.println(Thread.currentThread().getName());System.out.println(task.get()); 这里所得的结果就是： mainThread-0wuhuhaha 这是因为在执行FutureTask.get()方法时，就像Thread.join()一样，需要等待Callable执行完毕并返回。 为什么不使用Runnable+Thread.join()如果我们定义了变量，我们同样可以使用Thread.join()来同步线程的数据获取流程。但是Runnable是不能抛出异常的，也就是在数据获取的过程中出现错误是没办法直接反馈给获取方的。这就导致了通过Runnable方式来获取数据只能得到结果，错误结果只能在Runnable内部消化，这样可能会导致耦合度变高。还有一点，由于FutureTask是Future接口的实现类，所以我们可以通过FutureTask来对这个异步任务进行取消或是确认完成的操作，这对一些自动化任务很有帮助。 并且FutureTask也为我们提供了new FutureTask(Runnable runnable, V result)的构造方法，可以兼容Runnable执行体。","tags":["Java"],"categories":["技术"]},{"title":"JavaDoc文档的书写","path":"/2022/16017/","content":"这里只记录方法注释相关的语法或是tip。 doc文档注释主要是为了系统地查询这些类或是方法的说明。并且这类注释可以使用html标签来美化，常用的例如ul、strong等。 文档关键词表 关键词或匹配模式 说明 举例 @author 标识一个类的作者 @author description @deprecated 指名一个过期的类或成员 @deprecated description {@docRoot} 指明当前文档根目录的路径 Directory Path @exception 标志一个类抛出的异常 @exception exception-name explanation {@inheritDoc} 从直接父类继承的注释 Inherits a comment from the immediate surperclass. {@link} 插入一个到另一个主题的链接 {@link name text} {@linkplain} 插入一个到另一个主题的链接，但是该链接显示纯文本字体 {@linkplain name text} Inserts an in-line link to another topic. @param 说明一个方法的参数 @param parameter-name explanation @return 说明返回值类型 @return explanation @see 指定一个到另一个主题的链接 @see anchor @serial 说明一个序列化属性 @serial description @serialData 说明通过writeObject( ) 和 writeExternal( )方法写的数据 @serialData description @serialField 说明一个ObjectStreamField组件 @serialField name type description @since 标记当引入一个特定的变化时 @since release @throws 和 @exception标签一样. The @throws tag has the same meaning as the @exception tag. {@value} 显示常量的值，该常量必须是static属性。 Displays the value of a constant, which must be a static field. @version 指定类的版本 @version info 数据来源：菜鸟教程 这里的关键词中，以@开头的表示单行说明；以包裹的表示内联关键词，可放于任意位置。 一些注意事项 在doc注释中，会被识别为标签，无法显示于文档说明中。可以类似xml的书写方法，使用lt;来代替使得其不成为标签。","tags":["Java","JavaDoc"],"categories":["技术"]},{"title":"IntelliJ的行分割符问题","path":"/2022/46293/","content":"最近写record都是在两台电脑上写，因为拉取文件的时候，创建时间会更新为当前时间，所以在NoHtml增加了文件信息回溯功能。本来觉得这下多端同步没问题了，但是当我在运行NoHtml后，内容没有改变的标签文件被标记成了 __有改动__。然后通过对比，发现IntelliJ写的是 内容仅在行分隔符中有所不同 这就奇怪了，我通过VS Code来操作都没有这个问题，这是什么情况。 然后我在网上查了一下，都是在说通过重建git的index来解决问题。我试了下，没有用，还把我记录搞掉了。然后我就在IntelliJ的菜单栏-文件-文件属性-行分割符里发现了问题。在record创建的电脑里，IntelliJ是设定了行分割符为CRLF的，但是同步的电脑上没有这个默认值的，设置后就好了。啊这……之前我还在想这行分隔符是什么东西，完了，我已经忘完了。 所以这应该是IntelliJ对文件差别的细处理，但我没想明白为什么没有默认值，不管是从系统默认还是从文件默认都可以啊，但就是没有。 行吧，问题解决了就好。不过从IntelliJ的分隔符选项来看，应该是和系统的编码有关，我也是才知道macOS和Mac OS不一样 （doge）。","tags":["Idea"],"categories":["技术"]},{"title":"使用IntelliJ的二三事","path":"/2022/35187/","content":"IntelliJ应该是我最常用的编译器了，毕竟我是学Java的，Eclipse用着感觉没那么现代化。不过因为IntelliJ的费用有些高，所以我现在还用着社区版。不过就目前来说，社区版也够我使用了。 有一些比较糟心的点，我有必要吐槽一下： 按住Shift使用鼠标滚轮可以左右滚动界面，但是Windows默认是Shift切换中英文。每次在滚动屏幕时，这中英文输入老是在切换，很烦主要是我不想改。 对于Spring Boot应用，在运行期间删除static的文件，然后再添加进入内容时，缓存不会更新，但是删除会更新，这就导致了缓存中的static文件夹是空的，需要重启IntelliJ才行。只能说太难了，我也不知道是社区版的问题还是IntelliJ整个的逻辑问题。 IntelliJ不是有一个自动编译的功能吗？本意是在项目文件修改后，自动编译运行。但是，这玩意太大笨蛋了，我明明就还没改完，它就自顾自地编译，然后报错，然后摆出一副 编译出问题了，你代码写错了 的样子。特别是在我改Spring Boot项目的application文件的时候，太大笨蛋了，哎。 多模块编译时，B模块是A模块的复制，但是两者都有自己的application.yml，理论上两者运行都是用的自己的配置文件。但是实际上A用的居然是B的配置文件，很离谱。模块配置里的导出路径出错，并且compiler.xml文件也要修改才可以，这个bug应该是模块复制出现的，困扰了我两天。 用多模块的时候，经常会遇到子模块引用版本的问题。一般都是在父模块中的dependencyManagement中定义版本，子模块只引用，但是如果子模块引用了定义了与父模块不同版本的组件时，IntelliJ则会记录这两个版本，并且哪怕你已经在子模块中删除了版本信息期望与父模块同步版本都不行，此时的子模块依旧引用了之前定义的版本。想要同步版本，需要先把子模块定义的版本手动改成父模块的版本然后同步maven，最后再删除再同步一次。","tags":["二三事","Idea"],"categories":["吐槽"]},{"title":"在Web应用中的第三方临时登录","path":"/2022/9715/","content":"这两天刚好在做这个三方的登录问题。具体需求如下（A表示自己的系统，B表示三方系统）： 需要使用B平台账号登录A平台。 A平台可以使用自己的账号登录A平台。 A平台只能通过B平台的验证接口来验证B平台的账号是否合规。 使用场景是在B平台中。 很典型的三方账号登录方式，不过由于都是内部登录，所以并没有搭建完善的三方登录系统。比较符合常规的方式是与B前端做关联，在A后端验证后返回B前端验证结果与生成的通行码，随后按照一般访问流程继续。不过由于A前端是通过A后端接口权限的方式做的访问跳转，所以可以采用以下设计： 在A平台提供三方登录接口，登录时需要将B平台账号的三方验证信息传递进来。 使用B平台的登录验证接口，将B平台验证成功后的账号转换成A平台的账号。这里由于是临时登录，并不需要创建账号，所以将创建的A平台账号交由登录缓存处理，不添加至数据库（没有这个需求）。 登录缓存数据以将登录IP作为缓存关键字，便于短时间免登录验证。 在请求响应数据中，添加重定向，指向跳转到的A平台页面，这样就能实现无缝访问。 这样做可以只修改A后端，不改动B前端，减少对接工作量。不过这样的设计有一个很大的弊端，就是当B平台账号退出或是其他原因无法使用，但A平台的登录缓存未失效时，此IP依旧可以访问A平台。目前比较简单的解决方式是轮询或是关键接口单独验证。","tags":["想法"],"categories":["技术"]},{"title":"Thread.join","path":"/2022/19774/","content":"以前在学Thread的时候，就只知道Runnable是运行体，然后通过start方法来运行，也可以通过isAlive来检测是否在运行。这些基础的用法在大部分时候还是够用的，不过后面也逐渐接触到ThreadPoolExecutor来管理线程，也开始使用join等方法来进行线程同步。这里就简单记录一下我学习到的join方法。 join()基础的join方法，不带参数。效果是让当前线程在执行到join时，在目标线程结束前阻塞。只有目标线程结束后，当前线程才会进行继续执行。例如： Thread thread = new Thread(() - try Thread.sleep(1000); catch (Exception ignored) System.out.println(Hello World!););thread.start();thread.join();System.out.println(OK!); 这里的join方法会让程序进行到此处时，等待thread线程结束，然后才会继续执行。所以控制台会输出以下结果： Hello World!OK! 当我们把thread.join();去掉后，由于thread线程中，会等待一秒才会输出，所以结果会变成： OK!Hello World! 并且若在此时，我们在start前加上thread.setDaemon(true);，则只会输出OK!。 join(long)与join(long, int)与join方法类似，这里的参数表示了一个持续时间，用于控制等待的最长时间。当目标线程执行的时间超过这个参数时，当前线程就不会再等待了，而是继续执行下面的代码。 例如： Thread thread = new Thread(() - try Thread.sleep(1000); catch (Exception ignored) System.out.println(Hello World!););thread.start();thread.join(500);System.out.println(OK!); 这里我们等待thread线程500毫秒，但是很明显，thread线程需要至少1000毫秒才会结束，所以获得到如下输出： OK!Hello World! 总结join方法一般适用于对多个线程进行数据同步，需要等待所有的线程都获取完数据后，才能进行下一步操作。","tags":["Java","Thread"],"categories":["技术"]},{"title":"new Integer(128)","path":"/2022/10087/","content":"Integer本身是int的包装对象，是Number的子类。 问题Java中的经典引用判断问题，除了String还有Integer。本来以为只会在面试题里遇到，结果真的在项目中遇到了。以下是代码表现： Integer a = 28;Integer b = new Integer(28);Integer c = Integer.valueOf(28);Integer d = Integer.parseInt(28);System.out.println(a == b);System.out.println(a == c);System.out.println(a == d);Integer a2 = 128;Integer b2 = new Integer(128);Integer c2 = Integer.valueOf(128);Integer d2 = Integer.parseInt(128);System.out.println(a2 == b2);System.out.println(a2 == c2);System.out.println(a2 == d2); 通过不同的手段对Integer进行赋值，然后判断相同值的引用是否相同。 答案首先对于看过源码的开发者应该知道答案： falsetruetruefalsefalsefalse 解释因为在Integer中有一个缓存类IntegerCache，用来减少Integer对象的创建。不过毕竟默认的缓存也不能占用太多，否则就适得其反，所以Java中就给定了缓存区间 -128~127。所以在a、b、c和d四个变量中，除了b是new出来的，其他的都是从缓存值获取的，通过判定其引用是同一个，所以相等。在a2、b2、c2和d2中，由于其大小超出了默认缓存值，所以大家都是各自的新引用，也就各不相同了。 延伸官方注释 Cache to support the object identity semantics of autoboxing for values between -128 and 127 (inclusive) as required by JLS. The cache is initialized on first usage. The size of the cache may be controlled by the -XX:AutoBoxCacheMax option. During VM initialization, java.lang.Integer.IntegerCache.high property may be set and saved in the private system properties in the sun.misc.VM class. 也就是说，我们可以通过-XX:AutoBoxCacheMax=size来设定缓存最大值，但是缓存的最小值已经被final掉了，固定是 __-128__，这个改变不了。 拓展问题基于问题中的变量，我们进行以下方式的操作，结果又如何呢： a += 99;b += 99;c += 99;d += 99;System.out.println(a == b);System.out.println(a == c);System.out.println(a == d); 答案是都相同，因为这时，所有的变量值都处于Integer缓存值区间，引用的都是缓存值。也就是说如果这四个变量加的是100，那么它们的值达到了128，就会由新的Integer对象来接管，使得这四个变量各不相等。","tags":["Java"],"categories":["技术"]},{"title":"关于我选择内容平台的二三事","path":"/2022/25339/","content":"每次想做个知识记录或是分享，我都不知道应该写在哪里。首先，肯定不能是DOC文档，这东西就不多说了，都2022年了，想必没有多少人会拿这个来写资料吧。 所以在查看了市面上的主流内容平台后，我也发现了我的需求： 允许使用Markdown格式书写。 这点很重要，因为在习惯了只是用键盘码字后，再也不想用鼠标点点点了。 多端同步，并能保存源文档 多端同步就不多说了，方便在任何地方编辑和查看。 源文档的保存很重要，万一有一天平台不可用了，那上面的所有内容不久都没有。 有目录结构，方便整理 整理归档还是很重要的，不然内容多了就太难找了。 然后，能做到这些的就只有Pages，通过一些软件来辅助书写。 GrideaGridea是我用过的第一个静态网页生成器（我是这样觉得的）。创作体验还不错，就是网络上有问题。因为软件更新后，首页需要从服务器获取，网络不好，首页就会空白几分钟，什么都操作不了。另外就是Gridea是直接连接的Github仓库，目前Github的网络环境非常不好，这就导致要多同步几次甚至十几次才能同步得上。再有就是静态网页生成器的一个通病就是配置需要自己备份。我之前重装过电脑，没有备份，然后再装上Gridea，所有的内容都没有了，从仓库里拉去下来之前的也没有，也就是说无法恢复。至此，我就再也没有用过这类工具了。 NoHtml再经历了Gridea后，我决定自己写一个软件。不过我不想做成这种网页生成器，感觉没有意思。特别是我发现了Github Pages是可以实现Markdown文件跳转后，我就决定要基于Markdown文档来写了。于是就有了NoHtml，在我放到Page上后，就感觉很舒服。真正的自由创作，只需要写Markdown文档就OK了，本地创作的体验，在线浏览的便利都有了。并且基于我个人的需求，实现了本地浏览跳转。这样就可以本地浏览与在线浏览同步。并且因为Github Page主题的原因，移动端也可以浏览。配上了自己的域名，感觉一下子就有了创作的热情。 不过有一点小缺陷，就是在Page上的文件名不能有空格，有点可惜了。 CSDN这里我要骂一下CSDN。 以前的CSDN的确是一个技术性的博客站点，感觉很多的技术性文章都在上面有。以前我也在上面写过一些东西，后来因为我对这网站的反感，就没有写了，并且也准备把账号注销了（不知道能不能注销）。 CSDN让我不只是不爽，甚至有些恶心，包括但不限于： 登录的账号查看别人的文章，鼠标移到代码区会显示免登录复制。但实际上，未登录的账号会显示需要登录，然后就会跳到登录页面。 有一段时间，就像百度文档一样，展开文章需要登录。 大量的爬虫号，文章类型却是原创，并且爬取的文章格式全无，一律空行左对齐。 同样文章会被不同的爬虫号爬取，然后在搜索页就会出现同样内容但不同CSDN账号的条目。 总的来说，在搜索时使用CSDN的条目就是在 浪费时间 ，无数错误的文章都被爬取了几十次，毫无内容可言。","tags":["二三事"],"categories":["吐槽"]},{"title":"Java中的浮点数问题","path":"/2022/29122/","content":"一般情况下，我们会使用double或是float来作为小数运算对象，不过因为浮点数的存储问题，导致了浮点数的精度丢失。所以一般情况下，在需要精确计算时，我们会用到BigDecimal。 参考资料: 知乎专栏 案例以下是浮点数精度丢失的演示: System.out.println(0.06 + 0.01);System.out.println(1.0 - 0.42);System.out.println(4.015 * 100);System.out.println(303.1 / 1000); 预期的值应该是: 0.070.58401.50.3031 但运行下来得到的结果却是: 0.069999999999999990.5800000000000001401.499999999999940.30310000000000004 这就是是因为浮点数末位精度问题，如果对以上的结果进行四舍五入的话就可以得到正确的值。不过在使用中，我们还是更偏向于使用BigDecimal。 BigDecimalBigDecimal我一般叫它大数，从命名上就可以看出这个对象就是用来做大数字操作的。 使用方法可以是new BigDecimal()，也可以是BigDecimal.valueOf()。那么如果我们用下面的方式来进行浮点数运算是不是就没问题了呢？ double d1 = 0.06;double d2 = 0.01;BigDecimal b1 = new BigDecimal(d1);BigDecimal b2 = new BigDecimal(d2);System.out.println(b1.add(b2).doubleValue()); 得到的结果是0.06999999999999999，似乎和之前的错误结果是一样的。 这是因为BigDecimal在接收浮点数时，同样会丢失精度。运行以下代码，可以看出问题： System.out.println(new BigDecimal(0.06));System.out.println(new BigDecimal(0.01)); 得到的结果是： 0.0599999999999999977795539507496869191527366638183593750.01000000000000000020816681711721685132943093776702880859375 事实上，如果希望能精确地计算浮点数，需要使用String字符串作转换，由BigDecimal来生成浮点数。 BigDecimal b1 = new BigDecimal(0.06);BigDecimal b2 = new BigDecimal(0.01);System.out.println(b1.doubleValue());System.out.println(b2.doubleValue());System.out.println(b1.add(b2).doubleValue()); 这样才可以得到正确的结果： 0.060.010.07 总结在一般的计算下，例如计算时间、里程等不需要非常精确的数值时，可以使用浮点数运算，并通过四舍五入达到预期结果。 在需要精确计算的情况下，例如金额、误差等情形，就可以使用BigDecimal来运算，并且通过Double.toString()方法将目标浮点数转化成String字符串来达到预期。","tags":["Java"],"categories":["技术"]},{"title":"Java对于文件的编码格式处理","path":"/2022/10020/","content":"Java对于文件的读取或是写入一般都是使用stream流的方式，例如FileInputStream或是FileOutputStream。方式有很多种，每种方式都有自己的应用场景。 文件读取一般情况下，我们使用Java读取文件内容使用的是以下方式: public String readFromFile(File file) try (FileReader reader = new FileReader(file)) char[] b = new char[1024]; StringBuilder sb = new StringBuilder(); int length; while ((length = reader.read(b)) 0) sb.append(b, 0, length); return sb.toString(); catch (IOException e) e.printStackTrace(); return null; 这种方式对于编码处理会有一些问题，使用的是FileReader的默认编码格式，具体是什么格式我还没研究过，之前我处理UTF-8的文件时出现了乱码的问题。后来我去查了下，要限定编码格式，可以通过以下方式: public String readFromFile(File file) try (BufferedReader reader = new BufferedReader( new InputStreamReader(new FileInputStream(file), StandardCharsets.UTF_8))) char[] b = new char[1024]; StringBuilder sb = new StringBuilder(); int length; while ((length = reader.read(b)) 0) sb.append(b, 0, length); return sb.toString(); catch (IOException e) e.printStackTrace(); return null; 这里其实就只是把FileReader换成了BufferedReader，不过核心是InputStreamReader，因为其可以设定读取文件的InputStream编码格式。 文件写入文件写入也可以使用配套的FileWrite: public void writeToFile(String content, File target) throws IOException try (FileWriter writer = new FileWriter(target)) writer.write(content); writer.flush(); 同样的，FileWriter也是使用的默认的文件编码，可能会让中文字符变成乱码。这里也可以使用 public void writeToFile(String content, File target) throws IOException try (BufferedWriter writer = new BufferedWriter( new OutputStreamWriter(new FileOutputStream(target), StandardCharsets.UTF_8))) writer.write(content); writer.flush(); 不过在写入文件时，如果文件已存在，那么Writer会根据文件编码来自动匹配编码。","tags":["Java","文件"],"categories":["技术"]},{"title":"GIT服务器的搭建","path":"/2022/46417/","content":"本篇只有git服务器的基础搭建教程，没有git的其他教程。 准备● 能安装git的服务器一枚，性能不限，存储空间需要视仓库总容量而定，最少200MB。推荐Linux服务器。● 需要用git的客户端一枚（不然服务器给谁用） 服务器环境● 需要与客户机互通● 需要开放9418端口（git的默认访问端口） 搭建服务器 安装git 没什么好说的，就是安装git。当然，想自己写一个git也不是不行。 apt-get install git (ubuntu)yum install git (centos) 选择git仓库总地址 例如我们选择userprojectgit作为我们仓库的存储文件夹。 mkdir /usr/project/git;cd /usr/project 创建一个git仓库 这里使用了git的init命令。 git init --bare mygit.git --bare后面我们写上我们的一个仓库名称，这里的仓库名称叫做mygit.git。你也可以用如下的仓库名，不过不推荐。 git init --bare mygit.awsl 创建访问用户 有了仓库，我们就需要对仓库进行权限管理，就是设置一下访问用户。 访问git仓库的方式有很多，我们这里不考虑ssh的方式，直接用用户名与密码，简单粗暴。 useradd xiaoming;passwd xiaoming; (请自行设定密码) 这里我们添加了一个叫xiaoming的用户。 将访问用户授权到仓库 我们把之前创建的mygit.git仓库授权给xiaoming chown -R xiaoming mygit.git 这个其实就是将文件夹权限给了系统用户，而这个也是git的权限判定策略。 到这里其实服务器就差不多了。 配置客户机 安装git 请下载对应客户机系统的git，然后装上，谢谢。 客户机配置就完成了，是不是很简单。 拉取项目 创建项目目录 省略一万个字。 需要注意的是，拉取的项目会生成在这个文件夹下的子文件夹中，子文件夹名称就是项目名称。 在这个项目目录下进行git指令操作 window用户可以在项目文件夹下使用右键调出快捷菜单，然后点击git bash进入git命令行。 linux用户直接用输入git指令就行。 通过clone命令从git服务器拉取已有项目 之前我们在git服务器上创建了一个项目mygit.git，我们现在就把这个项目拉取下来。 git clone xiaoming@192.168.10.1:/usr/project/git/mygit.git 从命令中可以看出来，@符号前面的是用户，后面跟的是服务器IP地址。:符号后面的就是git项目路径了。 在命令运行后，会要求你输入xiaoming的密码，这个密码就是之前在服务器中设定的密码，输入后，项目就拉取下来了。 推送项目 假装搞项目 我们为了体验git的作用，就在这个文件夹里随意新建几个文本文件，当然，你也可以把一些什么视频啊、游戏啊放进来，不过不推荐。 将需要管理的文件推送到暂存区 git的管理属于被动式的，就是你要给它说它需要管理的东西，不然它就不会去管理。就像是我们之前创建的几个文本文件，虽然是属于git管理的目录下，但是它并不会把这些文件纳入管辖范畴，最多就是在你某些操作的时候提醒你还有文件可以管理。 这里使用add命令来添加文件： git add 这个文件.txt;git add 那个文件.txt; 然后这些文件就被添加到了暂存区。 为什么是暂存区呢？反正目前你只需要知道在暂存区的文件就会受到git的管理就行了，关于git的详细资料请移步 ● W3CSchool的git教程 ● 菜鸟的git教程 推送到本地仓库 git的管理首先是要推送到本地仓库的，然后才是远程仓库。也就是从暂存区将文件给到本地仓库。 这里用commit命令： git commit -m 这里写上本次提交的意义或是内容 推送到远程仓库 远程仓库可以是任何的git仓库，前提是你在仓库有推送权限。 这里用push命令推送： git push origin master; origin表示的是远程仓库名称，默认的就是origin，master表示推送到master分支。 对于新建的项目提交，需要使用其他的方法，本篇不做讲解。 完成以上，完成。","tags":["教程","Git"],"categories":["技术"]},{"title":"Centos7中搭建Minecraft官服","path":"/2022/50282/","content":"整体流程分为环境搭建、服务端启动、服务端配置、运行维护四个环节。 环境搭建Miecraft官服只需要安装Java环境即可，不过一般我们会用其他的辅助管理工具。 安装screen，用于服务器后台管理 yum install screen 安装lrzsz，用于上传和下载 yum install lrzsz 安装Java，最基本的服务器环境（注意，1.18版本的Minecraft需要Java17，其他版本使用Java8即可） yum -y install java-1.8.0-openjdk* 上传并启动服务端 创建服务器目录 cd /usr/games (进入/usr/games目录)mkdir minecraft (创建minecraft目录) 上传服务端文件 cd /usr/games/minecraft (进入minecraft目录)rz (在弹出窗口中选择需要启动的服务端文件或整合包) 开启视窗 screen -S minecraft (创建并进入minecraft视窗) 服务端启动 第一次启动服务端 java -Xmx1024M -Xms1024M -jar minecraft_1.14_server.jar nogui(启动文件名为minecraft_1.14_server.jar的服务端) 第一次启动时，会告知需要编辑许可文件。此时服务端会自动关闭，直到许可被同意，操作如下。 编辑许可与vi编辑器的基本使用 在minecraft目录下操作。 vi eula.txt (用vi编辑器修改eula.txt文件) 用键盘的方向键将光标移至文本末尾，按下A键，启动编辑模式。 将eula=false改为eula=true。 随后按下ESC键退出编辑模式。 按住SHIFT和L键旁边的:，启动命令模式，此时左下角会出现冒号。 输入wq（w表示write，q表示quit），按下ENTER键，保存退出。 服务端配置 服务器参数设置 vi server.properties (用vi编辑器修改服务器配置) 没有正版的玩家可以将online=true改为online=false(关闭在线验证) 其他参数参考 wiki 重新启动服务端 当服务端在运行时，可以在服务端内(screen视窗内)使用stop指令来正常停止服务器。也可以通过ps -ef | grep server指令找到服务端对应的运行进程，使用kill指令来强制停止(无法正常保存世界数据，不推荐) 当服务端停止运行时，可以用以下命令启动： java -Xmx1024M -Xms1024M -jar minecraft_1.14_server.jar nogui(启动文件名为minecraft_1.14_server.jar的服务端) -Xmx表示服务器能用的理论最大内存量 -Xms表示服务器初始时使用的内存量 出现Done For Help表示启动完成。 退出视窗 按住CTRL，随后先后分别按下A和D键，及可退出screen视窗 退出xshell exit (退出) 服务器维护 进入minecraft视窗 screen -r minecraft (进入minecraft视窗) 关闭服务器 进入minecraft视窗，输入stop 注意：服务器重启后，视窗需要重建。 附录 Minecraft官服下载 MinecraftWiki MiecraftBBS MinecraftMod","tags":["教程","Minecraft"],"categories":["技术"]},{"title":"Nginx在Linux服务器上的安装","path":"/2022/55359/","content":"首先最常用的就是在线安装了，比如yum install nginx，在线安装一般都不会有什么大问题，这里就不赘述了，就只谈谈离线安装的问题。 下载Nginx 下载可以是在本机下载，然后上传到服务器，也可以直接在服务器下载。 官网下载地址是: http://nginx.org/en/download.html 下载tar.gz版本就可以了。 解压文件 确定好解压文件夹后，通过tar -zxvf [filename]来解压文件。 解压后会根据nginx版本生成文件夹，例如nginx-1.20.0，然后cd进去。 检查依赖与安装依赖 在解压后的nginx文件夹下， 通过./configure命令来检测依赖与执行基础配置。如果有需要而未安装的依赖，会有命令提示。 依赖包括但不限于： gcc pcre pcre-devel zlib zlib-devel 依赖的安装请自行尝试，这里不做赘述。 编译与安装 ./configure成功之后，会生成makeFile文件，用于描述编译信息。这时在nginx目录下，通过make命令执行编译操作，过程需要几分钟。 在编译完成后，执行make install命令安装。 安装完成后，可以通过whereis nginx命令查看安装路径，一般都是/usr/local/nginx。 启动 在安装路径下，有一个sbin目录，其中的nginx就是可执行文件。通过./nginx来做临时启动。 剩下的就没什么可说了。 配置 nginx的基础配置文件是安装目录下的config/nginx.conf，在这里面可以修改配置。修改后在sbin目录下中执行nginx -t来检测配置文件的正确性。 一般情况下，我们都会使用额外的配置文件，所以需要在config/nginx.conf中添加include参数，例如include /usr/local/nginx-conf/*.conf;，这样，nginx就会在运行时包括/usr/local/nginx-conf下的所有配置文件了。 另外就是添加新的代理，这里我们以新的配置文件test.conf做演示。 server # 监听的端口 listen 81; # 代理的前端文件路径 root /usr/local/html/test; # url监听地址 location / # 首页文件名 index index.html; 随后nginx -t检查配置是否正确，检测通过后可以通过nginx -s reload热重载。 此时就可以通过访问IP:81来查看前端页面了。","tags":["教程","Nginx"],"categories":["技术"]},{"title":"SimpleDateFormat","path":"/2022/56533/","content":"SimpleDateFormat是Java用于构造与解析时间格式的类，虽然在多线程上有漏洞，但还是挺常用的。这里记录一下我遇到的问题。 SimpleDateFormat sdf = new SimpleDateFormat(yyyy-MM-dd hh:mm:ss.sss);System.out.println(sdf.format(new Date())); 上面这段代码在早上运行是没有问题的，但是到了下午就会发现这玩意会少12小时。代码写得多的阿猿肯定看出来了，hh表示的是 12进制 的小时，这里应该用HH来表示 __24进制__。注意，这里的sss也是有问题的，应该用SSS 这个问题是我在从数据库获取日期，然后返回给前端的时候出现的。最初我以为是数据库的时区没设置对。但是我去数据库查数据时，时间又是正常的。 数据库排除后，我又担心是我传给前端时出现了时区问题，所以就直接在本地打印了。然后时间就出错了。找到问题出现的地点后，就该排查问题根源了。 首先我先排查的时区，因为很巧的是只有小时数错误，而且是12小时。我以为是那种加个GMT+8的问题，就对SimpleDateFormat设了时区，不行。设置后，时间会走4个小时，然后就只差了8小时。这个就很干扰我了，这个8小时就是我们时区的时差。然后我就一直按着这条错误路线排查，一直都没有解决。我差点想对小时单独处理了，不过我还是先把这个问题给了我的一个同事。他表示没有遇到过这个问题，然后又在自己电脑上自己敲了一遍，时间正常的。 我就让同事把他的代码发给我，我来试一下（我开始相信玄学了）。运行后时间是正常的，啊这。。。到这里我都还没有发现是时间格式的问题，直到我同事让我把我之前的代码发给他。我发过去之后，他一下就看出问题了。嗯，挺好。 这里附一份时间格式表 字母 日期或时间元素 表示 示例 G Era标志符 Text AD y 年 Number 2022；22 M 月 Number 02；Feb w 年中周 Number 10 W 月中周 Number 2 d 月中天 Number 22 D 年中天 Number 72 F 月中周 Number 4 E 星期 Text Tue；星期二 a 时段 Text AM；PM h 小时（12进制）（1 - 12） Number 8 H 小时（24进制）（0 - 23） Number 16 m 时中分钟 Number 23 s 分中秒 Number 27 S 秒中毫秒 Number 271 k 小时（24进制）（1 - 24） Number 11 K 小时（12进制）（0 - 11） Number 00 z 时区（文字表示） Text CST Z 时区（数字表示） Text +0800","tags":["Java","文本解析"],"categories":["技术"]},{"title":"在Java中提取字符串中的区域数据","path":"/2022/48555/","content":"这里的区域数据指的是类似于Hello #name中的#name一样，是一段连续的字符串。 之前有一个需要将字符串中的宏变量替换的需求，而宏变量可以自定义，写在数据库中。所以需求变成了： 提取字符串中所有的宏变量，然后将宏变量进行替换 这里宏变量用的是花括号框起来的，并且其中的内容只有大写字母与小数点，并不会存在相互包含的关系，所以我一开始想用正则匹配去处理。但是字符串中的宏变量位置、数量都不确定 (最主要是因为正则我不熟悉)。最后只能简单粗暴地对字符串进行遍历了。 这里的遍历思路其实很简单，因为宏变量是用成对的括号括起来的，所以可以以为始，为终，将中间的所有字符提取出来就是需要的宏变量了。 /** * 获取字符串中的所有宏变量。br/ * 因为没有其他的提取需求，所以就直接将左右括号定死了。br/ * * @param s 需要提取宏变量的字符串 * @return 字符串中的所有宏变量 */public ListString macro(String s) // 获取字符串的字符数组 char[] chars = s.toCharArray(); // 构造宏变量列表 ListString list = new ArrayList(); // 用StringBuilder作为宏变量储存的临时容器 StringBuilder sb = new StringBuilder(); // 开始遍历 for (char c : chars) // 这里做的是分步处理，逻辑上更清晰。也可以将几个if统一起来，减少代码量 // 当出现左括号时，表示此处是宏变量的头 if (c == ) sb.append(c); // 出现有括号时，表示此处是宏变量的尾。此时可以存入宏变量列表并清空容器 else if (c == ) sb.append(c); list.add(sb.toString()); // 清空StringBuilder sb.delete(0, sb.length()); // 以容器的内容作为是否出于宏变量位置的标识 else if (sb.length() 0) sb.append(c); return list; 这个方法适用于没有包含关系的带有头尾标识的字符串提取，还是挺简单的。","tags":["Java","文本解析"],"categories":["技术"]},{"title":"Markdown的二三事","path":"/2022/28863/","content":"首先，我认为Markdown对于人类文明的进步做出了卓越的贡献，理由就是Markdown让人类文明进步了。 认真说，Markdown对于我这种不喜欢DOC文档的人来说，简直就是福音。富文本也是，用着不爽。我非常不喜欢在打字的时候，右手时不时的需要去动鼠标，很破坏那种沉浸的感觉。而Markdown就很好地解决了这个问题。 虽然说Html同样可以做到，但是太麻烦了。Markdown就像是内嵌了一套CSS，你只需要使用一些标记来关联样式就好了。 然后，以下是我收集的关于Markdown的一些小技巧： 在链接文本中出现空格时，可以用#32;来替换空格。 在使用列表时，属于列表单项的换行内容需要换行并对比此单项缩进两个空格。 例如这样，一是为了排版，二是为了有序列表的序号能正确地识别。 Markdown的锚点跳转只能对标题使用，例如通过 一级标题 来回到这个文档的大标题。同样，此链接可以链接到其他文件的标题。","tags":["二三事","Markdown"],"categories":["吐槽"]},{"title":"《关于我重新搞小站结果Gridea出了点问题只能手动push那档子事》","path":"/2022/63337/","content":"2022-02-01 记载 为什么要学习Git？为的就是减少受苦的次数 起因之前的小站源代码因为没有备份，所以在我几次重置电脑并且存档崩了之后就无了。这次打算好好搞，把自己的开源计划搞起来。所以我决定把源码和build档一起存在GtiHub上，这样换电脑也方便。 新的风暴然后就是重新下载Gridea，然后重新搞Token。在一切似乎都弄好后，我点了同步，在经过了长达一分钟的等待后，Gridea提示我同步出错。很好，生活又充实了起来 （看了issue，也有很多人无法同步，只能说好受了些，哈哈哈） 我觉得可能事Git的问题，我把verlif.github.io克隆到本地，然后把.git配置复制到Gridea的output目录（这个目录就是最终给到verlif.github.io仓库的项目目录）。再次点击同步，这次是直接出错，甚至没有等待。呵！ 行，我这次就直接把verlif.github.io里的文件除了.git全删了，然后把output下的除了.git文件全拷贝过去。然后打开VS Code，载入项目，提交、推送、超时……还行，超时至少还有机会 （到底是谁TMD污染GitHub的DNS的，焯） 。 在几次超时之后真的觉得还是命令好，不过很明显，我没有pull，但是这种状态下pull没有意义，我要启用终结协议--force了 （还没有force过，有点小期待） ，随后就是done。 最终刷新网页，nice。呵！就这？！ 不过，每次改完小站都要push两个项目着实麻烦，后面还是要写个自动化工具 （终于到我熟悉的领域了，T_T） 总结学会前端是多么的重要 但是除开这些问题，Gridea还是很好用的，就凭支持markdown这一点就足以让我继续使用 （html是人写的东西？？？）。 2024-01-17 更新 难搞啊，又重新装了一遍。","tags":["Gridea"],"categories":["吐槽"]},{"title":"关于","path":"/about/index.html","content":"关于就是关于，Verlif就是Verlif"},{"title":"[object Object]","path":"/template/文章发布模板.html","content":""},{"title":"玩意儿","path":"/toys/index.html","content":"MockData生成随机数据或填充随机数据到对象中的工具，非常适合用来做测试或接口开发。 MockApi基于SpringBootWeb的接口生成框架，只需要一个注解或是非侵入注册即可自动生成对应格式的数据接口。 比如有这样一个接口： @GetMappingpublic BaseResultUser getById(Integer id) return null; 访问在MockApi注册并生成的接口可以得到以下数据： Windonly简单的数据拖拽面板桌面应用，适用于频繁的复制粘贴。 OnePage 一页 · ONE 是一个基于 Vue3 的纯前端个人主页生成器。通过 组件 + 网格 的方式，你可以自由组合、定制并分享属于自己的页面。 在线访问：one"}]